{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;"},"docs":[{"location":"","text":"Harminder's Knowledge Base My knowledge base encompasses my work on projects involving object detection, single-board computers, RPA social media and web automation solutions, and more. Some other areas of interest of mine include: intelligent single and multi-agent systems, cognitive frameworks, and productivity. I also blog about my experiences and learnings. You can reach out to me on LinkedIn. GitHub Activity OSRS Tracker // This function will fetch OSRS player weekly gains and update the HTML content async function fetchAndDisplayOSRSWeeklyGains(playerName) { try { // Fetch the player gains using the Wise Old Man API const response = await fetch(`https://api.wiseoldman.net/v2/players/${encodeURIComponent(playerName)}/gained?period=week`, { headers: { 'Content-Type': 'application/json' } }); if (!response.ok) { throw new Error('Player gains could not be retrieved'); } const playerGains = await response.json(); // Select the container where the gains will be displayed const gainsContainer = document.getElementById('osrs-weekly-stats-gained-container'); // Clear any existing content in the gains container gainsContainer.innerHTML = ''; // Create a table to hold the skill rows const table = document.createElement('table'); table.className = 'w-full min-w-full'; // Create and append the header row const headerRow = document.createElement('tr'); headerRow.className = \"border-b border-gray-600\"; headerRow.innerHTML = ` <th class=\"whitespace-nowrap p-4 align-middle\">Skill</th> <th class=\"whitespace-nowrap p-4 align-middle\">XP Gained</th> <th class=\"whitespace-nowrap p-4 align-middle\">Levels Gained</th> `; table.appendChild(headerRow); // Sort skills by experience gained const sortedSkills = Object.entries(playerGains.data.skills).sort((a, b) => b[1].experience.gained - a[1].experience.gained); // Take only the top 5 skills const topSkills = sortedSkills.slice(0, 5); // Loop through the top 5 skills and create table rows for each one topSkills.forEach(([skill, data], index) => { const row = document.createElement('tr'); row.className = `border-b border-gray-600 ${index % 2 === 0 ? 'bg-gray-500' : 'bg-teal-200'} hover:bg-gray-600 transition-colors relative cursor-pointer`; // Skill icon and name always shown const skillCell = document.createElement('td'); skillCell.className = \"whitespace-nowrap p-4 align-middle\"; skillCell.innerHTML = `<div class=\"flex items-center gap-x-2\"> <img alt=\"${skill}\" loading=\"lazy\" width=\"16\" height=\"16\" decoding=\"async\" class=\"shrink-0\" src=\"https://raw.githubusercontent.com/wise-old-man/wise-old-man/4c9374bed80cf6eb622b4bddb38f29fb764462ed/app/public/img/metrics/${skill}.png\"> ${skill.charAt(0).toUpperCase() + skill.slice(1)} </div>`; // XP gained - always shown const xpCell = document.createElement('td'); xpCell.className = \"whitespace-nowrap p-4 align-middle\"; xpCell.innerHTML = `<span>${data.experience.gained.toLocaleString()}</span>`; // Level gained - always showm const levelCell = document.createElement('td'); levelCell.className = \"whitespace-nowrap p-4 align-middle\"; levelCell.innerHTML = `<span>${data.level.gained.toLocaleString()}</span>`; // Append the cells to the row row.appendChild(skillCell); row.appendChild(xpCell); row.appendChild(levelCell); // Append the row to the table table.appendChild(row); }); // Append the table to the gainsContainer gainsContainer.appendChild(table); // Call the adjustFontSize function here, after the table is in the DOM adjustFontSize(); } catch (error) { console.error('Error fetching OSRS weekly gains:', error); const gainsContainer = document.getElementById('osrs-weekly-stats-gained-container'); gainsContainer.innerHTML = '<p>Error fetching weekly gains. Please try again later.</p>'; } } // Replace 'yourPlayerName' with the actual player name fetchAndDisplayOSRSWeeklyGains('xogsherjattx'); // Add an image on the bottom of the card with the remaining height to fill the card const osrsWeeklyStatsGainedCard = document.getElementById('osrs-weekly-stats-gained-card'); const osrsWeeklyStatsGainedCardImage = document.createElement('img'); osrsWeeklyStatsGainedCardImage.src = 'https://raw.githubusercontent.com/harmindersinghnijjar/face/main/fotor_2023-8-13_21_49_31-fotor-2023081322615.png'; // Add the image to the card if there is extra space (typically on larger screens) if (osrsWeeklyStatsGainedCard.offsetHeight > 400) { osrsWeeklyStatsGainedCard.appendChild(osrsWeeklyStatsGainedCardImage); } // This function will adjust the font size of the table based on the container width function adjustFontSize() { const osrsWeeklyStatsGainedContainer = document.getElementById('osrs-weekly-stats-gained-container'); const osrsWeeklyStatsGainedTable = osrsWeeklyStatsGainedContainer.querySelector('table'); // Ensure the table is present before attempting to adjust font size if (osrsWeeklyStatsGainedTable) { const containerWidth = osrsWeeklyStatsGainedContainer.offsetWidth; let fontSize = 16; // Default font size for desktop // Check if on mobile if (window.matchMedia(\"(max-width: 600px)\").matches) { // If on mobile, scale down the font size fontSize = 12; } else { // For larger screens, adjust font size based on container width fontSize = Math.min(16, Math.max(12, Math.floor(containerWidth / 100))); } osrsWeeklyStatsGainedTable.style.fontSize = `${fontSize}px`; } } // Add an event listener to call adjustFontSize when the window resizes window.addEventListener('resize', adjustFontSize);","title":"Home"},{"location":"areas/","text":"Areas of Interest Culture and the arts Geography and places Health and fitness History and events Human activities Mathematics and logic Natural and physical sciences People and self Philosophy and thinking Religion and belief systems Society and social sciences Technology and applied sciences","title":"Areas of Interest"},{"location":"areas/#areas-of-interest","text":"Culture and the arts Geography and places Health and fitness History and events Human activities Mathematics and logic Natural and physical sciences People and self Philosophy and thinking Religion and belief systems Society and social sciences Technology and applied sciences","title":"Areas of Interest"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/sports/combat-sports/martial-arts/brazilian-jiu-jitsu/brazilian_jiu-jitsu/","text":"Brazilian Jiu-Jitsu","title":"Brazilian Jiu-Jitsu"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/sports/combat-sports/martial-arts/brazilian-jiu-jitsu/brazilian_jiu-jitsu/#brazilian-jiu-jitsu","text":"","title":"Brazilian Jiu-Jitsu"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/cloud-gaming/amazon-luna/amazon_luna/","text":"Amazon Luna","title":"Amazon Luna"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/cloud-gaming/amazon-luna/amazon_luna/#amazon-luna","text":"","title":"Amazon Luna"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/cloud-gaming/netflix-gaming/netflix-gaming/","text":"[[202210192155 Wed 1019 Techmeme#^5a6ab7]]","title":"Netflix gaming"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/cloud-gaming/xbox-cloud-gaming/xbox_cloud_gaming/","text":"Xbox Cloud Gaming","title":"Xbox Cloud Gaming"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/cloud-gaming/xbox-cloud-gaming/xbox_cloud_gaming/#xbox-cloud-gaming","text":"","title":"Xbox Cloud Gaming"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/console-gaming/xbox-adaptive-controller/charities/warfighter--engaged/","text":"Warfighter Engaged is a 501\u00a9(3) nonprofit organization that helps severely wounded and disabled warfighters.","title":"Warfighter  engaged"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/console-gaming/xbox-adaptive-controller/charities/the-ablegamers-foundation/titan-one/","text":"Titan One is a gaming device that allows you to use your favorite controller on any compatible console or system. It's compatible with PS4, Xbox One, Nintendo Switch, PS3, Xbox 360, PS TV, Android, PC, and Game Streaming Apps. https://www.consoletuner.com/products/titan-one/","title":"Titan one"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/mobile-gaming/mobile_games/","text":"Mobile games","title":"Mobile games"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/mobile-gaming/mobile_games/#mobile-games","text":"","title":"Mobile games"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/web-gaming/web/","text":"Web","title":"Web"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/games/video-games/video-game-hardware-platforms/web-gaming/web/#web","text":"","title":"Web"},{"location":"areas/culture-and-the-arts/recreation-and-entertainment/music/lofi/top-100-lofi-songs/","text":"Purrple Cat - Sugar Coat Kanisan x Frad - Fireflies 3.","title":"Top 100 lofi songs"},{"location":"areas/geography-and-places/geography_and_places/","text":"Geography and places Sub-Areas North America","title":"Geography and places"},{"location":"areas/geography-and-places/geography_and_places/#geography-and-places","text":"","title":"Geography and places"},{"location":"areas/geography-and-places/geography_and_places/#sub-areas","text":"North America","title":"Sub-Areas"},{"location":"areas/geography-and-places/north-america/north_america/","text":"North America Sub-Areas United States","title":"North America"},{"location":"areas/geography-and-places/north-america/north_america/#north-america","text":"","title":"North America"},{"location":"areas/geography-and-places/north-america/north_america/#sub-areas","text":"United States","title":"Sub-Areas"},{"location":"areas/geography-and-places/north-america/united-states/united_states/","text":"United States Sub-Areas California Washington","title":"United States"},{"location":"areas/geography-and-places/north-america/united-states/united_states/#united-states","text":"","title":"United States"},{"location":"areas/geography-and-places/north-america/united-states/united_states/#sub-areas","text":"California Washington","title":"Sub-Areas"},{"location":"areas/geography-and-places/north-america/united-states/california/california/","text":"California","title":"California"},{"location":"areas/geography-and-places/north-america/united-states/california/california/#california","text":"","title":"California"},{"location":"areas/geography-and-places/north-america/united-states/washington/washington/","text":"Washington","title":"Washington"},{"location":"areas/geography-and-places/north-america/united-states/washington/washington/#washington","text":"","title":"Washington"},{"location":"areas/health-and-fitness/health_and_fitness/","text":"Health and fitness Sub-Areas Medicine","title":"Health and fitness"},{"location":"areas/health-and-fitness/health_and_fitness/#health-and-fitness","text":"","title":"Health and fitness"},{"location":"areas/health-and-fitness/health_and_fitness/#sub-areas","text":"Medicine","title":"Sub-Areas"},{"location":"areas/health-and-fitness/medicine/medicine/","text":"Medicine Sub-Areas Psychiatry","title":"Medicine"},{"location":"areas/health-and-fitness/medicine/medicine/#medicine","text":"","title":"Medicine"},{"location":"areas/health-and-fitness/medicine/medicine/#sub-areas","text":"Psychiatry","title":"Sub-Areas"},{"location":"areas/health-and-fitness/medicine/psychiatry/psychiatry/","text":"Psychiatry","title":"Psychiatry"},{"location":"areas/health-and-fitness/medicine/psychiatry/psychiatry/#psychiatry","text":"","title":"Psychiatry"},{"location":"areas/history-and-events/history_and_events/","text":"History and events Sub-Areas Ancient History Early modern history","title":"History and events"},{"location":"areas/history-and-events/history_and_events/#history-and-events","text":"","title":"History and events"},{"location":"areas/history-and-events/history_and_events/#sub-areas","text":"Ancient History Early modern history","title":"Sub-Areas"},{"location":"areas/history-and-events/ancient-history/ancient_history/","text":"Ancient History Sub-Areas Ancient East","title":"Ancient History"},{"location":"areas/history-and-events/ancient-history/ancient_history/#ancient-history","text":"","title":"Ancient History"},{"location":"areas/history-and-events/ancient-history/ancient_history/#sub-areas","text":"Ancient East","title":"Sub-Areas"},{"location":"areas/history-and-events/ancient-history/ancient-east/ancient_east/","text":"Ancient East Sub-Areas Ancient Egypt","title":"Ancient East"},{"location":"areas/history-and-events/ancient-history/ancient-east/ancient_east/#ancient-east","text":"","title":"Ancient East"},{"location":"areas/history-and-events/ancient-history/ancient-east/ancient_east/#sub-areas","text":"Ancient Egypt","title":"Sub-Areas"},{"location":"areas/history-and-events/ancient-history/ancient-east/ancient-egypt/ancient_egypt/","text":"Ancient Egypt","title":"Ancient Egypt"},{"location":"areas/history-and-events/ancient-history/ancient-east/ancient-egypt/ancient_egypt/#ancient-egypt","text":"","title":"Ancient Egypt"},{"location":"areas/history-and-events/early-modern-history/early_modern_history/","text":"Early modern history","title":"Early modern history"},{"location":"areas/history-and-events/early-modern-history/early_modern_history/#early-modern-history","text":"","title":"Early modern history"},{"location":"areas/human-activities/human_activities/","text":"Human activities Sub-Areas Agriculture Communicating Government Religion","title":"Human activities"},{"location":"areas/human-activities/human_activities/#human-activities","text":"","title":"Human activities"},{"location":"areas/human-activities/human_activities/#sub-areas","text":"Agriculture Communicating Government Religion","title":"Sub-Areas"},{"location":"areas/human-activities/agriculture/agriculture/","text":"Agriculture","title":"Agriculture"},{"location":"areas/human-activities/agriculture/agriculture/#agriculture","text":"","title":"Agriculture"},{"location":"areas/human-activities/communicating/communicating/","text":"Communicating Sub-Areas Conversing Writing","title":"Communicating"},{"location":"areas/human-activities/communicating/communicating/#communicating","text":"","title":"Communicating"},{"location":"areas/human-activities/communicating/communicating/#sub-areas","text":"Conversing Writing","title":"Sub-Areas"},{"location":"areas/human-activities/communicating/conversing/discord/2022117193242-beginner-venezuelan-botter/","text":"","title":"2022117193242 beginner venezuelan botter"},{"location":"areas/human-activities/communicating/writing/writing/","text":"Writing","title":"Writing"},{"location":"areas/human-activities/communicating/writing/writing/#writing","text":"","title":"Writing"},{"location":"areas/human-activities/government/government/","text":"Government","title":"Government"},{"location":"areas/human-activities/government/government/#government","text":"","title":"Government"},{"location":"areas/human-activities/religion/religion/","text":"Religion Sub-Areas Sikhism","title":"Religion"},{"location":"areas/human-activities/religion/religion/#religion","text":"","title":"Religion"},{"location":"areas/human-activities/religion/religion/#sub-areas","text":"Sikhism","title":"Sub-Areas"},{"location":"areas/human-activities/religion/sikhism/sikhism/","text":"Sikhism","title":"Sikhism"},{"location":"areas/human-activities/religion/sikhism/sikhism/#sikhism","text":"","title":"Sikhism"},{"location":"areas/mathematics-and-logic/mathematics_and_logic/","text":"Mathematics and logic Sub-Areas Logic","title":"Mathematics and logic"},{"location":"areas/mathematics-and-logic/mathematics_and_logic/#mathematics-and-logic","text":"","title":"Mathematics and logic"},{"location":"areas/mathematics-and-logic/mathematics_and_logic/#sub-areas","text":"Logic","title":"Sub-Areas"},{"location":"areas/mathematics-and-logic/logic/logic/","text":"Logic","title":"Logic"},{"location":"areas/mathematics-and-logic/logic/logic/#logic","text":"","title":"Logic"},{"location":"areas/natural-and-physical-sciences/natural_and_physical_sciences/","text":"Natural and physical sciences Sub-Areas Biology Physics","title":"Natural and physical sciences"},{"location":"areas/natural-and-physical-sciences/natural_and_physical_sciences/#natural-and-physical-sciences","text":"","title":"Natural and physical sciences"},{"location":"areas/natural-and-physical-sciences/natural_and_physical_sciences/#sub-areas","text":"Biology Physics","title":"Sub-Areas"},{"location":"areas/natural-and-physical-sciences/biology/biology/","text":"Biology Sub-Areas Genetics Paleontology","title":"Biology"},{"location":"areas/natural-and-physical-sciences/biology/biology/#biology","text":"","title":"Biology"},{"location":"areas/natural-and-physical-sciences/biology/biology/#sub-areas","text":"Genetics Paleontology","title":"Sub-Areas"},{"location":"areas/natural-and-physical-sciences/biology/genetics/genetics/","text":"Genetics","title":"Genetics"},{"location":"areas/natural-and-physical-sciences/biology/genetics/genetics/#genetics","text":"","title":"Genetics"},{"location":"areas/natural-and-physical-sciences/biology/paleontology/paleontology/","text":"Paleontology Sub-Areas Dinosaurs","title":"Paleontology"},{"location":"areas/natural-and-physical-sciences/biology/paleontology/paleontology/#paleontology","text":"","title":"Paleontology"},{"location":"areas/natural-and-physical-sciences/biology/paleontology/paleontology/#sub-areas","text":"Dinosaurs","title":"Sub-Areas"},{"location":"areas/natural-and-physical-sciences/biology/paleontology/dinosaurs/dinosaurs/","text":"Dinosaurs","title":"Dinosaurs"},{"location":"areas/natural-and-physical-sciences/biology/paleontology/dinosaurs/dinosaurs/#dinosaurs","text":"","title":"Dinosaurs"},{"location":"areas/natural-and-physical-sciences/physics/physics/","text":"Physics","title":"Physics"},{"location":"areas/natural-and-physical-sciences/physics/physics/#physics","text":"","title":"Physics"},{"location":"areas/people-and-self/people_and_self/","text":"People and self Sub-Areas Self","title":"People and self"},{"location":"areas/people-and-self/people_and_self/#people-and-self","text":"","title":"People and self"},{"location":"areas/people-and-self/people_and_self/#sub-areas","text":"Self","title":"Sub-Areas"},{"location":"areas/people-and-self/self/self/","text":"Self","title":"Self"},{"location":"areas/people-and-self/self/self/#self","text":"","title":"Self"},{"location":"areas/philosophy-and-thinking/philosophy_and_thinking/","text":"Philosophy and thinking","title":"Philosophy and thinking"},{"location":"areas/philosophy-and-thinking/philosophy_and_thinking/#philosophy-and-thinking","text":"","title":"Philosophy and thinking"},{"location":"areas/religion-and-belief-systems/religion_and_belief_systems/","text":"Religion and belief systems","title":"Religion and belief systems"},{"location":"areas/religion-and-belief-systems/religion_and_belief_systems/#religion-and-belief-systems","text":"","title":"Religion and belief systems"},{"location":"areas/society-and-social-sciences/society_and_social_sciences/","text":"Society and social sciences Sub-Areas Business Communication Education","title":"Society and social sciences"},{"location":"areas/society-and-social-sciences/society_and_social_sciences/#society-and-social-sciences","text":"","title":"Society and social sciences"},{"location":"areas/society-and-social-sciences/society_and_social_sciences/#sub-areas","text":"Business Communication Education","title":"Sub-Areas"},{"location":"areas/society-and-social-sciences/business/management/second-brain/ali-abdaal%27s-the-second-brain/","text":"The Second Brain - A Life-Changing Productivity System 0:12 ![[Pasted image 20221005105501.jpg]] The Second Brain method is a system for managing knowledge and ideas that is based on the latest scientific research on how the brain works. It is designed to help you remember what you learn, make connections between ideas, and generate new ideas. The Second Brain method has four steps: 1. Collect: Collect information and ideas from a variety of sources. 2. Organize: Organize information and ideas using a system that works for you. 3. Reflect: Reflect on information and ideas to identify patterns and connections. 4. Create: Create new ideas by combining and expanding on existing ideas. 1:22 ![[Pasted image 20221005110004.jpg]] Borrowed creativity is the idea that creativity is all about remixing stuff. This means taking existing ideas and putting your own spin on them to create something new. This can be done by adding your own ideas to an existing idea, or by taking an existing idea and changing it to better suit your needs. 2:14 ![[Pasted image 20221005110027.jpg]] Our brains are for having ideas, not for storing them. This means that we should capture our ideas as they come to us so that we can free up our brain power for generating new ideas. To do this, we can use a variety of methods, such as keeping a notebook or using a voice recorder. 3:19 ![[Pasted image 20221005110234.jpg]] Ideas can be recycled and reused over time to create new and innovative products or solutions. The process of recycling ideas begins with brainstorming and then evaluating the potential of the ideas that are generated. Once the best ideas are identified, they can be implemented and further refined to create a new product or solution. The benefits of idea recycling include reducing the amount of time and resources required to generate new ideas and increasing the chances of success by reusing proven ideas. 4:23 ![[Pasted image 20221005110258.jpg]] If you associate notes with projects, you can easily see which notes are related to which project. This can be helpful when you are working on a project and need to reference notes from previous projects. Additionally, it can help you keep track of your thoughts and progress on a project over time. 5:05 ![[Pasted image 20221005110440.jpg]] Slow burns are important because they allow you to build something over time that is meaningful and significant. This could be a blog, a book, or even just a personal project that you're passionate about. By taking your time and slowly building something up, you're able to create something that is much more substantial than if you tried to do it all at once. Additionally, slow burns help to keep you motivated and excited about your projects, as you can see your progress over time. 6:51 ![[Pasted image 20221005110448.jpg]] One of the best ways to learn something new is to start with a wealth of knowledge. When you have a lot of notes from the past, you can start to see patterns and connections that you wouldn't have otherwise. This can help you learn the material more quickly and deeply. 8:01 ![[Pasted image 20221005110650.jpg]] Intermediate packets are pieces of work that are not the final product, but are necessary to complete the final product. They are often used to split up work so that it can be done more slowly and carefully, and to make sure that all the pieces are accounted for. 10:06 ![[Pasted image 20221005110818.jpg]] You can only speak on the articles you've published or the content you've created with true authority. Anything outside of that is simply an opinion. 11:53 ![[Pasted image 20221005110857.jpg]] When creating notes in Second-brain, it is important to keep your future self in mind. This means creating notes that are clear, concise, and well-organized. Taking the time to do this now will make it much easier for you to find and use the information later on. 13:20 ![[Pasted image 20221005110902.jpg]] The second brain is a place where you can organize your ideas and keep track of what you need to do. This is especially important for me because I tend to be a perfectionist and can get bogged down in the details. By keeping my ideas moving, I can stay focused on the task at hand and not get lost in the weeds.","title":"Ali abdaal's the second brain"},{"location":"areas/society-and-social-sciences/business/venture-capitalism/20221023193946-forbe%27s-ai-50/","text":"NAME INDUSTRY FUNDING HEADQUARTERS CEO Details 6sense Sales and Marketing $426 M San Francisco, California, United States Jason Zintak Abacus.AI Data Science $90 M San Francisco, California, United States Bindu Reddy Abnormal Security Cybersecurity $74 M San Francisco, California, United States Evan Reiser Amira Learning Education $21 M San Francisco, California, United States Mark Angel AMP Robotics Environment and Energy $78 M Louisville, Colorado, United States Matanya Horowitz Anyscale AI Infrastructure $160 M San Francisco, California, United States Robert Nishihara Arize AI Data Science $23 M Berkeley, California, United States Jason Lopatecki ASAPP Customer Service $400 M New York, New York, United States Gustavo Sapoznik Aurora Solar Environment and Energy $523 M San Francisco, California, United States Christopher Hopper Brain Technologies Consumer Technology $50 M San Mateo, California, United States Jerry Yue Brightseed Pharmaceutical $115 M San Francisco, California, United States Jim Flatt Canvas Construction $83 M San Francisco, California, United States Kevin Albert ClosedLoop Healthcare $45 M Austin, Texas, United States Andrew Eye Cohere Data Science $165 M Toronto, Ontario, Canada Aidan Gomez ConverseNow Customer Service $18 M Austin, Texas, United States Vinay Shukla Cresta Customer Service $157 M San Francisco, California, United States Zayd Enam Databricks Data Science $3.6 B San Francisco, California, United States Ali Ghodsi Dataiku Data Science $643 M New York, New York, United States Florian Douetteau Deepcell Biotech $100 M Menlo Park, California, United States Maddison Masaeli Domino Data Lab Data Science $228 M San Francisco, California, United States Nick Elprin Eigen Technologies Financial Services $80 M New York, New York, United States Lewis Liu Entos Pharmaceutical $58 M San Diego, California, United States Tom Miller Facet Media and Entertainment $18 M San Francisco, California, United States Joe Reisinger FarmWise Agriculture $65 M San Francisco, California, United States S\u00e9bastien Boyer Forethought Customer Service $92 M San Francisco, California, United States Deon Nicholas Generate Biomedicines Pharmacutical $420 M Cambridge, Massachusetts, United States Mike Nally Genesis Therapeutics Pharmacutical $56 M Burlingame, California, United States Evan Feinberg Glean AI Infrastructure $55 M Palo Alto, California, United States Arvind Jain Hugging Face AI Infrastructure $160 M New York, New York, United States Cl\u00e9ment Delangue Intenseye AI Infrastructure $29 M New York, New York, United States Sercan Esen Kintsugi Mindful Wellness Healthcare $28 M Berkeley, California, United States Grace Chang Komodo Health Healthcare $314 M New York, NY and San Francisco, California, United States Arif Nathoo Labelbox Data Science $180 M San Francisco, California, United States Manu Sharma Mashgin Retail $74 M Palo Alto, California, United States Abhinai Srivastava Matroid AI Infrastructure $33 M Palo Alto, California, United States Reza Zadeh MetaMap Financial Services $86 M San Francisco, California, United States Filip Victor Moveworks Employee Support $315 M Mountainview, California, United States Bhavin Shah Mutiny Sales and Marketing $71 M San Francisco, California, United States Jaleh Rezaei Nauto Transportation and Logistics $195 M Palo Alto, California, United States Stefan Heck Netradyne Transportation and Logistics $197 M San Diego, California, United States Avneesh Agrawal Nuro Transportation and Logistics $2 B Mountainview, California, United States Jiajun Zhu OctoML AI Infrastructure $132 M Seattle, Washington, United States Luis Ceze Overjet Healthcare $80 M Boston, Massachusetts, United States Wardah Inam Scale AI AI Infrastructure $602 M San Francisco, California, United States Alex Wang Sisu Data Data Science $129 M San Francisco, California, United States Peter Bailis Sprig Customer Service $65 M San Francisco, California, United States Ryan Glasgow Uniphore Customer Service $620 M Palo Alto, California, United States Umesh Sachdev Verge Genomics Pharmaceutical $134 M San Francisco, California, United States Alice Zhang Waabi Transportation and Logistics $83 M Toronto, Ontario, Canada Raquel Urtasun Whisper Healthcare $53 M San Francisco, California, United States Andrew Song","title":"20221023193946 forbe's ai 50"},{"location":"areas/society-and-social-sciences/communication/communication/","text":"Communication Sub-Areas Journalism","title":"Communication"},{"location":"areas/society-and-social-sciences/communication/communication/#communication","text":"","title":"Communication"},{"location":"areas/society-and-social-sciences/communication/communication/#sub-areas","text":"Journalism","title":"Sub-Areas"},{"location":"areas/society-and-social-sciences/communication/journalism/journalism/","text":"Journalism","title":"Journalism"},{"location":"areas/society-and-social-sciences/communication/journalism/journalism/#journalism","text":"","title":"Journalism"},{"location":"areas/society-and-social-sciences/education/education/","text":"Education Sub-Areas Columbia Basin College Internships","title":"Education"},{"location":"areas/society-and-social-sciences/education/education/#education","text":"","title":"Education"},{"location":"areas/society-and-social-sciences/education/education/#sub-areas","text":"Columbia Basin College Internships","title":"Sub-Areas"},{"location":"areas/society-and-social-sciences/education/columbia-basin-college/columbia_basin_college/","text":"Columbia Basin College","title":"Columbia Basin College"},{"location":"areas/society-and-social-sciences/education/columbia-basin-college/columbia_basin_college/#columbia-basin-college","text":"","title":"Columbia Basin College"},{"location":"areas/society-and-social-sciences/education/internships/internships/","text":"Internships","title":"Internships"},{"location":"areas/society-and-social-sciences/education/internships/internships/#internships","text":"","title":"Internships"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/computer-vision/cv-tasks/","text":"Computer Vision Tasks Image Classification Image Segmentation Image-to-Image Object Detection Unconditional Image Generation Zero-Shot Image Classification","title":"Cv tasks"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/computer-vision/cv-tasks/#computer-vision-tasks","text":"Image Classification Image Segmentation Image-to-Image Object Detection Unconditional Image Generation Zero-Shot Image Classification","title":"Computer Vision Tasks"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/computer-vision/overview/","text":"Computer vision is a field of computer science that is concerned with the way computers can be made to understand digital images or videos. This is done by algorithms that are designed to process images and extract information from them. The information that can be extracted includes things like object recognition, scene understanding, and motion detection.","title":"Overview"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/machine_learning/","text":"Machine Learning Sub-Areas Hugging Face Jasper MineDojo Open AI's Gynasium OpenAI's ChatGPT OpenAI's Codex OpenAI's GTP-3 OpenAI's Whisper Ryte Serpent AI","title":"Machine Learning"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/machine_learning/#machine-learning","text":"","title":"Machine Learning"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/machine_learning/#sub-areas","text":"Hugging Face Jasper MineDojo Open AI's Gynasium OpenAI's ChatGPT OpenAI's Codex OpenAI's GTP-3 OpenAI's Whisper Ryte Serpent AI","title":"Sub-Areas"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-universe/","text":"Universe allows an AI agent to use a computer like a human does: by looking at screen pixels and operating a virtual keyboard and mouse. We must train AI systems on the full range of tasks we expect them to solve, and Universe lets us train a single agent on any task a human can complete with a computer.","title":"Openai's universe"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/hugging-face/course/20221022175748-question-answering-with-merve/","text":"\ud83e\udd17 Tasks: Question Answering 0:08 ![[Pasted image 20221012230525.jpg]] Welcome to the Hugging Face [[NLP Tasks]] Series. 0:16 ![[Pasted image 20221012230535.jpg]] Question answering is the task of finding an answer in a given document. 0:38 ![[Pasted image 20221012230540.jpg]] Question answering is when a model takes a context, or the document you want to search in, and finds an answer within that context. This is also called extractive question answering. 0:44 ![[Pasted image 20221012230545.jpg]] The task is evaluated on two metrics, an exact match, and a fun score. Exact match measures how well the system understands the user's input, while the fun score measures how engaging the system is to use. 1:15 ![[Pasted image 20221012230550.jpg]] The F1 score is a metric used to measure the performance of a classification model. It is calculated as the average of two other metrics, precision and recall. Precision measures the percentage of items that were correctly classified, while recall measures the percentage of items that were classified correctly out of all the items that should have been classified correctly. 1:36 ![[Pasted image 20221012230640.jpg]] You can use question-answering models to automatically answer the questions asked by your customers. You simply need a document containing the information about your business and query through that document with the questions asked by your customers. 1:57 ![[Pasted image 20221012230644.jpg]] hf.co/course","title":"20221022175748 question answering with merve"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/articles/housing-comfort-in-the-united-states-and-india/","text":"Harminder Singh Nijjar, Lovepreet Kaur Kataria 13 Oct 2022 Abstract: Housing comfort is an essential aspect of life that is often taken for granted in the United States. Many factors contribute to comfort in a home, such as the type of flooring, insulation, central heating, air conditioning, etc. While these amenities are typically available in American homes, they may not be as accessible in other parts of the world, such as India. This can make it challenging to maintain a comfortable living environment in countries with extreme weather conditions. Subjects: Housing comfort and the factors that contribute to it. Housing comfort is an essential aspect of life that is often taken for granted. In the United States, various housing options are available to meet the needs of different people. While several options are available in India, housing comfort is not as good as in the United States. This is due to the limited number of housing options and amenities available in India. A few different things can affect how comfortable a house is. One consideration is the type of flooring in the house. In the US, many homes have carpets and rugs, which are softer to walk on and help keep feet warm in the winter. In India, however, many homes have cement or hardwood floors, which can be cold and uncomfortable in the winter. The second factor is insulation. In the US, insulation is typically made of fiberglass, cellulose, or rock wool, while in India, it generally is jute, mineral wool, or sheep's wool. Fiberglass is considered one of the better types of insulation because it is fire-resistant and sustainable. Another factor that contributes to home comfort is the presence of central heating and air conditioning. Homes in the United States are typically built with central heating and air conditioning, which helps to regulate the temperature inside the house and keep it comfortable year-round. This is not always the case in other parts of the world, such as India, where many homes are not equipped with these features, making it challenging to keep the inside of the home comfortable during extreme weather conditions. Overall, housing comfort is an essential aspect of life that is often taken for granted in the United States. Many factors contribute to comfort in a home, such as the type of flooring, insulation, central heating, air conditioning, etc. While these amenities are typically available in American homes, they may not be as accessible in other parts of the world, such as India. This can make it challenging to maintain a comfortable living environment in countries with extreme weather. [[The New Delhi Sketches by Harminder Nijjar]]","title":"Housing comfort in the united states and india"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/how-jasper-thinks/how-jasper-thinks/","text":"Jasper produces high-quality content but sometimes goes off the rails, repeats himself, or produces content you don't like. To get the most out of using Jasper, you need to understand how he thinks. This will greatly improve your ability to get high-quality content and troubleshoot any low-quality content.","title":"How jasper thinks"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/ai-outputs-or-content-history/","text":"All content is automatically saved to your AI outputs, including any generations inside of a document. In addition to your inbox, you have tabs for your favorite, flagged, and trash messages. 1. The Favorites folder contains any content that you have starred. This content will also be available in the general AI outputs folder. 2. The Flagged folder contains any content that you want to send back to the dev team for further review. 3. The Trash folder contains any content that you have deleted. This content will be permanently deleted after 30 days. ![[Pasted image 20221005201537.png]] Next up:[[2. Areas Personal/Technology and applied sciences/Computing/Computer science/Artificial Intelligence/Machine Learning/Jasper/Bootcamp/The Basics/Projects]]","title":"Ai outputs or content history"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/ai-templates/","text":"The 50+ templates that Jasper offers are simple to use, yet extremely versatile and useful for a variety of tasks. Each template is designed to perform a specific function, making it easy to find the right tool for the job at hand. With a little practice, you'll be able to get the most out of Jasper's templates and use them to their full potential. To access templates, click the \"Templates\" option on the left-hand sidebar, or click the \"Power mode\" icon when inside a document. ![[Pasted image 20221005200133.png]] Next up: [[Document Editor]]","title":"Ai templates"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/document-editor/","text":"Documents allow for more flexible content creation, similar to that of a Google Doc. They are available on the Boss Mode Plan. Docs should be used for writing longer content, like blog posts, emails, social posts, or books. You'll learn more about how to use docs in a later lesson. To access documents, click on Documents on the left-hand tab. Editor ![[Pasted image 20221005200807.png]] Documents list ![[Pasted image 20221005200909.png]] Next up: [[AI Outputs or Content History]]","title":"Document editor"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/document-editor/#editor","text":"![[Pasted image 20221005200807.png]]","title":"Editor"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/document-editor/#documents-list","text":"![[Pasted image 20221005200909.png]] Next up: [[AI Outputs or Content History]]","title":"Documents list"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/learn-the-basics-of-jasper/","text":"What You\u2019ll Learn in This Lesson In this lesson, you will learn about: The Dashboard Templates Documents AI Output History Projects Templates: Jasper's 50+ templates are designed to perform specific use cases and are easy to understand and use. Documents: The document can be used for longer content, such as blog posts, emails, social media posts, or books. AI Output History: The AI Output History is a record of all the content that Jasper has produced. This history is automatically saved and can be viewed in the AI Outputs tab. There is also a Favorites tab where you can save specific outputs that you like, and a Trash tab where outputs that you don't want are permanently deleted after 30 days. Projects: Projects can be used as folders for organizing your work. For example, you may want to create a new project for each client, or separate folders for blog posts, ads, etc.","title":"Learn the basics of jasper"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/learn-the-basics-of-jasper/#what-youll-learn-in-this-lesson","text":"In this lesson, you will learn about: The Dashboard Templates Documents AI Output History Projects Templates: Jasper's 50+ templates are designed to perform specific use cases and are easy to understand and use. Documents: The document can be used for longer content, such as blog posts, emails, social media posts, or books. AI Output History: The AI Output History is a record of all the content that Jasper has produced. This history is automatically saved and can be viewed in the AI Outputs tab. There is also a Favorites tab where you can save specific outputs that you like, and a Trash tab where outputs that you don't want are permanently deleted after 30 days. Projects: Projects can be used as folders for organizing your work. For example, you may want to create a new project for each client, or separate folders for blog posts, ads, etc.","title":"What You\u2019ll Learn in This Lesson"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/projects/","text":"Projects can be used to organize work by creating new projects for anything desired. Many users create new projects for each client or separate folders for blog posts, ads, etc. All users inside an account have access to all projects in that account. ![[Pasted image 20221005201659.png]] Create a new project: Click on the PROJECT dropdown in the bottom left of the dashboard Click the + icon at the top. Name the project Save If you accidentally create content in the wrong project, you can always move it to another project. Move content to a different project Navigate to Documents or AI outputs tab Click and drag to highlight the content you want to move Click \"Move\" Select the project name you want to move to Next up:[[How Jasper Thinks]]","title":"Projects"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/projects/#create-a-new-project","text":"Click on the PROJECT dropdown in the bottom left of the dashboard Click the + icon at the top. Name the project Save If you accidentally create content in the wrong project, you can always move it to another project.","title":"Create a new project:"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/projects/#move-content-to-a-different-project","text":"Navigate to Documents or AI outputs tab Click and drag to highlight the content you want to move Click \"Move\" Select the project name you want to move to Next up:[[How Jasper Thinks]]","title":"Move content to a different project"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/jasper/bootcamp/the-basics/the-dashboard/","text":"The app's dashboard will be your home base. From here, you can access all the app's features. To get to the dashboard, simply click Dashboard on the side navbar. ![[Pasted image 20221005195237.png]] The app has recently launched some new features and updated others. You can stay up to date on what's new by checking the app regularly. You can also access your favorite templates by pressing the star icon in the top left of the template. In addition, you can see how many words you've generated over the last 7 days, get quick links to the Facebook community, and earn free credits by leaving an honest review. Next up: [[AI Templates]]","title":"The dashboard"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/","text":"![[Jim_Fan_10Aug2022_19-19-1.png]] ![[55737.png]] Have you ever played a video game and thought about how the characters in the game move around and make decisions? Well, there are some special robots called \"autonomous agents\" that can do similar things in the real world! They can learn how to do different tasks by practicing, just like how you learn new things by practicing. But right now, these robots can only learn how to do a few specific things really well, like playing a certain video game or solving a certain puzzle. NVIDIA want to make robots that can learn to do lots of different things, just like how you can learn to do lots of different things in real life. To do this, they used a video game called Minecraft that has lots of different tasks for the robot to learn, like building a house or finding treasure. They also made a big library of information that the robot can learn from, like videos and tutorials. Then they made a special kind of robot that can learn from all of this information without anyone having to tell it what to do. It can understand what people are saying and use that information to learn how to do different tasks in Minecraft. The end goal is for other people to be able to use these autonomous agents to make even better robots in the future. It's like building a big toolbox full of tools that anyone can use to make something new and exciting! Generalist Agent In the field of artificial intelligence, researchers aim to develop machines that can learn and perform a wide range of tasks, similar to human beings. Such machines are called \"generalist agents\" and the research goals for creating them can be categorized into three main objectives. ![[3. Resources Personal/Images/Jim_Fan_10Aug2022_5-5-1.png]] Open-ended objectives: Firstly, a generalist agent should be given an \"open-ended\" objective, meaning that it should be capable of figuring out what it needs to do on its own, without being solely reliant on a predetermined set of instructions. Essentially, it is like providing a robot with a task and allowing it to complete it by itself. Massively multitask: Secondly, a generalist agent should be capable of performing multiple tasks simultaneously, a concept referred to as \"massively multitasking.\" Ideally, the agent should also be able to comprehend and execute commands that are given in natural language, much like how we give instructions to our friends. World knowledge: Lastly, the generalist agent should possess \"world knowledge,\" that is, a comprehensive understanding of how the world functions without requiring retraining each time. This would allow the agent to be equipped with a considerable amount of knowledge beforehand, minimizing the resources and time needed for it to learn and adapt. ![[Jim_Fan_10Aug2022_3-3-1 1.png]] In summary, the ultimate goal for developing a generalist agent is to design a machine that can learn on its own, perform a multitude of tasks simultaneously, and possess significant knowledge of the world. Minecraft Minecraft is a widely popular 3D open world game that offers players an opportunity to explore the game's virtual environment and create various structures. Currently, Minecraft has a large player base with over 140 million active players, which surpasses the population of Mexico in number. ![[Jim_Fan_10Aug2022_6-6-1.png]] MineDojo is a novel research framework for the exploration of embodied artificial intelligence (AI) agents, with the primary objective of facilitating research in the development of generalized agents through foundational models. This framework utilizes the popular game Minecraft as a simulation suite, featuring over 3,000 open-ended and language-prompted tasks, as well as a vast multimodal knowledge base. The three distinct techniques employed by MineDojo for enabling researchers to instruct a computer to perform various tasks include an open-ended environment, foundational models, and an internet-skill knowledge base. ![[Jim_Fan_10Aug2022_8-8-1.png]] The ultimate goal of the MineDojo framework is to produce machines capable of self-directed learning, simultaneous execution of numerous tasks, and a significant comprehension of the world, commonly referred to as generalist agents. This research framework, therefore, offers a promising avenue for researchers to delve into the creation of embodied AI agents and explore the application of foundational models in such agents' development. Open-ended environment ![[Jim_Fan_10Aug2022_9-9-1 2.png]] Foundation model ![[Jim_Fan_10Aug2022_28-28-1 1.png]] Internet-skill knowledge base ![[Jim_Fan_10Aug2022_17-17-1.png]] Open-ended environment ![[Jim_Fan_10Aug2022_9-9-1 2.png]] Versatile simulator: A versatile simulator in machine learning is like a pretend world that computers can use to practice doing things, just like you might use a play kitchen or a toy car to practice cooking or driving. The simulator can create many different situations, so the computer can learn how to do lots of different things. In the case of MineDojo, it uses a versatile simulator to create over 3,000 different tasks for the computer to practice on, which helps it get really good at those tasks. ![[Jim_Fan_10Aug2022_10-10-1.png]] Programmatic tasks: Out of the total 3,000 tasks, around half are programmatic tasks. These are tasks which have groundtruth success conditions. ![[Jim_Fan_10Aug2022_11-11-1.png]] Creative tasks: The other half are creative tasks, these are free-formed and open-ended. ![[Jim_Fan_10Aug2022_12-12-1.png]] Large pre-training model: In addition to the versatile simulator and foundation model, MineDojo also uses a large pre-training model that incorporates internet-scale data to improve the agent's ability to learn and adapt. This model leverages the vast amount of information available on the internet, including text, images, and videos, to teach the agent about a wide range of topics. Multitask learning: The pre-training model also allows the agent to learn multiple tasks simultaneously, a technique known as \"multitask learning.\" This means that the agent can learn to perform several different tasks at once, rather than focusing on one task at a time. This approach is more efficient and effective because it allows the agent to share common knowledge across tasks, leading to faster learning and better performance. Reinforcement learning: Finally, MineDojo uses reinforcement learning to fine-tune the agent's performance on specific tasks. Reinforcement learning is a type of machine learning that involves training the agent through trial and error, rewarding it when it performs well and punishing it when it performs poorly. This approach helps the agent improve its performance over time, making it better equipped to handle a wide range of tasks. Overall, MineDojo represents a major step forward in the development of embodied, generalized agents. By leveraging the power of Minecraft, a versatile simulator, a foundation model, a large pre-training model, multitask learning, and reinforcement learning, researchers are making significant progress toward creating machines that can learn and perform a wide range of tasks on their own. Foundation model Steps towards a generalist agent: ![[Jim_Fan_10Aug2022_28-28-1 1.png]] The team has developed MineCLIP, a contrastive video-language model that connects natural language subtitles to associated video segments. This model serves as a basic reward function for an AI agent that can learn a variety of skills to a certain degree. The team views MineCLIP as a promising tool for teaching AI agents, but they recognize that more work is needed to improve the technology. ![[Jim_Fan_10Aug2022_30-30-1.png]] It behaves similarly to how [[OpenAI's Clip]] learns to associate video and the text that describes the content. MineClip can then be repurposed to be a language conditioning model. In this way, it becomes integrated in the reward function for the agent. ![[Jim_Fan_10Aug2022_31-31-1.png]] Internet-scale knowledge base ![[Pasted image 20230219202653.png]] YouTube videos and transcripts Minecraft is one of the most streamed video games on YouTube. The NVIDIA team was able to collect more than 700,000 videos with two billion words in transcripts which provided rich learning material of human strategies and creativity. The time-aligned transcripts enable the agent to ground free-form natural language in video pixels and learn the semantics of diverse activities without laborious human labeling. ![[Jim_Fan_10Aug2022_20-20-1.png]] Minecraft Wiki There is a Minecraft specific Wikipedia that explains every entity and mechanism in the game. The NVIDIA team scraped 7,000 Wikipedia pages with interleaving multimodal data. Examples: Gallery of Minecraft monsters: ![[Jim_Fan_10Aug2022_21-21-1.png]] Crafting recipes: ![[Jim_Fan_10Aug2022_22-22-1.png]] ![[Jim_Fan_10Aug2022_23-23-1.png]] ![[Jim_Fan_10Aug2022_24-24-1.png]] Reddit The Minecraft subreddit is an active forum. Players showcase their creations and also ask questions for help. ![[Jim_Fan_10Aug2022_25-25-1.png]]![[Jim_Fan_10Aug2022_26-26-1.png]] ![[Jim_Fan_10Aug2022_27-27-1.png]]","title":"Minedojo   creating embodied ai agents with unlimited knowledge from the internet"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#generalist-agent","text":"In the field of artificial intelligence, researchers aim to develop machines that can learn and perform a wide range of tasks, similar to human beings. Such machines are called \"generalist agents\" and the research goals for creating them can be categorized into three main objectives. ![[3. Resources Personal/Images/Jim_Fan_10Aug2022_5-5-1.png]] Open-ended objectives: Firstly, a generalist agent should be given an \"open-ended\" objective, meaning that it should be capable of figuring out what it needs to do on its own, without being solely reliant on a predetermined set of instructions. Essentially, it is like providing a robot with a task and allowing it to complete it by itself. Massively multitask: Secondly, a generalist agent should be capable of performing multiple tasks simultaneously, a concept referred to as \"massively multitasking.\" Ideally, the agent should also be able to comprehend and execute commands that are given in natural language, much like how we give instructions to our friends. World knowledge: Lastly, the generalist agent should possess \"world knowledge,\" that is, a comprehensive understanding of how the world functions without requiring retraining each time. This would allow the agent to be equipped with a considerable amount of knowledge beforehand, minimizing the resources and time needed for it to learn and adapt. ![[Jim_Fan_10Aug2022_3-3-1 1.png]] In summary, the ultimate goal for developing a generalist agent is to design a machine that can learn on its own, perform a multitude of tasks simultaneously, and possess significant knowledge of the world.","title":"Generalist Agent"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#minecraft","text":"Minecraft is a widely popular 3D open world game that offers players an opportunity to explore the game's virtual environment and create various structures. Currently, Minecraft has a large player base with over 140 million active players, which surpasses the population of Mexico in number. ![[Jim_Fan_10Aug2022_6-6-1.png]] MineDojo is a novel research framework for the exploration of embodied artificial intelligence (AI) agents, with the primary objective of facilitating research in the development of generalized agents through foundational models. This framework utilizes the popular game Minecraft as a simulation suite, featuring over 3,000 open-ended and language-prompted tasks, as well as a vast multimodal knowledge base. The three distinct techniques employed by MineDojo for enabling researchers to instruct a computer to perform various tasks include an open-ended environment, foundational models, and an internet-skill knowledge base. ![[Jim_Fan_10Aug2022_8-8-1.png]] The ultimate goal of the MineDojo framework is to produce machines capable of self-directed learning, simultaneous execution of numerous tasks, and a significant comprehension of the world, commonly referred to as generalist agents. This research framework, therefore, offers a promising avenue for researchers to delve into the creation of embodied AI agents and explore the application of foundational models in such agents' development. Open-ended environment ![[Jim_Fan_10Aug2022_9-9-1 2.png]] Foundation model ![[Jim_Fan_10Aug2022_28-28-1 1.png]] Internet-skill knowledge base ![[Jim_Fan_10Aug2022_17-17-1.png]]","title":"Minecraft"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#open-ended-environment","text":"![[Jim_Fan_10Aug2022_9-9-1 2.png]]","title":"Open-ended environment"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#versatile-simulator","text":"A versatile simulator in machine learning is like a pretend world that computers can use to practice doing things, just like you might use a play kitchen or a toy car to practice cooking or driving. The simulator can create many different situations, so the computer can learn how to do lots of different things. In the case of MineDojo, it uses a versatile simulator to create over 3,000 different tasks for the computer to practice on, which helps it get really good at those tasks. ![[Jim_Fan_10Aug2022_10-10-1.png]]","title":"Versatile simulator:"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#programmatic-tasks","text":"Out of the total 3,000 tasks, around half are programmatic tasks. These are tasks which have groundtruth success conditions. ![[Jim_Fan_10Aug2022_11-11-1.png]]","title":"Programmatic tasks:"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#creative-tasks","text":"The other half are creative tasks, these are free-formed and open-ended. ![[Jim_Fan_10Aug2022_12-12-1.png]]","title":"Creative tasks:"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#large-pre-training-model","text":"In addition to the versatile simulator and foundation model, MineDojo also uses a large pre-training model that incorporates internet-scale data to improve the agent's ability to learn and adapt. This model leverages the vast amount of information available on the internet, including text, images, and videos, to teach the agent about a wide range of topics.","title":"Large pre-training model:"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#multitask-learning","text":"The pre-training model also allows the agent to learn multiple tasks simultaneously, a technique known as \"multitask learning.\" This means that the agent can learn to perform several different tasks at once, rather than focusing on one task at a time. This approach is more efficient and effective because it allows the agent to share common knowledge across tasks, leading to faster learning and better performance.","title":"Multitask learning:"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#reinforcement-learning","text":"Finally, MineDojo uses reinforcement learning to fine-tune the agent's performance on specific tasks. Reinforcement learning is a type of machine learning that involves training the agent through trial and error, rewarding it when it performs well and punishing it when it performs poorly. This approach helps the agent improve its performance over time, making it better equipped to handle a wide range of tasks. Overall, MineDojo represents a major step forward in the development of embodied, generalized agents. By leveraging the power of Minecraft, a versatile simulator, a foundation model, a large pre-training model, multitask learning, and reinforcement learning, researchers are making significant progress toward creating machines that can learn and perform a wide range of tasks on their own.","title":"Reinforcement learning:"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#foundation-model","text":"","title":"Foundation model"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#steps-towards-a-generalist-agent","text":"![[Jim_Fan_10Aug2022_28-28-1 1.png]] The team has developed MineCLIP, a contrastive video-language model that connects natural language subtitles to associated video segments. This model serves as a basic reward function for an AI agent that can learn a variety of skills to a certain degree. The team views MineCLIP as a promising tool for teaching AI agents, but they recognize that more work is needed to improve the technology. ![[Jim_Fan_10Aug2022_30-30-1.png]] It behaves similarly to how [[OpenAI's Clip]] learns to associate video and the text that describes the content. MineClip can then be repurposed to be a language conditioning model. In this way, it becomes integrated in the reward function for the agent. ![[Jim_Fan_10Aug2022_31-31-1.png]]","title":"Steps towards a generalist agent:"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#internet-scale-knowledge-base","text":"![[Pasted image 20230219202653.png]]","title":"Internet-scale knowledge base"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#youtube-videos-and-transcripts","text":"Minecraft is one of the most streamed video games on YouTube. The NVIDIA team was able to collect more than 700,000 videos with two billion words in transcripts which provided rich learning material of human strategies and creativity. The time-aligned transcripts enable the agent to ground free-form natural language in video pixels and learn the semantics of diverse activities without laborious human labeling. ![[Jim_Fan_10Aug2022_20-20-1.png]]","title":"YouTube videos and transcripts"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#minecraft-wiki","text":"There is a Minecraft specific Wikipedia that explains every entity and mechanism in the game. The NVIDIA team scraped 7,000 Wikipedia pages with interleaving multimodal data.","title":"Minecraft Wiki"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#examples","text":"Gallery of Minecraft monsters: ![[Jim_Fan_10Aug2022_21-21-1.png]] Crafting recipes: ![[Jim_Fan_10Aug2022_22-22-1.png]] ![[Jim_Fan_10Aug2022_23-23-1.png]] ![[Jim_Fan_10Aug2022_24-24-1.png]]","title":"Examples:"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/minedojo---creating-embodied-ai-agents-with-unlimited-knowledge-from-the-internet/#reddit","text":"The Minecraft subreddit is an active forum. Players showcase their creations and also ask questions for help. ![[Jim_Fan_10Aug2022_25-25-1.png]]![[Jim_Fan_10Aug2022_26-26-1.png]] ![[Jim_Fan_10Aug2022_27-27-1.png]]","title":"Reddit"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/usage-guide-for-minedojo----a-comprehensive-tutorial-on-utilizing-minedojo-platform/","text":"[[MineDojo - Creating Embodied AI Agents with Unlimited Knowledge from the Internet |MineDojo]] is a novel AI research framework that facilitates the development of embodied agents capable of open-ended learning. The framework comprises an extensive simulation suite built on Minecraft that offers a diverse set of tasks, and provides unrestricted access to an internet-scale knowledge base consisting of 730K YouTube videos, 7K Wiki pages, and 340K Reddit posts. With MineDojo, AI agents can engage in unconstrained exploration of a procedurally generated 3D world characterized by varied terrains, abundant mining resources, crafting tools, structural possibilities, and enigmatic wonders. Unlike traditional isolation-based learning, MineDojo enables agents to tap into the collective wisdom of millions of human players worldwide, thereby facilitating knowledge transfer and skill acquisition. Contents Installation Getting Started Benchmarking Suite Programmatic Tasks Creative Tasks Playthrough Tasks Installation To use MineDojo, you will need a computer with Python version 3.9 or later installed. MineDojo has been added on two types of computers: one that uses Ubuntu 20.04 and another that uses Mac OS X. Before you install MineDojo, you must first install some other programs that it needs to work properly. One of these programs is called JDK 8, which is necessary for running the Minecraft backend. To make sure that installing MineDojo does not affect other programs on your computer, it is recommend that you create a new Conda virtual environment to keep everything separate. If you find all of this too difficult, there is a pre-built Docker image that you can use instead. To install MineDojo, all you need to do is follow these simple steps: To install the stable version of MineDojo, you can use a program called \"pip\" by entering the following command: pip install minedojo This will download and install MineDojo onto your computer, allowing you to use it for your AI research. %%If you want to install the latest version of MineDojo directly from the main branch of the repository, you can do so by following these steps: Open a command prompt or terminal on your computer. Enter the command \"git clone https://github.com/MineDojo/MineDojo && cd MineDojo\" to download the MineDojo repository and navigate to its directory. Enter the command \"pip install -e .\" to install MineDojo onto your computer. These commands will allow you to download and install the latest version of MineDojo, which may include new features or improvements that are not yet available in the stable version. To ensure that MineDojo has been installed correctly on your computer, you can run the following script: python minedojo/scripts/validate_install.py Please note that the initial run of this script may take some time to compile the Java code. Once the compilation is complete, a Minecraft window will appear on your screen, featuring the same gaming interface that human players use. If everything has been installed correctly, you will see the message \"[INFO] Installation Success\".%%","title":"Usage guide for minedojo    a comprehensive tutorial on utilizing minedojo platform"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/usage-guide-for-minedojo----a-comprehensive-tutorial-on-utilizing-minedojo-platform/#contents","text":"Installation Getting Started Benchmarking Suite Programmatic Tasks Creative Tasks Playthrough Tasks","title":"Contents"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/minedojo/usage-guide-for-minedojo----a-comprehensive-tutorial-on-utilizing-minedojo-platform/#installation","text":"To use MineDojo, you will need a computer with Python version 3.9 or later installed. MineDojo has been added on two types of computers: one that uses Ubuntu 20.04 and another that uses Mac OS X. Before you install MineDojo, you must first install some other programs that it needs to work properly. One of these programs is called JDK 8, which is necessary for running the Minecraft backend. To make sure that installing MineDojo does not affect other programs on your computer, it is recommend that you create a new Conda virtual environment to keep everything separate. If you find all of this too difficult, there is a pre-built Docker image that you can use instead. To install MineDojo, all you need to do is follow these simple steps: To install the stable version of MineDojo, you can use a program called \"pip\" by entering the following command: pip install minedojo This will download and install MineDojo onto your computer, allowing you to use it for your AI research. %%If you want to install the latest version of MineDojo directly from the main branch of the repository, you can do so by following these steps: Open a command prompt or terminal on your computer. Enter the command \"git clone https://github.com/MineDojo/MineDojo && cd MineDojo\" to download the MineDojo repository and navigate to its directory. Enter the command \"pip install -e .\" to install MineDojo onto your computer. These commands will allow you to download and install the latest version of MineDojo, which may include new features or improvements that are not yet available in the stable version. To ensure that MineDojo has been installed correctly on your computer, you can run the following script: python minedojo/scripts/validate_install.py Please note that the initial run of this script may take some time to compile the Java code. Once the compilation is complete, a Minecraft window will appear on your screen, featuring the same gaming interface that human players use. If everything has been installed correctly, you will see the message \"[INFO] Installation Success\".%%","title":"Installation"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/open-ai%27s-gynasium/open_ai%27s_gynasium/","text":"Open AI's Gynasium","title":"Open AI's Gynasium"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/open-ai%27s-gynasium/open_ai%27s_gynasium/#open-ais-gynasium","text":"","title":"Open AI's Gynasium"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-chatgpt/202212181953-duel-arena-bot/","text":"chatgpt #dreambot Log in to your account on the Old School RuneScape (OSRS) game. Navigate to the Al-Kharid Duel Arena. Challenge another player to a duel. Accept any duel requests from other players. If you win the duel, teleport to the Edgeville bank and deposit your winnings. If you lose the duel, teleport to the Lumbridge castle and heal up at the kitchen. import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.wrappers.interactive.NPC ; @ScriptManifest ( author = \"Your Name\" , category = Category . DUEL_ARENA , name = \"Duel Arena Bot\" , version = 1.0 ) public class DuelArenaBot extends AbstractScript { private static final Area AL_KHARID_DUEL_ARENA = new Area ( 3312 , 3234 , 3295 , 3221 ); private static final Area LUMBRIDGE_CASTLE = new Area ( 3207 , 3217 , 3206 , 3218 ); private static final Area EDGEVILLE_BANK = new Area ( 3092 , 3491 , 3096 , 3488 ); @Override public void onStart () { log ( \"Starting Duel Arena bot\" ); } @Override public int onLoop () { // If we're not at the Duel Arena, walk there if ( ! getLocalPlayer (). isInArea ( AL_KHARID_DUEL_ARENA )) { getWalking (). walk ( AL_KHARID_DUEL_ARENA . getRandomTile ()); sleepUntil (() -> getLocalPlayer (). isInArea ( AL_KHARID_DUEL_ARENA ), Calculations . random ( 4800 , 5400 )); } else { // Look for a player to challenge NPC playerToChallenge = getNpcs (). closest ( npc -> npc . getName () != null && npc . getName (). equals ( \"Player\" )); if ( playerToChallenge != null && playerToChallenge . interact ( \"Challenge\" )) { sleepUntil (() -> getDialogues (). inDialogue (), Calculations . random ( 4800 , 5400 )); // Confirm the challenge getDialogues (). clickContinue (); sleepUntil (() -> getDueling (). isDueling (), Calculations . random ( 4800 , 5400 )); } // Accept any incoming challenges if ( getDueling (). isDueling ()) { if ( getDueling (). isChallenger ()) { getDueling (). accept (); } if ( getDueling (). isBeingChallenged ()) { getDueling (). accept (); } } // If the duel is over if ( getDueling (). isDueling ()) { // If we won the duel if ( getSkills (). getRealLevel ( Skill . HITPOINTS ) > 0 ) { // Teleport to Edgeville bank and deposit winnings getInventory (). get ( \"Coins\" ). interact ( \"Use\" ); getGameObjects (). closest ( \"Bank booth\" ). interact ( \"Use\" ); sleepUntil (() -> getBank (). isOpen (), Calculations . random ( 4800 , 5400 )); getBank (). depositAll (); getBank (). close (); } else { // Teleport to Lumbridge castle and heal up getInventory (). get ( \"Varrock teleport\" ). interact ( \"Break\" ); sleepUntil (() -> getLocalPlayer (). isInArea ( LUMBRIDGE_CASTLE ), Calculations . random ( 4800 , 5400 )); getGameObjects (). closest ( \"Range\" ). interact ( \"Cook\" ); sleepUntil (() -> getSkills (). getRealLevel ( Skill . HITPOINTS ) > 6 , Calculations . random ( 4800 , 5400 )); } } } return Calculations . random ( 600 , 1200 ); } @Override public void onExit () { log ( \"Stopping Duel Arena bot\" ); } } This script will continuously challenge other players to duels at the Al-Kharid Duel Arena, and accept any incoming challenges. If you win the duel, it will teleport you to the Edgeville bank and deposit your winnings. If you lose the duel, it will teleport you to the Lumbridge castle and heal up at the kitchen.","title":"chatgpt  #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-chatgpt/202212181953-duel-arena-bot/#chatgpt-dreambot","text":"Log in to your account on the Old School RuneScape (OSRS) game. Navigate to the Al-Kharid Duel Arena. Challenge another player to a duel. Accept any duel requests from other players. If you win the duel, teleport to the Edgeville bank and deposit your winnings. If you lose the duel, teleport to the Lumbridge castle and heal up at the kitchen. import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.wrappers.interactive.NPC ; @ScriptManifest ( author = \"Your Name\" , category = Category . DUEL_ARENA , name = \"Duel Arena Bot\" , version = 1.0 ) public class DuelArenaBot extends AbstractScript { private static final Area AL_KHARID_DUEL_ARENA = new Area ( 3312 , 3234 , 3295 , 3221 ); private static final Area LUMBRIDGE_CASTLE = new Area ( 3207 , 3217 , 3206 , 3218 ); private static final Area EDGEVILLE_BANK = new Area ( 3092 , 3491 , 3096 , 3488 ); @Override public void onStart () { log ( \"Starting Duel Arena bot\" ); } @Override public int onLoop () { // If we're not at the Duel Arena, walk there if ( ! getLocalPlayer (). isInArea ( AL_KHARID_DUEL_ARENA )) { getWalking (). walk ( AL_KHARID_DUEL_ARENA . getRandomTile ()); sleepUntil (() -> getLocalPlayer (). isInArea ( AL_KHARID_DUEL_ARENA ), Calculations . random ( 4800 , 5400 )); } else { // Look for a player to challenge NPC playerToChallenge = getNpcs (). closest ( npc -> npc . getName () != null && npc . getName (). equals ( \"Player\" )); if ( playerToChallenge != null && playerToChallenge . interact ( \"Challenge\" )) { sleepUntil (() -> getDialogues (). inDialogue (), Calculations . random ( 4800 , 5400 )); // Confirm the challenge getDialogues (). clickContinue (); sleepUntil (() -> getDueling (). isDueling (), Calculations . random ( 4800 , 5400 )); } // Accept any incoming challenges if ( getDueling (). isDueling ()) { if ( getDueling (). isChallenger ()) { getDueling (). accept (); } if ( getDueling (). isBeingChallenged ()) { getDueling (). accept (); } } // If the duel is over if ( getDueling (). isDueling ()) { // If we won the duel if ( getSkills (). getRealLevel ( Skill . HITPOINTS ) > 0 ) { // Teleport to Edgeville bank and deposit winnings getInventory (). get ( \"Coins\" ). interact ( \"Use\" ); getGameObjects (). closest ( \"Bank booth\" ). interact ( \"Use\" ); sleepUntil (() -> getBank (). isOpen (), Calculations . random ( 4800 , 5400 )); getBank (). depositAll (); getBank (). close (); } else { // Teleport to Lumbridge castle and heal up getInventory (). get ( \"Varrock teleport\" ). interact ( \"Break\" ); sleepUntil (() -> getLocalPlayer (). isInArea ( LUMBRIDGE_CASTLE ), Calculations . random ( 4800 , 5400 )); getGameObjects (). closest ( \"Range\" ). interact ( \"Cook\" ); sleepUntil (() -> getSkills (). getRealLevel ( Skill . HITPOINTS ) > 6 , Calculations . random ( 4800 , 5400 )); } } } return Calculations . random ( 600 , 1200 ); } @Override public void onExit () { log ( \"Stopping Duel Arena bot\" ); } } This script will continuously challenge other players to duels at the Al-Kharid Duel Arena, and accept any incoming challenges. If you win the duel, it will teleport you to the Edgeville bank and deposit your winnings. If you lose the duel, it will teleport you to the Lumbridge castle and heal up at the kitchen.","title":"chatgpt  #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-chatgpt/202212181959-air-orbs-bot/","text":"chatgpt #dreambot Certainly! Here is a simple Dreambot script written in Java that will perform the following actions: Log in to your account on the Old School RuneScape (OSRS) game. Navigate to the Al-Kharid mine. Mine some iron ore. Use the iron ore to craft some iron bars. Use the iron bars to craft some air orbs spells. Cast the air orb spells at an anvil to create air orbs runescape items. Here is the script: import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.wrappers.items.Item ; @ScriptManifest ( author = \"Your Name\" , category = Category . MINING , name = \"Air Orbs Bot\" , version = 1.0 ) public class AirOrbsBot extends AbstractScript { private static final Area AL_KHARID_MINE = new Area ( 3269 , 3167 , 3290 , 3145 ); @Override public void onStart () { log ( \"Starting Air Orbs bot\" ); } @Override public int onLoop () { // If we're not at the Al-Kharid mine, walk there if ( ! getLocalPlayer (). isInArea ( AL_KHARID_MINE )) { getWalking (). walk ( AL_KHARID_MINE . getRandomTile ()); sleepUntil (() -> getLocalPlayer (). isInArea ( AL_KHARID_MINE ), Calculations . random ( 4800 , 5400 )); } else { // Mine some iron ore if ( ! getInventory (). isFull ()) { getObjects (). closest ( \"Iron ore rock\" ). interact ( \"Mine\" ); sleepUntil (() -> getInventory (). isFull () || ! getLocalPlayer (). isAnimating (), Calculations . random ( 4800 , 5400 )); } // Use the iron ore to craft iron bars Item ironOre = getInventory (). get ( \"Iron ore\" ); if ( ironOre != null ) { ironOre . interact ( \"Use\" ); getGameObjects (). closest ( \"Furnace\" ). interact ( \"Use\" ); sleepUntil (() -> getInventory (). contains ( \"Iron bar\" ), Calculations . random ( 4800 , 5400 )); } // Use the iron bars to craft air orb spells Item ironBar = getInventory (). get ( \"Iron bar\" ); if ( ironBar != null ) { ironBar . interact ( \"Use\" ); getGameObjects (). closest ( \"Anvil\" ). interact ( \"Use\" ); sleepUntil (() -> getInventory (). contains ( \"Air orb\" ), Calculations . random ( 4800 , 5400 )); } // Cast the air orb spells at an anvil to create air orb runescape items Item airOrb = getInventory (). get ( \"Air orb\" ); if ( airOrb != null ) { airOrb . interact ( \"Cast\" ); getGameObjects (). closest ( \"Anvil\" ). interact ( \"Use\" ); sleepUntil (() -> getInventory (). contains ( \"Air orb (unpowered)\" ), Calculations . random ( 4800 , 5400 )); } } return Calculations . random ( 600 , 1200 ); } @Override public void onExit () { log ( \"Stopping Air Orbs bot\" ); } } This script will continuously mine iron ore at the Al-Kharid mine, use the iron ore to craft iron bars, use the iron bars to craft air orb spells, and cast the air orb spells at an anvil to create air orb runescape items. I hope this helps! Let me know if you have any questions or need further assistance.","title":"chatgpt #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-chatgpt/202212181959-air-orbs-bot/#chatgpt-dreambot","text":"Certainly! Here is a simple Dreambot script written in Java that will perform the following actions: Log in to your account on the Old School RuneScape (OSRS) game. Navigate to the Al-Kharid mine. Mine some iron ore. Use the iron ore to craft some iron bars. Use the iron bars to craft some air orbs spells. Cast the air orb spells at an anvil to create air orbs runescape items. Here is the script: import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.wrappers.items.Item ; @ScriptManifest ( author = \"Your Name\" , category = Category . MINING , name = \"Air Orbs Bot\" , version = 1.0 ) public class AirOrbsBot extends AbstractScript { private static final Area AL_KHARID_MINE = new Area ( 3269 , 3167 , 3290 , 3145 ); @Override public void onStart () { log ( \"Starting Air Orbs bot\" ); } @Override public int onLoop () { // If we're not at the Al-Kharid mine, walk there if ( ! getLocalPlayer (). isInArea ( AL_KHARID_MINE )) { getWalking (). walk ( AL_KHARID_MINE . getRandomTile ()); sleepUntil (() -> getLocalPlayer (). isInArea ( AL_KHARID_MINE ), Calculations . random ( 4800 , 5400 )); } else { // Mine some iron ore if ( ! getInventory (). isFull ()) { getObjects (). closest ( \"Iron ore rock\" ). interact ( \"Mine\" ); sleepUntil (() -> getInventory (). isFull () || ! getLocalPlayer (). isAnimating (), Calculations . random ( 4800 , 5400 )); } // Use the iron ore to craft iron bars Item ironOre = getInventory (). get ( \"Iron ore\" ); if ( ironOre != null ) { ironOre . interact ( \"Use\" ); getGameObjects (). closest ( \"Furnace\" ). interact ( \"Use\" ); sleepUntil (() -> getInventory (). contains ( \"Iron bar\" ), Calculations . random ( 4800 , 5400 )); } // Use the iron bars to craft air orb spells Item ironBar = getInventory (). get ( \"Iron bar\" ); if ( ironBar != null ) { ironBar . interact ( \"Use\" ); getGameObjects (). closest ( \"Anvil\" ). interact ( \"Use\" ); sleepUntil (() -> getInventory (). contains ( \"Air orb\" ), Calculations . random ( 4800 , 5400 )); } // Cast the air orb spells at an anvil to create air orb runescape items Item airOrb = getInventory (). get ( \"Air orb\" ); if ( airOrb != null ) { airOrb . interact ( \"Cast\" ); getGameObjects (). closest ( \"Anvil\" ). interact ( \"Use\" ); sleepUntil (() -> getInventory (). contains ( \"Air orb (unpowered)\" ), Calculations . random ( 4800 , 5400 )); } } return Calculations . random ( 600 , 1200 ); } @Override public void onExit () { log ( \"Stopping Air Orbs bot\" ); } } This script will continuously mine iron ore at the Al-Kharid mine, use the iron ore to craft iron bars, use the iron bars to craft air orb spells, and cast the air orb spells at an anvil to create air orb runescape items. I hope this helps! Let me know if you have any questions or need further assistance.","title":"chatgpt #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-chatgpt/202212182004-dreambot-template/","text":"chatgpt #dreambot Certainly! Here is a simple template for a Dreambot script written in Java: import org.dreambot.api.methods.Calculations ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; @ScriptManifest ( author = \"Your Name\" , category = Category . OTHER , name = \"Script Name\" , version = 1.0 ) public class ScriptTemplate extends AbstractScript { @Override public void onStart () { log ( \"Starting Script Name\" ); } @Override public int onLoop () { // Your code goes here return Calculations . random ( 600 , 1200 ); } @Override public void onExit () { log ( \"Stopping Script Name\" ); } } This template includes the basic structure of a Dreambot script, including the onStart() method which is called when the script is first started, the onLoop() method which is called repeatedly while the script is running, and the onExit() method which is called when the script is stopped. To use this template, you will need to fill in the Category and name fields in the ScriptManifest annotation at the top of the script with the appropriate values for your script. You will also need to add your code to the onLoop() method, replacing the placeholder // Your code goes here with your own code. I hope this helps! Let me know if you have any questions or need further assistance.","title":"chatgpt #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-chatgpt/202212182004-dreambot-template/#chatgpt-dreambot","text":"Certainly! Here is a simple template for a Dreambot script written in Java: import org.dreambot.api.methods.Calculations ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; @ScriptManifest ( author = \"Your Name\" , category = Category . OTHER , name = \"Script Name\" , version = 1.0 ) public class ScriptTemplate extends AbstractScript { @Override public void onStart () { log ( \"Starting Script Name\" ); } @Override public int onLoop () { // Your code goes here return Calculations . random ( 600 , 1200 ); } @Override public void onExit () { log ( \"Stopping Script Name\" ); } } This template includes the basic structure of a Dreambot script, including the onStart() method which is called when the script is first started, the onLoop() method which is called repeatedly while the script is running, and the onExit() method which is called when the script is stopped. To use this template, you will need to fill in the Category and name fields in the ScriptManifest annotation at the top of the script with the appropriate values for your script. You will also need to add your code to the onLoop() method, replacing the placeholder // Your code goes here with your own code. I hope this helps! Let me know if you have any questions or need further assistance.","title":"chatgpt #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-chatgpt/202212182009-zulrah/","text":"chatgpt #dreambot Certainly! Here is a simple Dreambot script written in Java that will perform the following actions: Log in to your account on the Old School RuneScape (OSRS) game. Navigate to Zulrah's shrine. Fight Zulrah until it is defeated or until you die. If Zulrah is defeated, teleport to the Edgeville bank and deposit your loot. If you die, teleport to the Lumbridge graveyard and bury your bones. Repeat steps 2-5 until the script is stopped. Please note that this script is intended to be a basic example and may not be optimal for all players. It is recommended that you customize the script to fit your specific needs and playstyle. Here is the script: import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.wrappers.interactive.NPC ; @ScriptManifest ( author = \"Your Name\" , category = Category . BOSSES , name = \"Zulrah Bot\" , version = 1.0 ) public class ZulrahBot extends AbstractScript { private static final Area ZULRAH_SHRINE = new Area ( 2199 , 3067 , 2205 , 3063 ); private static final Area EDGEVILLE_BANK = new Area ( 3092 , 3491 , 3096 , 3488 ); private static final Area LUMBRIDGE_GRAVEYARD = new Area ( 3222 , 3218 , 3226 , 3214 ); @Override public void onStart () { log ( \"Starting Zulrah bot\" ); } @Override public int onLoop () { // If we're not at Zulrah's shrine, walk there if ( ! getLocalPlayer (). isInArea ( ZULRAH_SHRINE )) { getWalking (). walk ( ZULRAH_SHRINE . getRandomTile ()); sleepUntil (() -> getLocalPlayer (). isInArea ( ZULRAH_SHRINE ), Calculations . random ( 4800 , 5400 )); } else { // Fight Zulrah until it is defeated or until we die NPC zulrah = getNpcs (). closest ( \"Zulrah\" ); if ( zulrah != null ) { zulrah . interact ( \"Attack\" ); sleepUntil (() -> ! getLocalPlayer (). isAnimating () || ! zulrah . exists (), Calculations . random ( 4800 , 5400 )); } // If Zulrah is defeated, teleport to Edgeville bank and deposit loot if ( ! zulrah . exists ()) { getInventory (). get ( \"Ring of dueling\" ). interact ( \"Rub\" ); sleepUntil (() -> getLocalPlayer (). isInArea ( EDGEVILLE_BANK ), Calculations . random ( 4800 , 5400 )); getBank (). depositAll (); getBank (). close (); } // If we died, teleport to Lumbridge graveyard and bury bones if ( getSkills (). getRealLevel ( Skill . HITPOINTS ) == 0 ) { getInventory (). get ( \"Ring of dueling\" ). interact ( \"Rub\" ); sleepUntil (() -> getLocalPlayer (). isInArea ( LUMBRIDGE_GRAVEYARD ), Calculations . random ( 4800 , 5400 )); getInventory (). get ( \"Bones\" ). interact ( \"Bury\" ); sleepUntil (() -> ! getInventory (). contains ( \"Bones\" ), Calculations . random ( 4800 , 5400 )); } } return Calculations . random ( 600 , 1200 ); } @Override public void onExit () { log ( \"Stopping Zulrah bot\" ); } } This script will continuously navigate to Zulrah's shrine, fight Zulrah until it is defeated or until you die, and if Zulrah is defeated, teleport to the Edgeville bank and deposit your loot. If you die, it will teleport to the Lumbridge graveyard and bury your bones. It will repeat this process until the script is stopped. I hope this helps! Let me know if you have any questions or need further assistance.","title":"chatgpt #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-chatgpt/202212182009-zulrah/#chatgpt-dreambot","text":"Certainly! Here is a simple Dreambot script written in Java that will perform the following actions: Log in to your account on the Old School RuneScape (OSRS) game. Navigate to Zulrah's shrine. Fight Zulrah until it is defeated or until you die. If Zulrah is defeated, teleport to the Edgeville bank and deposit your loot. If you die, teleport to the Lumbridge graveyard and bury your bones. Repeat steps 2-5 until the script is stopped. Please note that this script is intended to be a basic example and may not be optimal for all players. It is recommended that you customize the script to fit your specific needs and playstyle. Here is the script: import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.wrappers.interactive.NPC ; @ScriptManifest ( author = \"Your Name\" , category = Category . BOSSES , name = \"Zulrah Bot\" , version = 1.0 ) public class ZulrahBot extends AbstractScript { private static final Area ZULRAH_SHRINE = new Area ( 2199 , 3067 , 2205 , 3063 ); private static final Area EDGEVILLE_BANK = new Area ( 3092 , 3491 , 3096 , 3488 ); private static final Area LUMBRIDGE_GRAVEYARD = new Area ( 3222 , 3218 , 3226 , 3214 ); @Override public void onStart () { log ( \"Starting Zulrah bot\" ); } @Override public int onLoop () { // If we're not at Zulrah's shrine, walk there if ( ! getLocalPlayer (). isInArea ( ZULRAH_SHRINE )) { getWalking (). walk ( ZULRAH_SHRINE . getRandomTile ()); sleepUntil (() -> getLocalPlayer (). isInArea ( ZULRAH_SHRINE ), Calculations . random ( 4800 , 5400 )); } else { // Fight Zulrah until it is defeated or until we die NPC zulrah = getNpcs (). closest ( \"Zulrah\" ); if ( zulrah != null ) { zulrah . interact ( \"Attack\" ); sleepUntil (() -> ! getLocalPlayer (). isAnimating () || ! zulrah . exists (), Calculations . random ( 4800 , 5400 )); } // If Zulrah is defeated, teleport to Edgeville bank and deposit loot if ( ! zulrah . exists ()) { getInventory (). get ( \"Ring of dueling\" ). interact ( \"Rub\" ); sleepUntil (() -> getLocalPlayer (). isInArea ( EDGEVILLE_BANK ), Calculations . random ( 4800 , 5400 )); getBank (). depositAll (); getBank (). close (); } // If we died, teleport to Lumbridge graveyard and bury bones if ( getSkills (). getRealLevel ( Skill . HITPOINTS ) == 0 ) { getInventory (). get ( \"Ring of dueling\" ). interact ( \"Rub\" ); sleepUntil (() -> getLocalPlayer (). isInArea ( LUMBRIDGE_GRAVEYARD ), Calculations . random ( 4800 , 5400 )); getInventory (). get ( \"Bones\" ). interact ( \"Bury\" ); sleepUntil (() -> ! getInventory (). contains ( \"Bones\" ), Calculations . random ( 4800 , 5400 )); } } return Calculations . random ( 600 , 1200 ); } @Override public void onExit () { log ( \"Stopping Zulrah bot\" ); } } This script will continuously navigate to Zulrah's shrine, fight Zulrah until it is defeated or until you die, and if Zulrah is defeated, teleport to the Edgeville bank and deposit your loot. If you die, it will teleport to the Lumbridge graveyard and bury your bones. It will repeat this process until the script is stopped. I hope this helps! Let me know if you have any questions or need further assistance.","title":"chatgpt #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/20221112011642-zulrah/","text":"codex #dreambot //The script should: //1. Log in to the game. //2. Go to the Zulrah boss. //3. Kill the boss. //4. Repeat. //The script should be able to: //1. Handle the different phases of the boss. //2. Handle the different attacks of the boss. //3. Handle the different attacks of the player. //4. Handle the different items in the inventory. //5. Handle the different items on the ground. //6. Handle the different items in the bank. //The script should be able to: //1. Use the correct prayer. //2. Use the correct potion. //3. Use the correct food. //4. Use the correct weapon. //5. Use the correct attack style. //6. Use the correct special attack. //The script should be able to: //1. Loot the items. //2. Bank the items. //3. Sell the items. import java.awt.Color ; import java.awt.Graphics ; import java.awt.Graphics2D ; import java.awt.Image ; import java.awt.Point ; import java.awt.Rectangle ; import java.awt.event.KeyEvent ; import java.awt.event.MouseEvent ; import java.awt.image.BufferedImage ; import java.io.File ; import java.io.IOException ; import java.util.ArrayList ; import java.util.Arrays ; import java.util.Collections ; import java.util.List ; import java.util.Random ; import javax.imageio.ImageIO ; import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.container.impl.bank.BankLocation ; import org.dreambot.api.methods.container.impl.bank.BankMode ; import org.dreambot.api.methods.container.impl.bank.BankType ; import org.dreambot.api.methods.container.impl.equipment.EquipmentSlot ; import org.dreambot.api.methods.filter.Filter ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.map.Tile ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.methods.tabs.Tab ; import org.dreambot.api.methods.walking.path.impl.LocalPath ; import org.dreambot.api.methods.walking.path.impl.LocalPath.Step ; import org.dreambot.api.methods.walking.pathfinding.impl.web.node.Node ; import org.dreambot.api.methods.world.World ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.script.listener.MessageListener ; import org.dreambot.api.script.listener.PaintListener ; import org.dreambot.api.utilities.Timer ; import org.dreambot.api.utilities.impl.Condition ; import org.dreambot.api.wrappers.interactive.GameObject ; import org.dreambot.api.wrappers.interactive.NPC ; import org.dreambot.api.wrappers.items.GroundItem ; import org.dreambot.api.wrappers.items.Item ; import org.dreambot.api.wrappers.widgets.WidgetChild ; import org.dreambot.api.wrappers.widgets.message.Message ; @ScriptManifest ( author = \"Code a Dream\" , category = Category . COMBAT , name = \"Zulrah\" , version = 1.0 ) public class Zulrah extends AbstractScript implements PaintListener , MessageListener { private final int [] ZULRAH_IDS = { 2042 , 2043 , 2044 }; private final int [] ZULRAH_IDS_2 = { 2042 , 2043 , 2044 , 2045 }; private final int [] ZULRAH_IDS_3 = { 2042 , 2043 , 2044 , 2045 , 2046 }; private final int [] ZULRAH_IDS_4 = { 2042 , 2043 , 2044 , 2045 , 2046 , 2047 };","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/20221112011642-zulrah/#codex-dreambot","text":"//The script should: //1. Log in to the game. //2. Go to the Zulrah boss. //3. Kill the boss. //4. Repeat. //The script should be able to: //1. Handle the different phases of the boss. //2. Handle the different attacks of the boss. //3. Handle the different attacks of the player. //4. Handle the different items in the inventory. //5. Handle the different items on the ground. //6. Handle the different items in the bank. //The script should be able to: //1. Use the correct prayer. //2. Use the correct potion. //3. Use the correct food. //4. Use the correct weapon. //5. Use the correct attack style. //6. Use the correct special attack. //The script should be able to: //1. Loot the items. //2. Bank the items. //3. Sell the items. import java.awt.Color ; import java.awt.Graphics ; import java.awt.Graphics2D ; import java.awt.Image ; import java.awt.Point ; import java.awt.Rectangle ; import java.awt.event.KeyEvent ; import java.awt.event.MouseEvent ; import java.awt.image.BufferedImage ; import java.io.File ; import java.io.IOException ; import java.util.ArrayList ; import java.util.Arrays ; import java.util.Collections ; import java.util.List ; import java.util.Random ; import javax.imageio.ImageIO ; import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.container.impl.bank.BankLocation ; import org.dreambot.api.methods.container.impl.bank.BankMode ; import org.dreambot.api.methods.container.impl.bank.BankType ; import org.dreambot.api.methods.container.impl.equipment.EquipmentSlot ; import org.dreambot.api.methods.filter.Filter ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.map.Tile ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.methods.tabs.Tab ; import org.dreambot.api.methods.walking.path.impl.LocalPath ; import org.dreambot.api.methods.walking.path.impl.LocalPath.Step ; import org.dreambot.api.methods.walking.pathfinding.impl.web.node.Node ; import org.dreambot.api.methods.world.World ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.script.listener.MessageListener ; import org.dreambot.api.script.listener.PaintListener ; import org.dreambot.api.utilities.Timer ; import org.dreambot.api.utilities.impl.Condition ; import org.dreambot.api.wrappers.interactive.GameObject ; import org.dreambot.api.wrappers.interactive.NPC ; import org.dreambot.api.wrappers.items.GroundItem ; import org.dreambot.api.wrappers.items.Item ; import org.dreambot.api.wrappers.widgets.WidgetChild ; import org.dreambot.api.wrappers.widgets.message.Message ; @ScriptManifest ( author = \"Code a Dream\" , category = Category . COMBAT , name = \"Zulrah\" , version = 1.0 ) public class Zulrah extends AbstractScript implements PaintListener , MessageListener { private final int [] ZULRAH_IDS = { 2042 , 2043 , 2044 }; private final int [] ZULRAH_IDS_2 = { 2042 , 2043 , 2044 , 2045 }; private final int [] ZULRAH_IDS_3 = { 2042 , 2043 , 2044 , 2045 , 2046 }; private final int [] ZULRAH_IDS_4 = { 2042 , 2043 , 2044 , 2045 , 2046 , 2047 };","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/20221112012123-dylans-hill-giant-killer/","text":"codex #dreambot import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.container.impl.bank.BankLocation ; import org.dreambot.api.methods.container.impl.equipment.EquipmentSlot ; import org.dreambot.api.methods.filter.Filter ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.map.Tile ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.methods.tabs.Tab ; import org.dreambot.api.methods.walking.path.impl.LocalPath ; import org.dreambot.api.methods.walking.path.impl.WalkingPath ; import org.dreambot.api.methods.widget.Widget ; import org.dreambot.api.methods.world.World ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.wrappers.interactive.GameObject ; import org.dreambot.api.wrappers.interactive.NPC ; import org.dreambot.api.wrappers.items.GroundItem ; import org.dreambot.api.wrappers.items.Item ; import org.dreambot.api.wrappers.widgets.WidgetChild ; import java.awt.* ; import java.util.ArrayList ; import java.util.List ; @ScriptManifest ( author = \"Dylan\" , category = Category . COMBAT , description = \"Kills Hill Giants\" , name = \"Dylans Hill Giant Killer\" , version = 1.0 ) public class Main extends AbstractScript { private final Area hillGiantArea = new Area ( 3111 , 9850 , 3120 , 9840 ); private final Area bankArea = new Area ( 3093 , 3491 , 3097 , 3487 ); private final Area pathToBank = new Area ( 3111 , 9850 , 3120 , 9840 ); private final Area pathToHillGiants = new Area ( 3093 , 3491 , 3097 , 3487 ); private final Filter < NPC > hillGiantFilter = new Filter < NPC > () { @Override public boolean match ( NPC npc ) { return npc != null && npc . getName (). equals ( \"Hill Giant\" ) && npc . getHealthPercent () > 0 ; } }; private final Filter < GroundItem > foodFilter = new Filter < GroundItem > () { @Override public boolean match ( GroundItem groundItem ) { return groundItem != null && groundItem . getName (). equals ( \"Lobster\" ) && groundItem . distance () < 10 ; } }; private final Filter < GameObject > doorFilter = new Filter < GameObject > () { @Override public boolean match ( GameObject gameObject ) { return gameObject != null && gameObject . getName (). equals ( \"Door\" ) && gameObject . distance () < 10 ; } }; private final Filter < Item > foodInInventoryFilter = new Filter < Item > () { @Override public boolean match ( Item item ) { return item != null && item . getName (). equals ( \"Lobster\" ) && item . getAmount () > 0 ; } }; private final Filter < Item > foodInBankFilter = new Filter < Item > () { @Override public boolean match ( Item item ) { return item != null && item . getName (). equals ( \"Lobster\" ) && item . getAmount () > 0 ; } }; private final Filter < Item > foodInEquipmentFilter = new Filter < Item > () { @Override public boolean match ( Item item ) { return item != null && item . getName (). equals ( \"Lobster\" ) && item . getAmount () > 0 ; } };","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/20221112012123-dylans-hill-giant-killer/#codex-dreambot","text":"import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.container.impl.bank.BankLocation ; import org.dreambot.api.methods.container.impl.equipment.EquipmentSlot ; import org.dreambot.api.methods.filter.Filter ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.map.Tile ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.methods.tabs.Tab ; import org.dreambot.api.methods.walking.path.impl.LocalPath ; import org.dreambot.api.methods.walking.path.impl.WalkingPath ; import org.dreambot.api.methods.widget.Widget ; import org.dreambot.api.methods.world.World ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.wrappers.interactive.GameObject ; import org.dreambot.api.wrappers.interactive.NPC ; import org.dreambot.api.wrappers.items.GroundItem ; import org.dreambot.api.wrappers.items.Item ; import org.dreambot.api.wrappers.widgets.WidgetChild ; import java.awt.* ; import java.util.ArrayList ; import java.util.List ; @ScriptManifest ( author = \"Dylan\" , category = Category . COMBAT , description = \"Kills Hill Giants\" , name = \"Dylans Hill Giant Killer\" , version = 1.0 ) public class Main extends AbstractScript { private final Area hillGiantArea = new Area ( 3111 , 9850 , 3120 , 9840 ); private final Area bankArea = new Area ( 3093 , 3491 , 3097 , 3487 ); private final Area pathToBank = new Area ( 3111 , 9850 , 3120 , 9840 ); private final Area pathToHillGiants = new Area ( 3093 , 3491 , 3097 , 3487 ); private final Filter < NPC > hillGiantFilter = new Filter < NPC > () { @Override public boolean match ( NPC npc ) { return npc != null && npc . getName (). equals ( \"Hill Giant\" ) && npc . getHealthPercent () > 0 ; } }; private final Filter < GroundItem > foodFilter = new Filter < GroundItem > () { @Override public boolean match ( GroundItem groundItem ) { return groundItem != null && groundItem . getName (). equals ( \"Lobster\" ) && groundItem . distance () < 10 ; } }; private final Filter < GameObject > doorFilter = new Filter < GameObject > () { @Override public boolean match ( GameObject gameObject ) { return gameObject != null && gameObject . getName (). equals ( \"Door\" ) && gameObject . distance () < 10 ; } }; private final Filter < Item > foodInInventoryFilter = new Filter < Item > () { @Override public boolean match ( Item item ) { return item != null && item . getName (). equals ( \"Lobster\" ) && item . getAmount () > 0 ; } }; private final Filter < Item > foodInBankFilter = new Filter < Item > () { @Override public boolean match ( Item item ) { return item != null && item . getName (). equals ( \"Lobster\" ) && item . getAmount () > 0 ; } }; private final Filter < Item > foodInEquipmentFilter = new Filter < Item > () { @Override public boolean match ( Item item ) { return item != null && item . getName (). equals ( \"Lobster\" ) && item . getAmount () > 0 ; } };","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/20221112014113-fishingbot/","text":"codex #dreambot //This bot is capable of doing Fishing wearing your worn items and equipping the items you choose to. //The bot will also equip the bait you choose to use. using System ; using System . Collections . Generic ; using System . Linq ; using System . Text ; using System . Threading . Tasks ; using DreamBot . API ; using DreamBot . API . Objects ; using DreamBot . API . Objects . Items ; using DreamBot . API . Objects . Items . Equipment ; using DreamBot . API . Objects . Items . Equipment . Bait ; public class FishingBot : Bot { public FishingBot () { //Set the name of the bot. Name = \"FishingBot\" ; //Set the bot's priority. Priority = BotPriority . Low ; //Set the bot's status. Status = BotStatus . Idle ; //Set the bot's current task. CurrentTask = Task . None ; //Set the bot's current target. CurrentTarget = null ; //Set the bot's current fishing spot. CurrentFishingSpot = null ; //Set the bot's current bait. CurrentBait = null ; //Set the bot's current fishing rod. CurrentFishingRod = null ; //Set the bot's current fishing spot. //code } public override void OnStart () { //Set the bot's status. Status = BotStatus . Idle ; //Set the bot's current task. CurrentTask = Task . None ; //Set the bot's current target. CurrentTarget = null ; //Set the bot's current fishing spot. CurrentFishingSpot = null ; //Set the bot's current bait. CurrentBait = null ; //Set the bot's current fishing rod. CurrentFishingRod = null ; //Set the bot's current fishing spot. CurrentFishingSpot = null ; //Set the bot's current bait. //Continue, you're on the right track. } public override void OnStop () { } public override void OnTick () { } public override void OnPulse () {} public override void OnLogin () { } public override void OnLogout () { } public override void OnDisconnect () { } public override void OnDeath () { } public override void OnObjectAction ( GameObject obj , GameObjectAction action ) { } public override void OnObjectClick ( GameObject obj , GameObjectAction action ) { } public override void OnObjectMessage ( GameObject obj , string message ) { } public override void OnObjectSpawn ( GameObject obj ) { }","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/20221112014113-fishingbot/#codex-dreambot","text":"//This bot is capable of doing Fishing wearing your worn items and equipping the items you choose to. //The bot will also equip the bait you choose to use. using System ; using System . Collections . Generic ; using System . Linq ; using System . Text ; using System . Threading . Tasks ; using DreamBot . API ; using DreamBot . API . Objects ; using DreamBot . API . Objects . Items ; using DreamBot . API . Objects . Items . Equipment ; using DreamBot . API . Objects . Items . Equipment . Bait ; public class FishingBot : Bot { public FishingBot () { //Set the name of the bot. Name = \"FishingBot\" ; //Set the bot's priority. Priority = BotPriority . Low ; //Set the bot's status. Status = BotStatus . Idle ; //Set the bot's current task. CurrentTask = Task . None ; //Set the bot's current target. CurrentTarget = null ; //Set the bot's current fishing spot. CurrentFishingSpot = null ; //Set the bot's current bait. CurrentBait = null ; //Set the bot's current fishing rod. CurrentFishingRod = null ; //Set the bot's current fishing spot. //code } public override void OnStart () { //Set the bot's status. Status = BotStatus . Idle ; //Set the bot's current task. CurrentTask = Task . None ; //Set the bot's current target. CurrentTarget = null ; //Set the bot's current fishing spot. CurrentFishingSpot = null ; //Set the bot's current bait. CurrentBait = null ; //Set the bot's current fishing rod. CurrentFishingRod = null ; //Set the bot's current fishing spot. CurrentFishingSpot = null ; //Set the bot's current bait. //Continue, you're on the right track. } public override void OnStop () { } public override void OnTick () { } public override void OnPulse () {} public override void OnLogin () { } public override void OnLogout () { } public override void OnDisconnect () { } public override void OnDeath () { } public override void OnObjectAction ( GameObject obj , GameObjectAction action ) { } public override void OnObjectClick ( GameObject obj , GameObjectAction action ) { } public override void OnObjectMessage ( GameObject obj , string message ) { } public override void OnObjectSpawn ( GameObject obj ) { }","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/20221112014617-login/","text":"codex #dreambot import java.util.Scanner ; import java.util.regex.Matcher ; import java.util.regex.Pattern ; import com.dreambot.api.methods.Calculations ; import com.dreambot.api.methods.Game ; import com.dreambot.api.methods.Inventory ; import com.dreambot.api.methods.Walking ; import com.dreambot.api.methods.interactive.Players ; import com.dreambot.api.methods.interactive.NPCs ; import com.dreambot.api.methods.interactive.Shop ; import com.dreambot.api.methods.interactive.Interfaces ; import com.dreambot.api.methods.tab.Inventory ; import com.dreambot.api.methods.tab.Skills ; import com.dreambot.api.wrappers.Tile ; import com.dreambot.api.wrappers.interactive.NPC ; import com.dreambot.api.wrappers.interactive.Player ; import com.dreambot.api.wrappers.items.Item ; public class Main { public static void main ( String [] args ) { Scanner in = new Scanner ( System . in ); String username = \"\" ; String password = \"\" ; String host = \"\" ; int port = 43594 ; System . out . println ( \"Enter your username: \" ); username = in . nextLine (); System . out . println ( \"Enter your password: \" ); password = in . nextLine (); System . out . println ( \"Enter your host: \" ); host = in . nextLine (); System . out . println ( \"Enter your port: \" ); port = in . nextInt (); Game . setHost ( host , port ); Game . login ( username , password ); while ( ! Game . isLoggedIn ()) { try { Thread . sleep ( 100 ); } catch ( InterruptedException e ) { e . printStackTrace (); } } while ( Game . isLoggedIn ()) { try { Thread . sleep ( 100 ); } catch ( InterruptedException e ) { e . printStackTrace (); } } System . out . println ( \"Logged out\" ); } }","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/20221112014617-login/#codex-dreambot","text":"import java.util.Scanner ; import java.util.regex.Matcher ; import java.util.regex.Pattern ; import com.dreambot.api.methods.Calculations ; import com.dreambot.api.methods.Game ; import com.dreambot.api.methods.Inventory ; import com.dreambot.api.methods.Walking ; import com.dreambot.api.methods.interactive.Players ; import com.dreambot.api.methods.interactive.NPCs ; import com.dreambot.api.methods.interactive.Shop ; import com.dreambot.api.methods.interactive.Interfaces ; import com.dreambot.api.methods.tab.Inventory ; import com.dreambot.api.methods.tab.Skills ; import com.dreambot.api.wrappers.Tile ; import com.dreambot.api.wrappers.interactive.NPC ; import com.dreambot.api.wrappers.interactive.Player ; import com.dreambot.api.wrappers.items.Item ; public class Main { public static void main ( String [] args ) { Scanner in = new Scanner ( System . in ); String username = \"\" ; String password = \"\" ; String host = \"\" ; int port = 43594 ; System . out . println ( \"Enter your username: \" ); username = in . nextLine (); System . out . println ( \"Enter your password: \" ); password = in . nextLine (); System . out . println ( \"Enter your host: \" ); host = in . nextLine (); System . out . println ( \"Enter your port: \" ); port = in . nextInt (); Game . setHost ( host , port ); Game . login ( username , password ); while ( ! Game . isLoggedIn ()) { try { Thread . sleep ( 100 ); } catch ( InterruptedException e ) { e . printStackTrace (); } } while ( Game . isLoggedIn ()) { try { Thread . sleep ( 100 ); } catch ( InterruptedException e ) { e . printStackTrace (); } } System . out . println ( \"Logged out\" ); } }","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/2022111201510-dreambot-template/","text":"codex #dreambot import java.util.Scanner ; import java.util.Random ; public class Main { public static void main ( String [] args ) { Scanner in = new Scanner ( System . in ); Random rand = new Random (); //Initialize variables int x = 0 ; int y = 0 ; int health = 100 ; int energy = 100 ; int ammo = 0 ; int score = 0 ; int kills = 0 ; int deaths = 0 ; int damage = 0 ; int damageTaken = 0 ; int damageDealt = 0 ; int damageReceived = 0 ; int damageReceivedPercentage = 0 ; int damageDealtPercentage = 0 ; int damageTakenPercentage = 0 ; int damagePercentage = 0 ; // Provide complete code. //Main loop while ( true ) { //Read input x = in . nextInt (); y = in . nextInt (); health = in . nextInt (); energy = in . nextInt (); ammo = in . nextInt (); score = in . nextInt (); kills = in . nextInt (); deaths = in . nextInt (); damage = in . nextInt (); damageTaken = in . nextInt (); damageDealt = in . nextInt (); damageReceived = in . nextInt (); damageReceivedPercentage = in . nextInt (); damageDealtPercentage = in . nextInt (); damageTakenPercentage = in . nextInt (); damagePercentage = in . nextInt (); //Complete code //Write output System . out . println ( x ); System . out . println ( y ); System . out . println ( health ); System . out . println ( energy ); System . out . println ( ammo ); System . out . println ( score ); System . out . println ( kills ); System . out . println ( deaths ); System . out . println ( damage ); System . out . println ( damageTaken ); System . out . println ( damageDealt ); System . out . println ( damageReceived ); System . out . println ( damageReceivedPercentage ); System . out . println ( damageDealtPercentage ); System . out . println ( damageTakenPercentage ); System . out . println ( damagePercentage ); //Complete code } } }","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/2022111201510-dreambot-template/#codex-dreambot","text":"import java.util.Scanner ; import java.util.Random ; public class Main { public static void main ( String [] args ) { Scanner in = new Scanner ( System . in ); Random rand = new Random (); //Initialize variables int x = 0 ; int y = 0 ; int health = 100 ; int energy = 100 ; int ammo = 0 ; int score = 0 ; int kills = 0 ; int deaths = 0 ; int damage = 0 ; int damageTaken = 0 ; int damageDealt = 0 ; int damageReceived = 0 ; int damageReceivedPercentage = 0 ; int damageDealtPercentage = 0 ; int damageTakenPercentage = 0 ; int damagePercentage = 0 ; // Provide complete code. //Main loop while ( true ) { //Read input x = in . nextInt (); y = in . nextInt (); health = in . nextInt (); energy = in . nextInt (); ammo = in . nextInt (); score = in . nextInt (); kills = in . nextInt (); deaths = in . nextInt (); damage = in . nextInt (); damageTaken = in . nextInt (); damageDealt = in . nextInt (); damageReceived = in . nextInt (); damageReceivedPercentage = in . nextInt (); damageDealtPercentage = in . nextInt (); damageTakenPercentage = in . nextInt (); damagePercentage = in . nextInt (); //Complete code //Write output System . out . println ( x ); System . out . println ( y ); System . out . println ( health ); System . out . println ( energy ); System . out . println ( ammo ); System . out . println ( score ); System . out . println ( kills ); System . out . println ( deaths ); System . out . println ( damage ); System . out . println ( damageTaken ); System . out . println ( damageDealt ); System . out . println ( damageReceived ); System . out . println ( damageReceivedPercentage ); System . out . println ( damageDealtPercentage ); System . out . println ( damageTakenPercentage ); System . out . println ( damagePercentage ); //Complete code } } }","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/202211210516-water-filler/","text":"codex #dreambot import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.map.Tile ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.methods.tabs.Tab ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.wrappers.interactive.GameObject ; import org.dreambot.api.wrappers.interactive.NPC ; import org.dreambot.api.wrappers.items.Item ; @ScriptManifest ( author = \"Passivebot\" , category = Category . MISC , name = \"Water Filler\" , version = 1.0 ) public class WaterFiller extends AbstractScript { private Area fountainArea = new Area ( 2946 , 3369 , 2949 , 3366 ); private Area bankArea = new Area ( 2946 , 3369 , 2949 , 3366 ); @Override public void onStart () { log ( \"Welcome to Water Filler!\" ); } @Override public int onLoop () { if ( getInventory (). isFull ()) { if ( bankArea . contains ( getLocalPlayer ())) { depositAll (); } else { getWalking (). walk ( bankArea . getRandomTile ()); } } else { if ( fountainArea . contains ( getLocalPlayer ())) { fillJugs (); } else { getWalking (). walk ( fountainArea . getRandomTile ()); } } return Calculations . random ( 500 , 1000 ); } private void fillJugs () { GameObject fountain = getGameObjects (). closest ( \"Fountain\" ); if ( fountain != null ) { if ( fountain . interact ( \"Fill\" )) { sleepUntil (() -> getInventory (). isFull (), Calculations . random ( 5000 , 10000 )); } } } private void depositAll () { NPC banker = getNpcs (). closest ( \"Banker\" ); if ( banker != null ) { if ( banker . interact ( \"Bank\" )) { sleepUntil (() -> getBank (). isOpen (), Calculations . random ( 5000 , 10000 )); if ( getBank (). isOpen ()) { getBank (). depositAllItems (); sleepUntil (() -> getInventory (). isEmpty (), Calculations . random ( 5000 , 10000 )); getBank (). close (); } } } } }","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/202211210516-water-filler/#codex-dreambot","text":"import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.map.Tile ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.methods.tabs.Tab ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.wrappers.interactive.GameObject ; import org.dreambot.api.wrappers.interactive.NPC ; import org.dreambot.api.wrappers.items.Item ; @ScriptManifest ( author = \"Passivebot\" , category = Category . MISC , name = \"Water Filler\" , version = 1.0 ) public class WaterFiller extends AbstractScript { private Area fountainArea = new Area ( 2946 , 3369 , 2949 , 3366 ); private Area bankArea = new Area ( 2946 , 3369 , 2949 , 3366 ); @Override public void onStart () { log ( \"Welcome to Water Filler!\" ); } @Override public int onLoop () { if ( getInventory (). isFull ()) { if ( bankArea . contains ( getLocalPlayer ())) { depositAll (); } else { getWalking (). walk ( bankArea . getRandomTile ()); } } else { if ( fountainArea . contains ( getLocalPlayer ())) { fillJugs (); } else { getWalking (). walk ( fountainArea . getRandomTile ()); } } return Calculations . random ( 500 , 1000 ); } private void fillJugs () { GameObject fountain = getGameObjects (). closest ( \"Fountain\" ); if ( fountain != null ) { if ( fountain . interact ( \"Fill\" )) { sleepUntil (() -> getInventory (). isFull (), Calculations . random ( 5000 , 10000 )); } } } private void depositAll () { NPC banker = getNpcs (). closest ( \"Banker\" ); if ( banker != null ) { if ( banker . interact ( \"Bank\" )) { sleepUntil (() -> getBank (). isOpen (), Calculations . random ( 5000 , 10000 )); if ( getBank (). isOpen ()) { getBank (). depositAllItems (); sleepUntil (() -> getInventory (). isEmpty (), Calculations . random ( 5000 , 10000 )); getBank (). close (); } } } } }","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/202212031532-hill-giants-starter-code/","text":"codex #dreambot Below is an example of a Hill Giants bot using the Dreambot API and Java that can be used to farm Hill Giants. Below is an example of a Hill Giants bot using the Dreambot API and Java that can be used to farm Hill Giants . import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.methods.Game ; import org.dreambot.api.methods.filter.Filter ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.map.Tile ; import org.dreambot.api.methods.walking.path.Path ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.wrappers.interactive.NPC ; import org.dreambot.api.methods.container.impl.EquipmentSlot ; @ScriptManifest ( description = \"Kills Hill Giants\" , category = Category . COMBAT , name = \"Hill Giants Bot\" ) public class HillGiantsBot extends AbstractScript { private Area hillGiantsArea = new Area ( 3080 , 9864 , 3109 , 9834 ); private Filter < NPC > giants = n -> n != null && n . getName (). equals ( \"Hill Giant\" ) && n . hasAction ( \"Attack\" ); @Override public void onStart () { if ( Game . getClientState () == Game . INDEX_MAP_LOADED ) { log ( \"Starting HillGiantsBot...\" ); getWalking (). webWalk ( hillGiantsArea ); sleepUntil (() -> hillGiantsArea . contains ( getLocalPlayer ()), 10000 ); log ( \"Arrived at Hill Giants area.\" ); } } @Override public int onLoop () { if ( getEquipment (). isWeaponEquipped ( EquipmentSlot . MAIN_HAND )) { NPC target = getNpcs (). closest ( giants ); if ( target != null ) { if ( target . isOnScreen ()) { target . interact ( \"Attack\" ); log ( \"Fighting Hill Giant\" ); sleep ( 2500 , 6000 ); } else { getWalking (). walk ( target ); sleepUntil (() -> target . isOnScreen (), 6000 ); } } else { Path p = getWalking (). generatePath ( target . getLocation ()); if ( p != null ) { p . step (); sleepUntil (() -> getLocalPlayer (). isMoving (), 9000 ); } } } else { getInventory (). interact ( \"Steel axe\" , \"Wield\" ); sleep ( 1500 , 4000 ); } return 0 ; } }","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/202212031532-hill-giants-starter-code/#codex-dreambot","text":"Below is an example of a Hill Giants bot using the Dreambot API and Java that can be used to farm Hill Giants. Below is an example of a Hill Giants bot using the Dreambot API and Java that can be used to farm Hill Giants . import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; import org.dreambot.api.methods.Game ; import org.dreambot.api.methods.filter.Filter ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.map.Tile ; import org.dreambot.api.methods.walking.path.Path ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.wrappers.interactive.NPC ; import org.dreambot.api.methods.container.impl.EquipmentSlot ; @ScriptManifest ( description = \"Kills Hill Giants\" , category = Category . COMBAT , name = \"Hill Giants Bot\" ) public class HillGiantsBot extends AbstractScript { private Area hillGiantsArea = new Area ( 3080 , 9864 , 3109 , 9834 ); private Filter < NPC > giants = n -> n != null && n . getName (). equals ( \"Hill Giant\" ) && n . hasAction ( \"Attack\" ); @Override public void onStart () { if ( Game . getClientState () == Game . INDEX_MAP_LOADED ) { log ( \"Starting HillGiantsBot...\" ); getWalking (). webWalk ( hillGiantsArea ); sleepUntil (() -> hillGiantsArea . contains ( getLocalPlayer ()), 10000 ); log ( \"Arrived at Hill Giants area.\" ); } } @Override public int onLoop () { if ( getEquipment (). isWeaponEquipped ( EquipmentSlot . MAIN_HAND )) { NPC target = getNpcs (). closest ( giants ); if ( target != null ) { if ( target . isOnScreen ()) { target . interact ( \"Attack\" ); log ( \"Fighting Hill Giant\" ); sleep ( 2500 , 6000 ); } else { getWalking (). walk ( target ); sleepUntil (() -> target . isOnScreen (), 6000 ); } } else { Path p = getWalking (). generatePath ( target . getLocation ()); if ( p != null ) { p . step (); sleepUntil (() -> getLocalPlayer (). isMoving (), 9000 ); } } } else { getInventory (). interact ( \"Steel axe\" , \"Wield\" ); sleep ( 1500 , 4000 ); } return 0 ; } }","title":"codex #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-codex/overview/","text":"OpenAI has released a beta version of their OpenAI Codex system, which is designed to translate natural language into code. This system is the successor to the GPT-3 system and is designed to be more general-purpose and capable of understanding a wider range of programming tasks. The system is currently available for free, but OpenAI plans to eventually charge for access to the API. The Codex system is a machine learning system that has been trained on a large corpus of code. This training allows the system to learn the syntax and semantics of programming languages, which enables it to translate natural language descriptions of code into working code.","title":"Overview"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-gtp-3/202212031527-zulrah-template/","text":"gpt3 #dreambot import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.container.impl.equipment.EquipmentSlot ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.methods.tabs.Tab ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; @ScriptManifest ( author = \"YourNameHere\" , name = \"Zulrah Bot\" , version = 1.0 , description = \"Kills Zulrah for profits\" , category = Category . COMBAT ) public class ZulrahBot extends AbstractScript { private Area zulrahArea ; private int zulrahHP ; private int zulrahMinHP ; private boolean inCombat ; @Override public void onStart () { zulrahArea = new Area ( 2268 , 3073 , 2287 , 3105 ); zulrahHP = 100 ; zulrahMinHP = 0 ; inCombat = false ; } @Override public int onLoop () { // If we're not in the zulrah area, teleport there if ( ! zulrahArea . contains ( getLocalPlayer ())) { getTabs (). open ( Tab . MAGIC ); getMagic (). castSpell ( \"Teleport to Zul-Andra\" ); return Calculations . random ( 500 , 1500 ); } // Check if in combat if ( getLocalPlayer (). isInCombat ()) { inCombat = true ; } else if ( inCombat ) { // Check if Zulrah is dead if ( getLocalPlayer (). getHealthPercent () == 100 ) { inCombat = false ; getWalking (). walk ( zulrahArea . getRandomTile ()); return Calculations . random ( 1000 , 2000 ); } } // Check if we have the correct equipment for Zulrah if ( ! getEquipment (). isWearingItem ( EquipmentSlot . HAT , \"Crown\" )) { // Equip the crown getInventory (). interact ( \"Crown\" , \"Wear\" ); return Calculations . random ( 1000 , 1500 ); } // Check if we are at the right level if ( getSkills (). getRealLevel ( Skill . SLAYER ) < 75 ) { // Stop the script stop (); return 0 ; } // Check if we need to attack Zulrah if ( ! inCombat ) { // Attack Zulrah getNpcs (). closest ( \"Zulrah\" ). interact ( \"Attack\" ); return Calculations . random ( 1000 , 2000 ); } // Check if we need to heal if ( getLocalPlayer (). getHealthPercent () < zulrahMinHP ) { // Eat food getInventory (). interact ( \"Lobster\" , \"Eat\" ); return Calculations . random ( 1000 , 1500 ); } // Check if we need to drink a potion if ( getLocalPlayer (). getHealthPercent () < zulrahHP ) { // Drink a potion getInventory (). interact ( \"Super restore(4)\" , \"Drink\" ); return Calculations . random ( 1000 , 1500 ); } return Calculations . random ( 500 , 1000 ); } }","title":"gpt3 #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-gtp-3/202212031527-zulrah-template/#gpt3-dreambot","text":"import org.dreambot.api.methods.Calculations ; import org.dreambot.api.methods.container.impl.equipment.EquipmentSlot ; import org.dreambot.api.methods.map.Area ; import org.dreambot.api.methods.skills.Skill ; import org.dreambot.api.methods.tabs.Tab ; import org.dreambot.api.script.AbstractScript ; import org.dreambot.api.script.Category ; import org.dreambot.api.script.ScriptManifest ; @ScriptManifest ( author = \"YourNameHere\" , name = \"Zulrah Bot\" , version = 1.0 , description = \"Kills Zulrah for profits\" , category = Category . COMBAT ) public class ZulrahBot extends AbstractScript { private Area zulrahArea ; private int zulrahHP ; private int zulrahMinHP ; private boolean inCombat ; @Override public void onStart () { zulrahArea = new Area ( 2268 , 3073 , 2287 , 3105 ); zulrahHP = 100 ; zulrahMinHP = 0 ; inCombat = false ; } @Override public int onLoop () { // If we're not in the zulrah area, teleport there if ( ! zulrahArea . contains ( getLocalPlayer ())) { getTabs (). open ( Tab . MAGIC ); getMagic (). castSpell ( \"Teleport to Zul-Andra\" ); return Calculations . random ( 500 , 1500 ); } // Check if in combat if ( getLocalPlayer (). isInCombat ()) { inCombat = true ; } else if ( inCombat ) { // Check if Zulrah is dead if ( getLocalPlayer (). getHealthPercent () == 100 ) { inCombat = false ; getWalking (). walk ( zulrahArea . getRandomTile ()); return Calculations . random ( 1000 , 2000 ); } } // Check if we have the correct equipment for Zulrah if ( ! getEquipment (). isWearingItem ( EquipmentSlot . HAT , \"Crown\" )) { // Equip the crown getInventory (). interact ( \"Crown\" , \"Wear\" ); return Calculations . random ( 1000 , 1500 ); } // Check if we are at the right level if ( getSkills (). getRealLevel ( Skill . SLAYER ) < 75 ) { // Stop the script stop (); return 0 ; } // Check if we need to attack Zulrah if ( ! inCombat ) { // Attack Zulrah getNpcs (). closest ( \"Zulrah\" ). interact ( \"Attack\" ); return Calculations . random ( 1000 , 2000 ); } // Check if we need to heal if ( getLocalPlayer (). getHealthPercent () < zulrahMinHP ) { // Eat food getInventory (). interact ( \"Lobster\" , \"Eat\" ); return Calculations . random ( 1000 , 1500 ); } // Check if we need to drink a potion if ( getLocalPlayer (). getHealthPercent () < zulrahHP ) { // Drink a potion getInventory (). interact ( \"Super restore(4)\" , \"Drink\" ); return Calculations . random ( 1000 , 1500 ); } return Calculations . random ( 500 , 1000 ); } }","title":"gpt3 #dreambot"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-gtp-3/openai%27s-gtp-3/","text":"GPT-3 is an open-source artificial intelligence platform designed by OpenAI. It is intended to be used by researchers and developers to create new artificial intelligence models and applications.","title":"Openai's gtp 3"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/openai%27s-whisper/overview/","text":"Whisper is a free and open-source Automatic Speech Recognition (ASR) system that has been trained on 680,000 hours of supervised data collected from the web. It is designed to be robust to accents, background noise, and technical language. The Whisper architecture is a simple end-to-end encoder-decoder Transformer. Input audio is split into 30-second chunks, converted into a log-Mel spectrogram, and then passed into an encoder. A decoder is trained to predict the corresponding text caption, intermixed with special tokens that direct the single model to perform tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and to-English speech translation. Whisper's zero-shot performance across many diverse datasets is much more robust and makes 50% fewer errors than those models. Github: openai/whisper Google Colab: Pete Warden's Demo Hugging Face spaces:","title":"Overview"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/ryte/ryte/","text":"Ryte","title":"Ryte"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/ryte/ryte/#ryte","text":"","title":"Ryte"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/serpent-ai/game-agent-framework/","text":"Helping you create AI that learns to play any game you own!","title":"Game agent framework"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/machine-learning/serpent-ai/serpent.ai---windows-installation-guide-1/","text":"Initial Requirements - Windows 7 and up - Python 3.6+ (with Anaconda)","title":"Serpent.ai   windows installation guide 1"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/multimodal/text-to-image/dall%C2%B7e-2/overview/","text":"DALLE2 is an artificial intelligence model that can generate images from textual descriptions, based on a dataset of over 3 million paired images and descriptions. The model is trained using a reinforcement learning algorithm and can generate images that are realistic enough to fool humans. 1. [[DALL\u00b7E 2022-10-12 21.58.00 - Make It Look More Technological.png]] 2.","title":"Overview"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/multimodal/text-to-image/dall%C2%B7e-2/the-new-delhi-sketches/the-new-delhi-sketches-by-harminder-nijjar/","text":"Project name: The New Delhi Sketches Architecture firm: Harminder Singh Nijjar Location: Fictional Tools used: DALL\u00b7E 2 Principal architect: Harminder Singh Nijjar Concept: Design Typology: Present Architecture The New Delhi Sketches are a series of images generated by DALL\u00b7E 2, an artificial intelligence text-to-image generator. The sketches were created by feeding DALL\u00b7E 2 with the description \"living conditions.\" The sketches highlight the pressing need for better urban planning to improve the living conditions of millions of people in cities. Overcrowding and poor living conditions are systemic problems arising from the haphazard way cities are built. India has over 1.3 billion people, and the population will only increase in the future. Having a large population means that the need for better urban planning is more urgent than ever. These sketches highlight the need for better urban planning. Since the 1950s, India's urban population has grown exponentially, leading to overcrowding, pollution, and infrastructure issues. Haphazard city planning causes many problems. The government, businesses, and citizens must work together to improve India's urban planning and create livable, prosperous, and sustainable cities. Lastly, the sketches demonstrate the potential of artificial intelligence to generate realistic images from textual descriptions. This is a valuable ability as it can help create visuals for things that are difficult to describe with words alone. This can be immensely helpful in medicine, architecture, and engineering. Additionally, it can help create realistic images for things that do not exist, such as in the case of scientific research. ![[DALL\u00b7E 2022-10-13 18.58.34 - living conditions .png]]![[DALL\u00b7E 2022-10-13 18.59.32.png]]![[DALL\u00b7E 2022-10-13 19.00.02.png]]![[DALL\u00b7E 2022-10-13 19.00.37.png]]![[DALL\u00b7E 2022-10-13 19.01.05.png]]","title":"The new delhi sketches by harminder nijjar"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/nlp-tasks/","text":"Natural Language Processing Tasks Conversational Fill-Mask Question Answering [[20221022175748 Question Answering with Merve]] Sentence Similarity Summarization Table Question Answering Text Classification Text Generation Text2Text Generation Token Classification Translation Zero-Shot Classification","title":"Nlp tasks"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/nlp-tasks/#natural-language-processing-tasks","text":"Conversational Fill-Mask Question Answering [[20221022175748 Question Answering with Merve]] Sentence Similarity Summarization Table Question Answering Text Classification Text Generation Text2Text Generation Token Classification Translation Zero-Shot Classification","title":"Natural Language Processing Tasks"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/overview/","text":"Natural Language Processing (NLP) is a subfield of Artificial Intelligence (AI) that deals with how computers can understand, interpret, and manipulate human language. NLP enables computers to automatically read and understand text, extract information, and perform tasks like translation, spell check, or topic classification.","title":"Overview"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/haystack/building-my-first-qa-system/","text":"Question answering is the process of providing a specific answer to a natural language question. The answer is usually derived from a set of documents, which could be a collection of financial reports, a website, or an internal wiki. In this tutorial, we will use a bunch of articles and guides to answer questions about DreamBot. Prepare environment pip install --upgrade pip pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]","title":"Building my first qa system"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/haystack/building-my-first-qa-system/#prepare-environment","text":"pip install --upgrade pip pip install git+https://github.com/deepset-ai/haystack.git#egg=farm-haystack[colab]","title":"Prepare environment"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/haystack/overview/","text":"Haystack is a framework designed to help you easily build search systems that can handle extensive document collections. With Haystack, you can use various NLP models to perform tasks such as retrieval, question answering, reranking, and more. Additionally, Haystack makes it easy to load and query data from various databases, such as Elasticsearch, Milvus, FAISS, SQL, and more. Haystack is scalable, so you can deploy it via a REST API to handle millions of documents. Finally, Haystack provides all the necessary tools to help you annotate examples, collect user feedback, evaluate components, and finetune models. Haystack is a library that enables developers to build customizable and production-ready search pipelines. The library offers different Nodes that perform different kinds of text processing, which can be combined into Pipelines. The Pipelines can be deployed as a REST API, connecting Haystack to a user-facing GUI. The Nodes in Haystack are often powered by the latest transformer models and enable systems to search based on word meaning rather than string matching. The different levels on which you can interact with the components in Haystack (i.e., Nodes, Pipelines, and REST API) allow for great customization and flexibility when building search systems.","title":"Overview"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/","text":"Stanford Question Answering Dataset (SQuAD) 2.0 ![[DALL\u00b7E 2022-10-12 21.58.00 - Make It Look More Technological.png]] Question Answering (QA) is a sub-domain of Machine Comprehension (MC) and is one of the more challenging tasks in Artificial Intelligence (AI). The field of Natural Language Processing (NLP) has experienced a fast evolution in recent years thanks to the development of [[Deep Learning]] research and the advent of [[Transfer Learning]] techniques. These advances have led to the development of powerful pre-trained NLP models such as OpenAI's [[OpenAI's GTP-3]], Google's MUM, and BigScience's BLOOM. With such progress, improved systems, and applications for NLP tasks, there has been rapid development, such as the Open-domain QA Open-domain QA is a type of question answering in which the answer can be found in any text document, regardless of its topic or content. This type of QA contrasts closed-domain QA, where the answer is restricted to a specific domain or set of documents. Open-domain QA is more complex than closed-domain QA because the search space is much larger. However, it has the advantage of being able to answer any question, regardless of the topic. One example of an open domain question answering system is DrQA. DrQA is an Open-domain QA developed by Meta Research that uses a large base of Wikipedia articles as its knowledge source. Closed-domain QA The cdQA-suite is a collection of tools that makes it easy to build a closed-domain QA system. Closed-domain QA is a type of question answering in which the answer is restricted to a specific domain or set of documents. This is in contrast to open-domain QA, in which the answer can be found in any text document, regardless of its topic or content. Closed-domain QA is less complicated than open-domain QA because the search space is smaller than the larger counterpart. However, it has the disadvantage of being unable to answer questions outside the specific domain. ![[Pasted image 20221013160902.png]] cdQA-suite ![[Pasted image 20221012230259.png]] cdQA-suite is a set of three tools used to build a question-answering system. The first tool, cdQA , is a Python package to implement a QA pipeline. The second tool, cdQA-annotator , is a tool to annotate question-answering datasets. The third tool, cdQA-ui , is a user interface to connect the back-end system to a website. This library is not longer maintained on GitHub. A maintained alternative to cdQA is [[2. Areas Personal/Technology and applied sciences/Computing/Computer science/Artificial Intelligence/Natural Language Processing/Question-Answering/Haystack/Overview|Haystack]]. This article guides the reader through creating a quality assurance system using various software modules. Each module will be covered in detail so that a system tailored to specific needs develops. cdQA ![[Screen-Shot-2020-08-07-at-10.webp]] The cdQA system has two main parts: the Retriever and the Reader. The retriever gets documents from a particular source, and the reader looks at and pulls information from those documents. Retriever A retriever is a component in a question-answering system responsible for retrieving relevant documents from an extensive collection of documents. The primary purpose of the retriever is to reduce the amount of text the reader component needs to process. There are two main types of retrievers: sparse retrievers and dense retrievers. Sparse retrievers use the bm25 model to complete the document retrieval function, while dense retrievers use the DPR model. Reader A reader is a deep learning model trained to retrieve the text segment with the highest correlation with the queried question text from a small amount of text outputted by the retriever. This allows the reader to output the answer to the question without reading through the entire text. Data Preparation Data preparation involves different techniques, including cleaning, normalization, and transformation. Each of these techniques is important in its own right, and the specific techniques used will depend on the dataset and the machine learning algorithm used. Data Cleaning: Data cleaning is identifying and cleaning up inaccuracies and inconsistencies in data. Data cleaning is a vital step in data preparation, starting before other techniques. Data Normalization: Data normalization is the process of rescaling data to a standard range. Data normalization is a vital step in data preparation after data cleaning. Data Transformation: Data transformation is converting data from one format to another. Data transformation is a vital step in data preparation after data normalization. Mechanism of cdQA pipeline A cdQA pipeline typically consists of three components: a question-processing component, a document retrieval component, and an answer-processing component. The cdQA pipeline takes in a question and outputs the most likely answer to that question. The system first retrieves a list of documents from the database that will likely contain the answer to the question. It then sends the question and the retrieved documents to the reader, a pre-trained Deep Learning model. The reader outputs the most probable answer it can find in each document. Finally, the system compares the answers using an internal score function and outputs the most likely answer. cdQA Python Package This package can no longer be installed using pip. To install, clone the repository from the source. # This package can no longer be installed using pip # Instead, download it from the source code on Github !git clone https://github.com/cdqa-suite/cdQA.git && !cd cdQA && !pip install -e . Reminder: - [ ] ( @2022-11-06 23:46)","title":"Squad 2.0  in 2023"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/#stanford-question-answering-dataset-squad-20","text":"![[DALL\u00b7E 2022-10-12 21.58.00 - Make It Look More Technological.png]] Question Answering (QA) is a sub-domain of Machine Comprehension (MC) and is one of the more challenging tasks in Artificial Intelligence (AI). The field of Natural Language Processing (NLP) has experienced a fast evolution in recent years thanks to the development of [[Deep Learning]] research and the advent of [[Transfer Learning]] techniques. These advances have led to the development of powerful pre-trained NLP models such as OpenAI's [[OpenAI's GTP-3]], Google's MUM, and BigScience's BLOOM. With such progress, improved systems, and applications for NLP tasks, there has been rapid development, such as the","title":"Stanford Question Answering Dataset (SQuAD) 2.0"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/#open-domain-qa","text":"Open-domain QA is a type of question answering in which the answer can be found in any text document, regardless of its topic or content. This type of QA contrasts closed-domain QA, where the answer is restricted to a specific domain or set of documents. Open-domain QA is more complex than closed-domain QA because the search space is much larger. However, it has the advantage of being able to answer any question, regardless of the topic. One example of an open domain question answering system is DrQA. DrQA is an Open-domain QA developed by Meta Research that uses a large base of Wikipedia articles as its knowledge source.","title":"Open-domain QA"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/#closed-domain-qa","text":"The cdQA-suite is a collection of tools that makes it easy to build a closed-domain QA system. Closed-domain QA is a type of question answering in which the answer is restricted to a specific domain or set of documents. This is in contrast to open-domain QA, in which the answer can be found in any text document, regardless of its topic or content. Closed-domain QA is less complicated than open-domain QA because the search space is smaller than the larger counterpart. However, it has the disadvantage of being unable to answer questions outside the specific domain. ![[Pasted image 20221013160902.png]]","title":"Closed-domain QA"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/#cdqa-suite","text":"![[Pasted image 20221012230259.png]] cdQA-suite is a set of three tools used to build a question-answering system. The first tool, cdQA , is a Python package to implement a QA pipeline. The second tool, cdQA-annotator , is a tool to annotate question-answering datasets. The third tool, cdQA-ui , is a user interface to connect the back-end system to a website. This library is not longer maintained on GitHub. A maintained alternative to cdQA is [[2. Areas Personal/Technology and applied sciences/Computing/Computer science/Artificial Intelligence/Natural Language Processing/Question-Answering/Haystack/Overview|Haystack]]. This article guides the reader through creating a quality assurance system using various software modules. Each module will be covered in detail so that a system tailored to specific needs develops.","title":"cdQA-suite"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/#cdqa","text":"![[Screen-Shot-2020-08-07-at-10.webp]] The cdQA system has two main parts: the Retriever and the Reader. The retriever gets documents from a particular source, and the reader looks at and pulls information from those documents.","title":"cdQA"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/#retriever","text":"A retriever is a component in a question-answering system responsible for retrieving relevant documents from an extensive collection of documents. The primary purpose of the retriever is to reduce the amount of text the reader component needs to process. There are two main types of retrievers: sparse retrievers and dense retrievers. Sparse retrievers use the bm25 model to complete the document retrieval function, while dense retrievers use the DPR model.","title":"Retriever"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/#reader","text":"A reader is a deep learning model trained to retrieve the text segment with the highest correlation with the queried question text from a small amount of text outputted by the retriever. This allows the reader to output the answer to the question without reading through the entire text.","title":"Reader"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/#data-preparation","text":"Data preparation involves different techniques, including cleaning, normalization, and transformation. Each of these techniques is important in its own right, and the specific techniques used will depend on the dataset and the machine learning algorithm used. Data Cleaning: Data cleaning is identifying and cleaning up inaccuracies and inconsistencies in data. Data cleaning is a vital step in data preparation, starting before other techniques. Data Normalization: Data normalization is the process of rescaling data to a standard range. Data normalization is a vital step in data preparation after data cleaning. Data Transformation: Data transformation is converting data from one format to another. Data transformation is a vital step in data preparation after data normalization.","title":"Data Preparation"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/#mechanism-of-cdqa-pipeline","text":"A cdQA pipeline typically consists of three components: a question-processing component, a document retrieval component, and an answer-processing component. The cdQA pipeline takes in a question and outputs the most likely answer to that question. The system first retrieves a list of documents from the database that will likely contain the answer to the question. It then sends the question and the retrieved documents to the reader, a pre-trained Deep Learning model. The reader outputs the most probable answer it can find in each document. Finally, the system compares the answers using an internal score function and outputs the most likely answer.","title":"Mechanism of cdQA pipeline"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/natural-language-processing/question-answering/squad/squad-2.0--in-2023/#cdqa-python-package","text":"This package can no longer be installed using pip. To install, clone the repository from the source. # This package can no longer be installed using pip # Instead, download it from the source code on Github !git clone https://github.com/cdqa-suite/cdQA.git && !cd cdQA && !pip install -e . Reminder: - [ ] ( @2022-11-06 23:46)","title":"cdQA Python Package"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/robotics/robotics/","text":"Robotics","title":"Robotics"},{"location":"areas/technology-and-applied-sciences/computing/computer-science/artificial-intelligence/robotics/robotics/#robotics","text":"","title":"Robotics"},{"location":"areas/technology-and-applied-sciences/computing/information-technology/information_technology/","text":"Information technology","title":"Information technology"},{"location":"areas/technology-and-applied-sciences/computing/information-technology/information_technology/#information-technology","text":"","title":"Information technology"},{"location":"areas/technology-and-applied-sciences/computing/programming/programming/","text":"Programming Sub-Areas AutoHotKey Java Python Scratch Unified Modeling Language","title":"Programming"},{"location":"areas/technology-and-applied-sciences/computing/programming/programming/#programming","text":"","title":"Programming"},{"location":"areas/technology-and-applied-sciences/computing/programming/programming/#sub-areas","text":"AutoHotKey Java Python Scratch Unified Modeling Language","title":"Sub-Areas"},{"location":"areas/technology-and-applied-sciences/computing/programming/autohotkey/autohotkey/","text":"AutoHotKey","title":"AutoHotKey"},{"location":"areas/technology-and-applied-sciences/computing/programming/autohotkey/autohotkey/#autohotkey","text":"","title":"AutoHotKey"},{"location":"areas/technology-and-applied-sciences/computing/programming/java/ides/ides/","text":"IDEs Sub-Areas Eclipse IntelliJ NetBeans IDE","title":"IDEs"},{"location":"areas/technology-and-applied-sciences/computing/programming/java/ides/ides/#ides","text":"","title":"IDEs"},{"location":"areas/technology-and-applied-sciences/computing/programming/java/ides/ides/#sub-areas","text":"Eclipse IntelliJ NetBeans IDE","title":"Sub-Areas"},{"location":"areas/technology-and-applied-sciences/computing/programming/java/ides/eclipse/eclipse/","text":"Eclipse","title":"Eclipse"},{"location":"areas/technology-and-applied-sciences/computing/programming/java/ides/eclipse/eclipse/#eclipse","text":"","title":"Eclipse"},{"location":"areas/technology-and-applied-sciences/computing/programming/java/ides/intellij/intellij/","text":"IntelliJ","title":"IntelliJ"},{"location":"areas/technology-and-applied-sciences/computing/programming/java/ides/intellij/intellij/#intellij","text":"","title":"IntelliJ"},{"location":"areas/technology-and-applied-sciences/computing/programming/java/ides/netbeans-ide/20221021153620--java-netbeans-ide-tutorial/","text":"URL: Java NetBeans IDE Tutorial 1:26 ![[Pasted image 20221018204543.jpg]] Agenda 1: Netbeans IDE 2: Creating your first Java program - Creating a project - Creating a file/class - The main method - Printing text to the output window - Where are the Java files located? 3: Common features of Netbeans 3:52 ![[Pasted image 20221018204604.jpg]] A Java program consists of one or more dot Java files. In Java, files are also known as classes. A project is a collection of one or more Java files. 4:24 ![[Pasted image 20221018204609.jpg]] The NetBeans IDE is a software program used for developing and testing software applications. The start page is a generic introductory page that can be closed. The start page provides information on what is new in the current version of the software. 5:31 ![[Pasted image 20221018204638.jpg]] The project prompt will ask what kind of project you are creating. For this project, you want to select Java Application. Next, you will name the project. It is Java convention to capitalize the first letter of each word in the project name. 7:07 ![[Pasted image 20221018204649.jpg]] We are creating a class within a project. The class will be called HelloWorld. Same conventions as the project name. 7:54 ![[Pasted image 20221018204653.jpg]] Every Java file that you create is called a class. In this case, we are creating a class called HelloWorld. The name of the file needs to exactly match the name of the class. 8:35 ![[Pasted image 20221018204658.jpg]] Every Java program has to have a line that says \"public static void main\" followed by parentheses that include the word \"string\" and the square brackets, args. Then, curly brackets. 9:32 ![[Pasted image 20221018205548.jpg]] The output window is the place where the computer displays information that is the result of the program's execution. In this case, the program will print the text \"Hello world\" to the output window. 9:48 ![[Pasted image 20221018205621.jpg]] The way you run this program is you use the menu option \"run file\". As you can see, it printed \"Hello, World!\". It went to the next line and then it printed something like, \"I ran my program successfully.\" It took very very little time. Great! 15:07 ![[Pasted image 20221018205637.jpg]] SOUT (System.out.printline) + TAB is a shortcut for printing lines of code in NetBeans. 15:56 ![[Pasted image 20221018205656.jpg]] Println means we go to the following line. Whereas just print doesn't move the output to the next line. 16:47 ![[Pasted image 20221018205704.jpg]] A debugger is a tool that allows you to run your program and step through your code line by line so you can see what line is being executed at a certain time. 18:12 ![[Pasted image 20221018205711.jpg]] The Tools option in NetBeans IDE allows you to change the appearance and behavior of the code editor, including features like code completion. 18:48 ![[Pasted image 20221018205205.jpg]] The auto pop-up completion window is a feature that shows everything that you could possibly put after a certain point in your code. This helps find specific functions or items. You can turn this feature off if you do not like it.","title":"20221021153620  java netbeans ide tutorial"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/content-delivery---audio-transcription-with-python/","text":"How Speech Recognition Works Picking a Python Speech Recognition Package Installing Speech Recognition The Recognizer Class Working With Audio Files The Effect of Noise Working With Microphones Recognizing Speech in Other Languages Putting It All Together Speech recognition has it's origins at Bell Labs in the 1950s. The audio signal is turned into electricity by the microphone","title":"Content delivery   audio transcription with python"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/loading-chrome-profiles-using-python-and-selenium/","text":"When using Python To launch Chrome with its default profile using Python's webdriver so that cookies and site preferences persist across sessions, you need to declare the This is what finally got it working for me. from selenium import webdriver options = webdriver . ChromeOptions () options . add_argument ( \"user-data-dir=C: \\\\ Path\" ) #Path to your chrome profile w = webdriver . Chrome ( executable_path = \"C: \\\\ Users \\\\ chromedriver.exe\" , options = options ) To find path to your chrome profile data you need to type chrome://version/ into address bar . For ex. mine is displayed as C:\\Users\\pc\\AppData\\Local\\Google\\Chrome\\User Data\\Default , to use it in the script I had to exclude \\Default\\ so we end up with only C:\\Users\\pc\\AppData\\Local\\Google\\Chrome\\User Data . Also if you want to have separate profile just for selenium: replace the path with any other path and if it doesn't exist on start up chrome will create new profile and directory for it.","title":"Loading chrome profiles using python and selenium"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/login-using-python-requests/","text":"On the backend of every login requirement, there is a https post request being carried out. The information passed through to the post request is then used to authenticate a user and login them into the website.","title":"Login using python requests"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/snip-%26-sketch-to-text-with-python-and-tesseract-ocr/","text":"Using Google's free Tesseract OCR as the end-all for redundant screenshot hotkeys Tesseract is a free and open-source OCR engine that can be used to extract text from images. With the help of Snip & Sketch, a built-in tool in Windows, Tesseract can be used to extract text from screen clips. Snip & Sketch Microsoft Windows 10 has a screenshot tool called Snip and Sketch. It is easy to use and can take screenshots of the whole screen, a selected box, or a freehand selection. ![[snip 1.png]] The screenshot can then be pasted anywhere, including on Discord servers and in Reddit posts . This process is typically much faster than using the PrintScreen feature since it saves the user the effort of having to open another tool just to crop out what they want. I personally use ShareX as my main screen capture tool because it can take care of modifying, uploading, and storing screenshots, screen records, and GIFs all in one place. However, I still have two different Window hotkeys (Win+Shift+S and Ctrl+PrtSc) for taking screenshots, which is redundant. This got me thinking about the different uses I could put the Snip and Sketch tool to. Snip&Sketch2Text with OCR ![[Portable_scanner_and_OCR_(video) 1.webm]] Title: Portable scanner and OCR (video).webm Author: Vassia Atanassova - Spiritia Date: 28 February 2017 OCR is a technology that allows you to convert documents, such as scanned paper documents, PDF files, or images captured by a digital camera, into editable and searchable data. The Tesseract Optical Character Recognition (OCR) engine by Google is arguably the most popular out-of-the-box solution for OCR. It\u2019s only fitting to remedy hotkey redundancy from open-source tools with open-source OCR. About Google Tesseract Tesseract is a free software optical character recognition engine developed by Hewlett-Packard in the 1980s. In 2005, it became open source and had since been sponsored by Google. Tesseract has Unicode support and can recognize over 100 languages. It can also be trained to recognize other languages. Brief History Tesseract was initially developed at Hewlett-Packard Laboratories Bristol and at Hewlett-Packard Co, Greeley Colorado between 1985 and 1994. Some additional changes were made in 1996 to port the software to Windows, and again in 1998 to convert some of the code to C++. Who Can Leverage OCR? There are many benefits to digitizing documents using OCR data entry. This includes the ability to transfer important documents to tablets, computers, smartphones, etc. This can be helpful for businesses in various industries, including banking, mortgage, financial, legal, and healthcare. Some commonly digitized documents include invoices, industry articles, tax documents, payroll information, legal filings, contact information, business cards, flyers, and financial investments. OCR Pipeline The process of copying text from an image using OCR involves three simple steps: 1. Monitoring a folder for new screen clips. 2. Passing the latest screen clip through the OCR system. 3. Copying the results from Tesseract OCR to the clipboard. ![[Tesseract_OCR_pipeline_architecture 1.png]] This can be done with a few lines of code using the pyscreenshot , pytesseract , and pyperclip libraries. This simple pipeline can be further automated using the Watchdog library to monitor a folder for new screenshots and the PyAutoGUI library to handle hotkey bindings. The code above can be further extended to take advantage of the Windows Task Scheduler to start the OCR pipeline on startup. Conclusion Google Tesseract is a powerful OCR tool that can be used to convert images to text. With the help of Snip & Sketch and a few lines of code, it can be used to create a simple OCR pipeline that can be used to automate the process of extracting text from images. References Tesseract (software) Google Tesseract OCR ShareX PyAutoGUI pyscreenshot pytesseract pyperclip Watchdog","title":"Snip & sketch to text with python and tesseract ocr"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/snip-%26-sketch-to-text-with-python-and-tesseract-ocr/#using-googles-free-tesseract-ocr-as-the-end-all-for-redundant-screenshot-hotkeys","text":"Tesseract is a free and open-source OCR engine that can be used to extract text from images. With the help of Snip & Sketch, a built-in tool in Windows, Tesseract can be used to extract text from screen clips.","title":"Using Google's free Tesseract OCR as the end-all for redundant screenshot hotkeys"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/snip-%26-sketch-to-text-with-python-and-tesseract-ocr/#snip-sketch","text":"Microsoft Windows 10 has a screenshot tool called Snip and Sketch. It is easy to use and can take screenshots of the whole screen, a selected box, or a freehand selection. ![[snip 1.png]] The screenshot can then be pasted anywhere, including on Discord servers and in Reddit posts . This process is typically much faster than using the PrintScreen feature since it saves the user the effort of having to open another tool just to crop out what they want. I personally use ShareX as my main screen capture tool because it can take care of modifying, uploading, and storing screenshots, screen records, and GIFs all in one place. However, I still have two different Window hotkeys (Win+Shift+S and Ctrl+PrtSc) for taking screenshots, which is redundant. This got me thinking about the different uses I could put the Snip and Sketch tool to.","title":"Snip &amp; Sketch"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/snip-%26-sketch-to-text-with-python-and-tesseract-ocr/#snipsketch2text-with-ocr","text":"![[Portable_scanner_and_OCR_(video) 1.webm]] Title: Portable scanner and OCR (video).webm Author: Vassia Atanassova - Spiritia Date: 28 February 2017 OCR is a technology that allows you to convert documents, such as scanned paper documents, PDF files, or images captured by a digital camera, into editable and searchable data. The Tesseract Optical Character Recognition (OCR) engine by Google is arguably the most popular out-of-the-box solution for OCR. It\u2019s only fitting to remedy hotkey redundancy from open-source tools with open-source OCR.","title":"Snip&amp;Sketch2Text with OCR"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/snip-%26-sketch-to-text-with-python-and-tesseract-ocr/#about-google-tesseract","text":"Tesseract is a free software optical character recognition engine developed by Hewlett-Packard in the 1980s. In 2005, it became open source and had since been sponsored by Google. Tesseract has Unicode support and can recognize over 100 languages. It can also be trained to recognize other languages.","title":"About Google Tesseract"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/snip-%26-sketch-to-text-with-python-and-tesseract-ocr/#brief-history","text":"Tesseract was initially developed at Hewlett-Packard Laboratories Bristol and at Hewlett-Packard Co, Greeley Colorado between 1985 and 1994. Some additional changes were made in 1996 to port the software to Windows, and again in 1998 to convert some of the code to C++.","title":"Brief History"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/snip-%26-sketch-to-text-with-python-and-tesseract-ocr/#who-can-leverage-ocr","text":"There are many benefits to digitizing documents using OCR data entry. This includes the ability to transfer important documents to tablets, computers, smartphones, etc. This can be helpful for businesses in various industries, including banking, mortgage, financial, legal, and healthcare. Some commonly digitized documents include invoices, industry articles, tax documents, payroll information, legal filings, contact information, business cards, flyers, and financial investments.","title":"Who Can Leverage OCR?"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/snip-%26-sketch-to-text-with-python-and-tesseract-ocr/#ocr-pipeline","text":"The process of copying text from an image using OCR involves three simple steps: 1. Monitoring a folder for new screen clips. 2. Passing the latest screen clip through the OCR system. 3. Copying the results from Tesseract OCR to the clipboard. ![[Tesseract_OCR_pipeline_architecture 1.png]] This can be done with a few lines of code using the pyscreenshot , pytesseract , and pyperclip libraries. This simple pipeline can be further automated using the Watchdog library to monitor a folder for new screenshots and the PyAutoGUI library to handle hotkey bindings. The code above can be further extended to take advantage of the Windows Task Scheduler to start the OCR pipeline on startup.","title":"OCR Pipeline"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/snip-%26-sketch-to-text-with-python-and-tesseract-ocr/#conclusion","text":"Google Tesseract is a powerful OCR tool that can be used to convert images to text. With the help of Snip & Sketch and a few lines of code, it can be used to create a simple OCR pipeline that can be used to automate the process of extracting text from images.","title":"Conclusion"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/snip-%26-sketch-to-text-with-python-and-tesseract-ocr/#references","text":"Tesseract (software) Google Tesseract OCR ShareX PyAutoGUI pyscreenshot pytesseract pyperclip Watchdog","title":"References"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/libraries/","text":"Libraries Sub-Areas Kivy Requests Selenium Streamlit","title":"Libraries"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/libraries/#libraries","text":"","title":"Libraries"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/libraries/#sub-areas","text":"Kivy Requests Selenium Streamlit","title":"Sub-Areas"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/kivy/build-a-mobile-bot-with-the-kivy-python-framework/","text":"Python doesn\u2019t have built-in mobile development capabilities, but there are packages you can use to create mobile applications, like Kivy, PyQt , or even Beeware\u2019s Toga library. These libraries are all major players in the Python mobile space. However, there are some benefits you\u2019ll see if you choose to create mobile applications with Kivy . Not only will your application look the same on all platforms, but you also won\u2019t need to compile your code after every change. What\u2019s more, you\u2019ll be able to use Python\u2019s clear syntax to build your applications. In this tutorial, you\u2019ll learn how to: Work with Kivy widgets Lay out the UI Add events Use the KV language Create a calculator application Package your application for iOS, Android, Windows, and macOS This tutorial assumes you\u2019re familiar with object-oriented programming. If you\u2019re not, then check out [[Object-Oriented Programming (OOP) in Python 3]] Let\u2019s get started. Understanding the Kivy Framework Fresh \u00b6 Kivy is made for today and tomorrow. Novel input methods such as Multi-Touch have become increasingly important. We created Kivy from scratch, specifically for this kind of interaction. That means we were able to rethink many things in terms of human computer interaction, whereas older (not to mean \u2018outdated\u2019, rather \u2018well-established\u2019) toolkits carry their legacy, which is often a burden. We\u2019re not trying to force this new approach to using a computer into the corset of existing models (say single-pointer mouse interaction). We want to let it flourish and let you explore the possibilities. This is what really sets Kivy apart. Fast \u00b6 Kivy is fast. This applies to both application development and application execution speeds. We have optimized Kivy in many ways. We implement time-critical functionality on the C level to leverage the power of existing compilers. More importantly, we also use intelligent algorithms to minimize costly operations. We also use the GPU wherever it makes sense in our context. The computational power of today\u2019s graphics cards surpasses that of today\u2019s CPUs by far for some tasks and algorithms, especially drawing. That\u2019s why we try to let the GPU do as much of the work as possible, thus increasing performance considerably. Flexible \u00b6 Kivy is flexible. This means it can be run on a variety of different devices , including Android powered smartphones and tablets. We support all major operating systems (Windows, Linux, OS X). Being flexible also means that Kivy\u2019s fast-paced development allows it to adapt to new technologies quickly . More than once have we added support for new external devices and software protocols, sometimes even before they were released. Lastly, Kivy is also flexible in that it is possible to use it in combination with a great number of different third-party solutions. For example, on Windows we support WM_TOUCH, which means that any device that has Windows 7 Pen & Touch drivers will just work with Kivy. On OS X you can use Apple\u2019s Multi-Touch capable devices, such as trackpads and mice. On Linux, you can use HID kernel input events. In addition to that, we support TUIO (Tangible User Interface Objects) and a number of other input sources. Free \u00b6 Kivy is free to use. You don\u2019t have to pay for it. You don\u2019t even have to pay for it if you\u2019re making money out of selling an application that uses Kivy. Installing Kivy python -m pip install Kivy Working With Kivy Widgets A widget is an onscreen control that the user will interact with. All graphical user interface toolkits come with a set of widgets. Some common widgets that you may have used include buttons, combo boxes, and tabs. Kivy has many widgets built into its framework. Running a \u201cHello, Kivy!\u201d Program To see how Kivy works, take a look at the following \u201cHello, World!\u201d application:","title":"Build a mobile bot with the kivy python framework"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/kivy/build-a-mobile-bot-with-the-kivy-python-framework/#understanding-the-kivy-framework","text":"","title":"Understanding the Kivy Framework"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/kivy/build-a-mobile-bot-with-the-kivy-python-framework/#fresh","text":"Kivy is made for today and tomorrow. Novel input methods such as Multi-Touch have become increasingly important. We created Kivy from scratch, specifically for this kind of interaction. That means we were able to rethink many things in terms of human computer interaction, whereas older (not to mean \u2018outdated\u2019, rather \u2018well-established\u2019) toolkits carry their legacy, which is often a burden. We\u2019re not trying to force this new approach to using a computer into the corset of existing models (say single-pointer mouse interaction). We want to let it flourish and let you explore the possibilities. This is what really sets Kivy apart.","title":"Fresh\u00b6"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/kivy/build-a-mobile-bot-with-the-kivy-python-framework/#fast","text":"Kivy is fast. This applies to both application development and application execution speeds. We have optimized Kivy in many ways. We implement time-critical functionality on the C level to leverage the power of existing compilers. More importantly, we also use intelligent algorithms to minimize costly operations. We also use the GPU wherever it makes sense in our context. The computational power of today\u2019s graphics cards surpasses that of today\u2019s CPUs by far for some tasks and algorithms, especially drawing. That\u2019s why we try to let the GPU do as much of the work as possible, thus increasing performance considerably.","title":"Fast\u00b6"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/kivy/build-a-mobile-bot-with-the-kivy-python-framework/#flexible","text":"Kivy is flexible. This means it can be run on a variety of different devices , including Android powered smartphones and tablets. We support all major operating systems (Windows, Linux, OS X). Being flexible also means that Kivy\u2019s fast-paced development allows it to adapt to new technologies quickly . More than once have we added support for new external devices and software protocols, sometimes even before they were released. Lastly, Kivy is also flexible in that it is possible to use it in combination with a great number of different third-party solutions. For example, on Windows we support WM_TOUCH, which means that any device that has Windows 7 Pen & Touch drivers will just work with Kivy. On OS X you can use Apple\u2019s Multi-Touch capable devices, such as trackpads and mice. On Linux, you can use HID kernel input events. In addition to that, we support TUIO (Tangible User Interface Objects) and a number of other input sources.","title":"Flexible\u00b6"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/kivy/build-a-mobile-bot-with-the-kivy-python-framework/#free","text":"Kivy is free to use. You don\u2019t have to pay for it. You don\u2019t even have to pay for it if you\u2019re making money out of selling an application that uses Kivy.","title":"Free\u00b6"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/kivy/build-a-mobile-bot-with-the-kivy-python-framework/#installing-kivy","text":"python -m pip install Kivy","title":"Installing Kivy"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/kivy/build-a-mobile-bot-with-the-kivy-python-framework/#working-with-kivy-widgets","text":"A widget is an onscreen control that the user will interact with. All graphical user interface toolkits come with a set of widgets. Some common widgets that you may have used include buttons, combo boxes, and tabs. Kivy has many widgets built into its framework.","title":"Working With Kivy Widgets"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/kivy/build-a-mobile-bot-with-the-kivy-python-framework/#running-a-hello-kivy-program","text":"To see how Kivy works, take a look at the following \u201cHello, World!\u201d application:","title":"Running a \u201cHello, Kivy!\u201d Program"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/requests/http-for-humans/","text":"Requests is an elegant and simple HTTP library for Python, built for human beings.","title":"Http for humans"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/selenium/selenium/","text":"Selenium","title":"Selenium"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/selenium/selenium/#selenium","text":"","title":"Selenium"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/streamlit/scripts/cdwa-customtkinter/cdwa/","text":"","title":"Cdwa"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/libraries/streamlit/scripts/cdwa-customtkinter/instructions/","text":"Click on the \"Submit timesheet for {date}\" button. ![[CDWA_d6GOkU03Jj.png]] Click on the personal care slot that correlates to today's date to submit your shift time.![[chrome_us0wvFhcaV.png]] Before inputting the time worked for a specific day, ensure that a minimum of two tasks are selected, preferably \"Housework\" and \"Meal Preparation.\" ![[chrome_x2luFKkrpZ.png]] Submit the timesheet with both the hours and minutes filled.![[chrome_wXyLyvVzQm.png]] Click \"OK\" to accept CDWA's Medicaid Fraud attestation. ![[chrome_RayidmQXNZ.png]] Congratulations on successfully submitting the timesheet! There will be a green pop-up in the upper right hand of the webpage as a confirmation. ![[chrome_pavnELYfbo.png]] Exit out of the Chrome instance if there are no more shifts to submit.","title":"Instructions"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/scripts/pdf-to-png/","text":"In this blog we'll learn how you can convert a PDF into any image format (PNG, JPG and JPEG) using Python. We're going to be creating a basic GUI with the Tkinter Python module. This GUI will take the user inputted PDF file from the local directory and convert into an image format. Step 1 1 st of all Download Poppler from here here ,Then extract it. In the code section just add poppler_path=r\u2019C:Program Filespoppler-0.68.0bin\u2019 (for eg.) like below Step 2 you need to install pdf2image library by using the commond below # install this library pip install pdf2image Step 2 you need to install pdf2image library by using the commond below # install this library pip install pdf2image step 3 Write a python script and also put download file name poppler into same directly # http://blog.alivate.com.au/poppler-windows/ from tkinter import filedialog as fd filename = fd.askopenfilename() from pdf2image import convert\\_from\\_path from tkinter import \\* from tkinter import messagebox print(filename) def pdf2img(): try: images = convert\\_from\\_path(filename,dpi=200,poppler\\_path=r'poppler-0.68.0\\\\bin') for i, image in enumerate(images): fname = 'image'+str(i)+'.png' image.save(fname, \"PNG\") except : Result = \"NO pdf found\" messagebox.showinfo(\"Result\", Result) else: Result = \"success\" messagebox.showinfo(\"Result\", Result) master = Tk() Label(master, text=\"File Location\").grid(row=0, sticky=W) b = Button(master, text=\"Convert\", command=pdf2img) b.grid(row=0, column=2,columnspan=2, rowspan=2,padx=5, pady=5) mainloop() Run the script and it will show you results like this https://youtu.be/3XkwR_D9WoY","title":"Pdf to png"},{"location":"areas/technology-and-applied-sciences/computing/programming/python/scripts/pdf-to-png/#step-1","text":"1 st of all Download Poppler from here here ,Then extract it. In the code section just add poppler_path=r\u2019C:Program Filespoppler-0.68.0bin\u2019 (for eg.) like below Step 2 you need to install pdf2image library by using the commond below # install this library pip install pdf2image Step 2 you need to install pdf2image library by using the commond below # install this library pip install pdf2image step 3 Write a python script and also put download file name poppler into same directly # http://blog.alivate.com.au/poppler-windows/ from tkinter import filedialog as fd filename = fd.askopenfilename() from pdf2image import convert\\_from\\_path from tkinter import \\* from tkinter import messagebox print(filename) def pdf2img(): try: images = convert\\_from\\_path(filename,dpi=200,poppler\\_path=r'poppler-0.68.0\\\\bin') for i, image in enumerate(images): fname = 'image'+str(i)+'.png' image.save(fname, \"PNG\") except : Result = \"NO pdf found\" messagebox.showinfo(\"Result\", Result) else: Result = \"success\" messagebox.showinfo(\"Result\", Result) master = Tk() Label(master, text=\"File Location\").grid(row=0, sticky=W) b = Button(master, text=\"Convert\", command=pdf2img) b.grid(row=0, column=2,columnspan=2, rowspan=2,padx=5, pady=5) mainloop() Run the script and it will show you results like this https://youtu.be/3XkwR_D9WoY","title":"Step 1"},{"location":"areas/technology-and-applied-sciences/computing/programming/scratch/scratch/","text":"Scratch","title":"Scratch"},{"location":"areas/technology-and-applied-sciences/computing/programming/scratch/scratch/#scratch","text":"","title":"Scratch"},{"location":"areas/technology-and-applied-sciences/computing/programming/unified-modeling-language/20221021204502---unified-modeling-language-%28uml%29-class-diagram-tutorial/","text":"UML Class Diagram Tutorial 0:08 ![[Pasted image 20221013224452.jpg]] UML Class Diagram 0:11 ![[Pasted image 20221013224503.jpg]] - The basics of UML class diagrams - Relationships in UML class diagrams - How to interpret UML class diagrams 0:50 ![[Pasted image 20221013224509.jpg]] The purpose of a class is to represent some entity in the system being modeled. 1:13 ![[Pasted image 20221013224514.jpg]] Attributes are significant pieces of data that contain values that describe each instance of a class. 1:36 ![[Pasted image 20221013224551.jpg]] Visibility is optional, but if you include it, it goes first. After that, you write the attribute's name in lowercase, followed by a colon and the data type. 1:52 ![[Pasted image 20221013224608.jpg]] In Unified Modeling Language (UML), methods are the behavior-related parts of each class in an object-oriented program. These methods specify how objects in a program interact with each other and how they modify or retrieve data. 2:25 ![[Pasted image 20221013224613.jpg]] Visibility refers to the ability of other classes to see or access a given property or class member. 3:05 ![[Pasted image 20221013224620.jpg]] The symbols for visibility in UML are the minus sign, the plus sign, the hash, and the tilde. The minus sign indicates that an attribute or method is private, the plus sign indicates that an attribute or method is public, the hash indicates that an attribute or method is protected, and the tilde indicates that an attribute or method has package or default visibility. 3:35 ![[Pasted image 20221013224627.jpg]] Private attributes: name, employee ID, phone number, and department. Public method: updating the phone number. 5:19 ![[Pasted image 20221013224634.jpg]] Abstraction is a process of hiding the implementation details from the user and showing only functionality. 5:29 ![[Pasted image 20221013224640.jpg]] Inheritance is when a child class inherits the attributes and methods of a parent class. 5:53 ![[Pasted image 20221013224648.jpg]] An association relationship is a relationship between two classes in which one class is associated with the other. 6:23 ![[Pasted image 20221013224657.jpg]] An aggregation relationship is a special type of association that specifies a whole and its parts. In other words, it is a relationship between a group of objects and the individual objects that make up the group. 7:04 ![[Pasted image 20221013224705.jpg]] A composition relationship between two classes exists when the child class couldn't exist without the parent class. In other words, the child object is an integral part of the parent object. 7:32 ![[Pasted image 20221013224714.jpg]] Multiplicity allows you to set numerical constraints on your relationships. 7:48 ![[Pasted image 20221013224718.jpg]] Other types of multiplicity are zero to one, one to many, or a specific number range. 8:25 ![[Pasted image 20221013224730.jpg]] The user class consists of attributes for user ID, password, login status, and register date; and a public method for verified login, returning a Boolean. 8:50 ![[Pasted image 20221013224734.jpg]] The customer class is a class that inherits from the user class. It has its own specific attributes and methods, like being able to register, login and update the profile. 8:59 ![[Pasted image 20221013224738.jpg]] Composition relationships are when the parts cannot exist without the whole. For example, if an instance of the customer class was destroyed, his shopping cart and orders would be lost. They can't exist outside of the customer. 9:27 ![[Pasted image 20221013224743.jpg]] Multiplicity is used to indicate how many of one thing are related to another. In the first example, the multiplicity indicates that a customer can have zero or many orders. This means that a customer may never place an order, or they may place multiple orders. 9:49 ![[Pasted image 20221013224748.jpg]] In the second example, the multiplicity indicates that each order has one and only one order detail. This means that each order can only have one set of order details.","title":"20221021204502   unified modeling language (uml) class diagram tutorial"},{"location":"areas/technology-and-applied-sciences/computing/software-development/software_development/","text":"Software development","title":"Software development"},{"location":"areas/technology-and-applied-sciences/computing/software-development/software_development/#software-development","text":"","title":"Software development"},{"location":"blog/","text":"Blog","title":"Blog"},{"location":"blog/#blog","text":"","title":"Blog"},{"location":"blog/posts/11zpjrka6/","text":"Building an Indexing Pipeline for LinkedIn Skill Assessments Quizzes Repository Creating an efficient indexing pipeline for the linkedin-skill-assessments-quizzes repository involves systematic cloning, data processing, indexing, and query service setup. This comprehensive guide will walk you through each step with detailed code snippets, leveraging the Whoosh library for indexing. Pre-requisites You want to create a new directory and name it linkedin_skill_assessments_quizzes , you need to first open the command prompt in the current working directory. To do this, you can use the following command in the command prompt. cd path_to_your_current_working_directory Replace path_to_your_current_working_directory with the actual path where you want to create the new directory. Alternatively, on Windows, you can open the command prompt in the current working directory by clicking on the address bar and typing cmd , and pressing enter. Once you are in the desired working directory, create a new directory named linkedin_skill_assessments_quizzes by executing the following command: mkdir linkedin_skill_assessments_quizzes Now navigate to this new directory by executing the following command: cd linkedin_skill_assessments_quizzes This is where you will be cloning the repository and creating the indexing pipeline. Introduction LinkedIn Skill Assessments Quizzes is a repository of quizzes for various skills. It contains MD files for each quiz. The repository is available on GitHub . The repository has over 27,400 stars and 13,600 forks. It is a popular repository that is used by many people to prepare for interviews and improve their skills. Step 1: Cloning the Repository Start by cloning the repository to your local environment. This makes the content available for processing. git clone https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes.git Step 2: Converting the MD Files to JSON Processing the data involves parsing the MD files converting them to JSON format to extract the relevant information. The following code snippet demonstrates how to extract the question, answer, image, and options from the MD files and save them in a JSON file. This is required for indexing the data which we will cover in the next step. Add the following code to a file named process_data.py in the same directory where you cloned the repository. import os import json import markdown2 import re # Get the markdown files directory cloned_repository_directory = r \"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\" # Create a folder to store the JSON files output_folder = os . path . join ( cloned_repository_directory , \"json_output\" ) # Create the output folder if it doesn't exist os . makedirs ( output_folder , exist_ok = True ) # Create a list to store data for each MD file data_for_each_md = [] # Iterate through the Markdown files (\\*.md) in the current directory and its subdirectories for root , dirs , files in os . walk ( cloned_repository_directory ): for name in files : if name . endswith ( \".md\" ): # Construct the full path to the Markdown file md_file_path = os . path . join ( root , name ) # Read the Markdown file with open ( md_file_path , \"r\" , encoding = \"utf-8\" ) as md_file : md_content = md_file . read () # Split the content into sections for each question and answer sections = re . split ( r \"####\\s+Q\\d+\\.\" , md_content ) # Remove the first empty section sections . pop ( 0 ) # Create a list to store questions and answers for this MD file questions_and_answers = [] # Iterate through sections and extract questions and answers for section in sections : # Split the section into lines lines = section . strip () . split ( \" \\n \" ) # Extract the question question = lines [ 0 ] . strip () # Extract the answers answers = [ line . strip () for line in lines [ 1 :] if line . strip ()] # Create a dictionary for this question and answers qa_dict = { \"question\" : question , \"answers\" : answers } # Append to the list of questions and answers questions_and_answers . append ( qa_dict ) # Create a dictionary for this MD file md_data = { \"markdown_file\" : name , \"questions_and_answers\" : questions_and_answers , } # Append to the list of data for each MD file data_for_each_md . append ( md_data ) # Save JSON files in the output folder for md_data in data_for_each_md : json_file_name = os . path . splitext ( md_data [ \"markdown_file\" ])[ 0 ] + \".json\" json_file_path = os . path . join ( output_folder , json_file_name ) with open ( json_file_path , \"w\" , encoding = \"utf-8\" ) as json_file : json . dump ( md_data , json_file , indent = 4 ) print ( f \"JSON files created for each MD file in the ' { output_folder } ' folder.\" ) w Step 3: Indexing the Data After processing the data, you can index it to make it searchable. Indexing refers to the process of creating an index for the data. The following code snippet demonstrates how to index the data using the Whoosh library. This is how the indexing pipeline will work. Add the following code to a file named indexing_pipeline.py in the same directory where you cloned the repository. import os import json import whoosh from whoosh.fields import TEXT , ID , Schema from whoosh.index import create_in # Define the directory where your processed JSON files are located json_files_directory = r \"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\\json_output\" # Define the directory where you want to create the Whoosh index index_directory = r \"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\\index\" # Create the schema for the Whoosh index schema = Schema ( markdown_file = ID ( stored = True ), question = TEXT ( stored = True ), answers = TEXT ( stored = True ), ) # Create the index directory if it doesn't exist os . makedirs ( index_directory , exist_ok = True ) # Create the Whoosh index index = create_in ( index_directory , schema ) # Open the index writer writer = index . writer () # Iterate through JSON files and add documents to the index for json_file_name in os . listdir ( json_files_directory ): if json_file_name . endswith ( \".json\" ): json_file_path = os . path . join ( json_files_directory , json_file_name ) with open ( json_file_path , \"r\" , encoding = \"utf-8\" ) as json_file : json_data = json . load ( json_file ) # Extract 'question' and 'answers' from the JSON file question = json_data . get ( \"question\" , \"\" ) answers = json_data . get ( \"answers\" , []) # Combine 'question' and 'answers' into a single field for searching content = f \" { question } { ' ' . join ( answers ) } \" writer . add_document ( markdown_file = json_file_name , question = content , answers = answers , # Use the extracted 'answers' or an empty list if not present ) # Commit changes to the index writer . commit () print ( \"Indexing completed.\" ) Step 4: Setting Up the Query Service After indexing the data, you can set up a query service to search the index for a given search term. The following code snippet demonstrates how to set up a query service using the Whoosh library. This is how the query service will work. import os import json import re from whoosh.index import create_in , open_dir from whoosh.fields import Schema , TEXT , ID from whoosh.qparser import MultifieldParser from whoosh.analysis import StemmingAnalyzer # Define the schema for the index schema = Schema ( question = TEXT ( stored = True , analyzer = StemmingAnalyzer ()), answer = TEXT ( stored = True ), image = ID ( stored = True ), options = TEXT ( stored = True ), ) def create_search_index ( json_files_directory , index_dir ): if not os . path . exists ( index_dir ): os . makedirs ( index_dir ) index = create_in ( index_dir , schema ) writer = index . writer () for json_filename in os . listdir ( json_files_directory ): json_file_path = os . path . join ( json_files_directory , json_filename ) if json_file_path . endswith ( \".json\" ): try : with open ( json_file_path , \"r\" , encoding = \"utf-8\" ) as file : data = json . load ( file ) for question_data in data [ \"questions_and_answers\" ]: question_text = question_data [ \"question\" ] answer_text = \" \\n \" . join ( question_data [ \"answers\" ]) image_id = data . get ( \"image_id\" ) options = question_data . get ( \"options\" , \"\" ) writer . add_document ( question = question_text , answer = answer_text , image = image_id , options = options , ) except Exception as e : print ( f \"Failed to process file { json_file_path } : { e } \" ) writer . commit () print ( \"Indexing completed successfully.\" ) def extract_correct_answer ( answer_text ): # Use regular expression to find the portion with \"- [x]\" match = re . search ( r \"- \\[x\\].\\*\" , answer_text ) if match : return match . group () return None def search_index ( query_str , index_dir ): try : ix = open_dir ( index_dir ) with ix . searcher () as searcher : parser = MultifieldParser ([ \"question\" , \"options\" ], schema = ix . schema ) query = parser . parse ( query_str ) results = searcher . search ( query , limit = None ) print ( f \"Search for ' { query_str } ' returned { len ( results ) } results.\" ) return [ { \"question\" : result [ \"question\" ], \"correct_answer\" : extract_correct_answer ( result [ \"answer\" ]), \"image\" : result . get ( \"image\" ), } for result in results ] except Exception as e : print ( \"An error occurred during the search.\" ) return [] if __name__ == \"__main__\" : json_files_directory = r \"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\\json_output\" # Replace with your JSON files directory path index_dir = \"index\" # Replace with your index directory path create_search_index ( json_files_directory , index_dir ) original_string = \"Why would you use a virtual environment?\" # Replace with your actual search term # Remove the special characters from the original string query_string = re . sub ( r \"[^a-zA-Z0-9\\s]\" , \"\" , original_string ) query_string = query_string . lower () query_string = query_string . strip () search_results = search_index ( query_string , index_dir ) if search_results : for result in search_results : print ( f \"Question: { result [ 'question' ] } \" ) # Remove the \"- [x]\" portion from the answer print ( f \"Correct answer: { result [ 'correct_answer' ] . replace ( '- [x] ' , '' ) } \" ) if result . get ( \"image\" ): print ( f \"Image: { result [ 'image' ] } \" ) print ( \" \\n \" ) print ( f \"Search for ' { original_string } ' completed successfully.\" ) print ( f \"Found { len ( search_results ) } results.\" ) else : print ( \"No results found.\" ) Conclusion Creating an efficient indexing pipeline for the 'linkedin-skill-assessments-quizzes' repository involves systematic cloning, data processing, indexing, and query service setup. This comprehensive guide has walked you through each step with detailed code snippets, leveraging the Whoosh library for indexing. You should now be able to query the index and get the results. The script will print the question, answer, and image (if available) for each result. Since the data is indexed, you can easily search for a given term and get the results. This can be useful for finding the answers to specific questions or searching for a particular topic. You can also use the query service to create a web application that allows users to search the index and get the results. References Whoosh Documentation","title":"Building an Indexing Pipeline for LinkedIn Skill Assessments Quizzes Repository"},{"location":"blog/posts/11zpjrka6/#building-an-indexing-pipeline-for-linkedin-skill-assessments-quizzes-repository","text":"Creating an efficient indexing pipeline for the linkedin-skill-assessments-quizzes repository involves systematic cloning, data processing, indexing, and query service setup. This comprehensive guide will walk you through each step with detailed code snippets, leveraging the Whoosh library for indexing.","title":"Building an Indexing Pipeline for LinkedIn Skill Assessments Quizzes Repository"},{"location":"blog/posts/11zpjrka6/#pre-requisites","text":"You want to create a new directory and name it linkedin_skill_assessments_quizzes , you need to first open the command prompt in the current working directory. To do this, you can use the following command in the command prompt. cd path_to_your_current_working_directory Replace path_to_your_current_working_directory with the actual path where you want to create the new directory. Alternatively, on Windows, you can open the command prompt in the current working directory by clicking on the address bar and typing cmd , and pressing enter. Once you are in the desired working directory, create a new directory named linkedin_skill_assessments_quizzes by executing the following command: mkdir linkedin_skill_assessments_quizzes Now navigate to this new directory by executing the following command: cd linkedin_skill_assessments_quizzes This is where you will be cloning the repository and creating the indexing pipeline.","title":"Pre-requisites"},{"location":"blog/posts/11zpjrka6/#introduction","text":"LinkedIn Skill Assessments Quizzes is a repository of quizzes for various skills. It contains MD files for each quiz. The repository is available on GitHub . The repository has over 27,400 stars and 13,600 forks. It is a popular repository that is used by many people to prepare for interviews and improve their skills.","title":"Introduction"},{"location":"blog/posts/11zpjrka6/#step-1-cloning-the-repository","text":"Start by cloning the repository to your local environment. This makes the content available for processing. git clone https://github.com/Ebazhanov/linkedin-skill-assessments-quizzes.git","title":"Step 1: Cloning the Repository"},{"location":"blog/posts/11zpjrka6/#step-2-converting-the-md-files-to-json","text":"Processing the data involves parsing the MD files converting them to JSON format to extract the relevant information. The following code snippet demonstrates how to extract the question, answer, image, and options from the MD files and save them in a JSON file. This is required for indexing the data which we will cover in the next step. Add the following code to a file named process_data.py in the same directory where you cloned the repository. import os import json import markdown2 import re # Get the markdown files directory cloned_repository_directory = r \"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\" # Create a folder to store the JSON files output_folder = os . path . join ( cloned_repository_directory , \"json_output\" ) # Create the output folder if it doesn't exist os . makedirs ( output_folder , exist_ok = True ) # Create a list to store data for each MD file data_for_each_md = [] # Iterate through the Markdown files (\\*.md) in the current directory and its subdirectories for root , dirs , files in os . walk ( cloned_repository_directory ): for name in files : if name . endswith ( \".md\" ): # Construct the full path to the Markdown file md_file_path = os . path . join ( root , name ) # Read the Markdown file with open ( md_file_path , \"r\" , encoding = \"utf-8\" ) as md_file : md_content = md_file . read () # Split the content into sections for each question and answer sections = re . split ( r \"####\\s+Q\\d+\\.\" , md_content ) # Remove the first empty section sections . pop ( 0 ) # Create a list to store questions and answers for this MD file questions_and_answers = [] # Iterate through sections and extract questions and answers for section in sections : # Split the section into lines lines = section . strip () . split ( \" \\n \" ) # Extract the question question = lines [ 0 ] . strip () # Extract the answers answers = [ line . strip () for line in lines [ 1 :] if line . strip ()] # Create a dictionary for this question and answers qa_dict = { \"question\" : question , \"answers\" : answers } # Append to the list of questions and answers questions_and_answers . append ( qa_dict ) # Create a dictionary for this MD file md_data = { \"markdown_file\" : name , \"questions_and_answers\" : questions_and_answers , } # Append to the list of data for each MD file data_for_each_md . append ( md_data ) # Save JSON files in the output folder for md_data in data_for_each_md : json_file_name = os . path . splitext ( md_data [ \"markdown_file\" ])[ 0 ] + \".json\" json_file_path = os . path . join ( output_folder , json_file_name ) with open ( json_file_path , \"w\" , encoding = \"utf-8\" ) as json_file : json . dump ( md_data , json_file , indent = 4 ) print ( f \"JSON files created for each MD file in the ' { output_folder } ' folder.\" ) w","title":"Step 2: Converting the MD Files to JSON"},{"location":"blog/posts/11zpjrka6/#step-3-indexing-the-data","text":"After processing the data, you can index it to make it searchable. Indexing refers to the process of creating an index for the data. The following code snippet demonstrates how to index the data using the Whoosh library. This is how the indexing pipeline will work. Add the following code to a file named indexing_pipeline.py in the same directory where you cloned the repository. import os import json import whoosh from whoosh.fields import TEXT , ID , Schema from whoosh.index import create_in # Define the directory where your processed JSON files are located json_files_directory = r \"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\\json_output\" # Define the directory where you want to create the Whoosh index index_directory = r \"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\\index\" # Create the schema for the Whoosh index schema = Schema ( markdown_file = ID ( stored = True ), question = TEXT ( stored = True ), answers = TEXT ( stored = True ), ) # Create the index directory if it doesn't exist os . makedirs ( index_directory , exist_ok = True ) # Create the Whoosh index index = create_in ( index_directory , schema ) # Open the index writer writer = index . writer () # Iterate through JSON files and add documents to the index for json_file_name in os . listdir ( json_files_directory ): if json_file_name . endswith ( \".json\" ): json_file_path = os . path . join ( json_files_directory , json_file_name ) with open ( json_file_path , \"r\" , encoding = \"utf-8\" ) as json_file : json_data = json . load ( json_file ) # Extract 'question' and 'answers' from the JSON file question = json_data . get ( \"question\" , \"\" ) answers = json_data . get ( \"answers\" , []) # Combine 'question' and 'answers' into a single field for searching content = f \" { question } { ' ' . join ( answers ) } \" writer . add_document ( markdown_file = json_file_name , question = content , answers = answers , # Use the extracted 'answers' or an empty list if not present ) # Commit changes to the index writer . commit () print ( \"Indexing completed.\" )","title":"Step 3: Indexing the Data"},{"location":"blog/posts/11zpjrka6/#step-4-setting-up-the-query-service","text":"After indexing the data, you can set up a query service to search the index for a given search term. The following code snippet demonstrates how to set up a query service using the Whoosh library. This is how the query service will work. import os import json import re from whoosh.index import create_in , open_dir from whoosh.fields import Schema , TEXT , ID from whoosh.qparser import MultifieldParser from whoosh.analysis import StemmingAnalyzer # Define the schema for the index schema = Schema ( question = TEXT ( stored = True , analyzer = StemmingAnalyzer ()), answer = TEXT ( stored = True ), image = ID ( stored = True ), options = TEXT ( stored = True ), ) def create_search_index ( json_files_directory , index_dir ): if not os . path . exists ( index_dir ): os . makedirs ( index_dir ) index = create_in ( index_dir , schema ) writer = index . writer () for json_filename in os . listdir ( json_files_directory ): json_file_path = os . path . join ( json_files_directory , json_filename ) if json_file_path . endswith ( \".json\" ): try : with open ( json_file_path , \"r\" , encoding = \"utf-8\" ) as file : data = json . load ( file ) for question_data in data [ \"questions_and_answers\" ]: question_text = question_data [ \"question\" ] answer_text = \" \\n \" . join ( question_data [ \"answers\" ]) image_id = data . get ( \"image_id\" ) options = question_data . get ( \"options\" , \"\" ) writer . add_document ( question = question_text , answer = answer_text , image = image_id , options = options , ) except Exception as e : print ( f \"Failed to process file { json_file_path } : { e } \" ) writer . commit () print ( \"Indexing completed successfully.\" ) def extract_correct_answer ( answer_text ): # Use regular expression to find the portion with \"- [x]\" match = re . search ( r \"- \\[x\\].\\*\" , answer_text ) if match : return match . group () return None def search_index ( query_str , index_dir ): try : ix = open_dir ( index_dir ) with ix . searcher () as searcher : parser = MultifieldParser ([ \"question\" , \"options\" ], schema = ix . schema ) query = parser . parse ( query_str ) results = searcher . search ( query , limit = None ) print ( f \"Search for ' { query_str } ' returned { len ( results ) } results.\" ) return [ { \"question\" : result [ \"question\" ], \"correct_answer\" : extract_correct_answer ( result [ \"answer\" ]), \"image\" : result . get ( \"image\" ), } for result in results ] except Exception as e : print ( \"An error occurred during the search.\" ) return [] if __name__ == \"__main__\" : json_files_directory = r \"C:\\Users\\Harminder Nijjar\\Desktop\\blog\\kb-blog-portfolio-mkdocs-master\\scripts\\linkedin-skill-assessments-quizzes\\json_output\" # Replace with your JSON files directory path index_dir = \"index\" # Replace with your index directory path create_search_index ( json_files_directory , index_dir ) original_string = \"Why would you use a virtual environment?\" # Replace with your actual search term # Remove the special characters from the original string query_string = re . sub ( r \"[^a-zA-Z0-9\\s]\" , \"\" , original_string ) query_string = query_string . lower () query_string = query_string . strip () search_results = search_index ( query_string , index_dir ) if search_results : for result in search_results : print ( f \"Question: { result [ 'question' ] } \" ) # Remove the \"- [x]\" portion from the answer print ( f \"Correct answer: { result [ 'correct_answer' ] . replace ( '- [x] ' , '' ) } \" ) if result . get ( \"image\" ): print ( f \"Image: { result [ 'image' ] } \" ) print ( \" \\n \" ) print ( f \"Search for ' { original_string } ' completed successfully.\" ) print ( f \"Found { len ( search_results ) } results.\" ) else : print ( \"No results found.\" )","title":"Step 4: Setting Up the Query Service"},{"location":"blog/posts/11zpjrka6/#conclusion","text":"Creating an efficient indexing pipeline for the 'linkedin-skill-assessments-quizzes' repository involves systematic cloning, data processing, indexing, and query service setup. This comprehensive guide has walked you through each step with detailed code snippets, leveraging the Whoosh library for indexing. You should now be able to query the index and get the results. The script will print the question, answer, and image (if available) for each result. Since the data is indexed, you can easily search for a given term and get the results. This can be useful for finding the answers to specific questions or searching for a particular topic. You can also use the query service to create a web application that allows users to search the index and get the results.","title":"Conclusion"},{"location":"blog/posts/11zpjrka6/#references","text":"Whoosh Documentation","title":"References"},{"location":"blog/posts/56zvvow4p/","text":"Transferring Script Files to Local System or VPS Local System Setup Process (Windows) This document outlines the process for transferring a Python script and setting it up on your local system. The script, in this case, is a Facebook Marketplace Scraper that allows you to collect and manage data from online listings. Prerequisites Before proceeding with the setup, ensure you have the following prerequisites ready: Python installed on your system (Python 3.6 or higher is recommended). Access to a Google Cloud project with required credentials for Google Sheets API. SQLite database support. A Telegram bot token (if you wish to receive notifications). Dependencies listed in the requirements.txt file provided with the script. Setup Steps Step 1: Obtain Script Files 1.1. Obtain the necessary script files from your source, typically provided as a ZIP archive or downloadable files. 1.2. Ensure you have the following script files: fb_parser.py : The main Python script. requirements.txt : A file containing the required Python dependencies. Step 2: Install Dependencies 2.1. Open a terminal/command prompt and navigate to the directory containing the script files. 2.2. Install the required Python dependencies using the following command: pip install -r requirements.txt This command installs packages such as requests , beautifulsoup4 , and others. Step 3: Configure Credentials 3.1. Set up Google Cloud credentials for accessing the Google Sheets API: Create or use an existing Google Cloud project. Enable the Google Sheets API for your project. Create OAuth 2.0 credentials for a desktop application and download the credentials.json file. Place the credentials.json file in the same directory as the script. Step 4: Initialize the Database 4.1. Initialize the SQLite database by running the following command in the script's directory: python fb_parser.py --initdb This command creates the SQLite database file ( market_listings.db ) in the script's directory. Step 5: Configure Telegram Bot Token (Optional) 5.1. If you want to receive notifications via Telegram, edit the fb_parser.py script and update the bot_token and bot_chat_id variables with your own values. Step 6: Run the Scraper 6.1. Start the scraper by running the following command in the script's directory: python fb_parser.py The scraper will begin collecting data from Facebook Marketplace listings, and notifications will be sent if configured. Step 7: Monitor and Review 7.1. Monitor the script's output for any messages or errors. 7.2. Review the Google Sheets document to ensure that it's collecting data accurately. Step 8: Ongoing Management 8.1. Consider setting up automated scheduling, if required, to run the scraper at specific intervals. VPS Setup Process Overview This document outlines the process for transferring a Python script and setting it up on your VPS (Virtual Private Server). The script, in this case, is a Facebook Marketplace Scraper designed to collect and manage data from online listings. Prerequisites Before proceeding with the setup, ensure you have the following prerequisites ready: Access to a VPS : You should have access to a VPS with administrative privileges. You can obtain VPS services from providers like AWS, DigitalOcean, or any other preferred hosting provider. Operating System : The VPS should be running a compatible operating system, preferably a Linux distribution such as Ubuntu or CentOS. Python Installed : Python 3.6 or higher should be installed on your VPS. You can check the installed Python version using the python3 --version command. Access to SSH : Ensure you can access your VPS via SSH (Secure Shell) with a terminal or SSH client. Script Files : Obtain the necessary script files for the Facebook Marketplace Scraper. These files are typically provided as a ZIP archive or downloadable files. Dependencies : Review the script's documentation to identify and install any required Python dependencies. Setup Steps Step 1: Access Your VPS Log in to your VPS using SSH. You should have received SSH credentials from your hosting provider. ssh username@hostname Replace username with your VPS username and your-vps-ip with the actual IP address or hostname of your VPS. Step 2: Upload Script Files Transfer the necessary script files to your VPS. You can use secure file transfer methods like SCP or SFTP to upload files from your local machine to the VPS. Step 3: Install Python Dependencies Install the required Python dependencies on your VPS. Use the package manager appropriate for your Linux distribution. For example, on Ubuntu, you can use apt-get : sudo apt-get update sudo apt-get install python3-pip pip3 install -r requirements.txt Replace requirements.txt with the actual filename containing the dependencies. Step 4: Configure Credentials Set up any necessary credentials for the script. This may include configuring API keys, OAuth tokens, or other authentication details required for your specific use case. Google Sheets API Go to the Google Cloud Console. Create a new project if you don't have one. In the project dashboard, navigate to \"APIs & Services\" > \"Credentials.\" Click on \"Create credentials\" and choose \"OAuth client ID.\" Configure the OAuth consent screen with the necessary details. Select \"Desktop App\" as the application type. Create the OAuth client ID. Download the JSON credentials file (usually named credentials.json). Telegram Bot API (Chat ID) Message the parser bot on Telegram. Navigate to the following URL in your browser: https://api.telegram.org/bot<yourtoken>/getUpdates Replace <yourtoken> with your bot's token. Look for the \"chat\" object in the response. The \"id\" value is your chat ID. Step 5: Execute the Script Run the Python script on your VPS. Navigate to the directory where you uploaded the script files and execute it. python3 fb_parser.py Replace fb_parser.py with the actual filename of the script. Monitor the script's output for any messages or errors. Depending on your VPS setup, you may choose to run the script in the background using tools like nohup or within a screen session for detached operation. Step 6: Ongoing Management Consider setting up automated scheduling, if required, to run the scraper at specific intervals. You can use tools like cron for scheduling periodic tasks on your VPS. Conclusion Transferring script files to your local system or VPS to set up a Facebook Marketplace Scraper is a straightforward process. By following the steps outlined in this document, you can quickly get started with the scraper and begin collecting data from online listings. References Facebook Marketplace Scraper Google Sheets API SQLite Telegram Bot API Python Smartproxy","title":"Transferring Script Files to Local System or VPS"},{"location":"blog/posts/56zvvow4p/#transferring-script-files-to-local-system-or-vps","text":"","title":"Transferring Script Files to Local System or VPS"},{"location":"blog/posts/56zvvow4p/#local-system-setup-process-windows","text":"This document outlines the process for transferring a Python script and setting it up on your local system. The script, in this case, is a Facebook Marketplace Scraper that allows you to collect and manage data from online listings.","title":"Local System Setup Process (Windows)"},{"location":"blog/posts/56zvvow4p/#prerequisites","text":"Before proceeding with the setup, ensure you have the following prerequisites ready: Python installed on your system (Python 3.6 or higher is recommended). Access to a Google Cloud project with required credentials for Google Sheets API. SQLite database support. A Telegram bot token (if you wish to receive notifications). Dependencies listed in the requirements.txt file provided with the script.","title":"Prerequisites"},{"location":"blog/posts/56zvvow4p/#setup-steps","text":"","title":"Setup Steps"},{"location":"blog/posts/56zvvow4p/#step-1-obtain-script-files","text":"1.1. Obtain the necessary script files from your source, typically provided as a ZIP archive or downloadable files. 1.2. Ensure you have the following script files: fb_parser.py : The main Python script. requirements.txt : A file containing the required Python dependencies.","title":"Step 1: Obtain Script Files"},{"location":"blog/posts/56zvvow4p/#step-2-install-dependencies","text":"2.1. Open a terminal/command prompt and navigate to the directory containing the script files. 2.2. Install the required Python dependencies using the following command: pip install -r requirements.txt This command installs packages such as requests , beautifulsoup4 , and others.","title":"Step 2: Install Dependencies"},{"location":"blog/posts/56zvvow4p/#step-3-configure-credentials","text":"3.1. Set up Google Cloud credentials for accessing the Google Sheets API: Create or use an existing Google Cloud project. Enable the Google Sheets API for your project. Create OAuth 2.0 credentials for a desktop application and download the credentials.json file. Place the credentials.json file in the same directory as the script.","title":"Step 3: Configure Credentials"},{"location":"blog/posts/56zvvow4p/#step-4-initialize-the-database","text":"4.1. Initialize the SQLite database by running the following command in the script's directory: python fb_parser.py --initdb This command creates the SQLite database file ( market_listings.db ) in the script's directory.","title":"Step 4: Initialize the Database"},{"location":"blog/posts/56zvvow4p/#step-5-configure-telegram-bot-token-optional","text":"5.1. If you want to receive notifications via Telegram, edit the fb_parser.py script and update the bot_token and bot_chat_id variables with your own values.","title":"Step 5: Configure Telegram Bot Token (Optional)"},{"location":"blog/posts/56zvvow4p/#step-6-run-the-scraper","text":"6.1. Start the scraper by running the following command in the script's directory: python fb_parser.py The scraper will begin collecting data from Facebook Marketplace listings, and notifications will be sent if configured.","title":"Step 6: Run the Scraper"},{"location":"blog/posts/56zvvow4p/#step-7-monitor-and-review","text":"7.1. Monitor the script's output for any messages or errors. 7.2. Review the Google Sheets document to ensure that it's collecting data accurately.","title":"Step 7: Monitor and Review"},{"location":"blog/posts/56zvvow4p/#step-8-ongoing-management","text":"8.1. Consider setting up automated scheduling, if required, to run the scraper at specific intervals.","title":"Step 8: Ongoing Management"},{"location":"blog/posts/56zvvow4p/#vps-setup-process","text":"","title":"VPS Setup Process"},{"location":"blog/posts/56zvvow4p/#overview","text":"This document outlines the process for transferring a Python script and setting it up on your VPS (Virtual Private Server). The script, in this case, is a Facebook Marketplace Scraper designed to collect and manage data from online listings.","title":"Overview"},{"location":"blog/posts/56zvvow4p/#prerequisites_1","text":"Before proceeding with the setup, ensure you have the following prerequisites ready: Access to a VPS : You should have access to a VPS with administrative privileges. You can obtain VPS services from providers like AWS, DigitalOcean, or any other preferred hosting provider. Operating System : The VPS should be running a compatible operating system, preferably a Linux distribution such as Ubuntu or CentOS. Python Installed : Python 3.6 or higher should be installed on your VPS. You can check the installed Python version using the python3 --version command. Access to SSH : Ensure you can access your VPS via SSH (Secure Shell) with a terminal or SSH client. Script Files : Obtain the necessary script files for the Facebook Marketplace Scraper. These files are typically provided as a ZIP archive or downloadable files. Dependencies : Review the script's documentation to identify and install any required Python dependencies.","title":"Prerequisites"},{"location":"blog/posts/56zvvow4p/#setup-steps_1","text":"","title":"Setup Steps"},{"location":"blog/posts/56zvvow4p/#step-1-access-your-vps","text":"Log in to your VPS using SSH. You should have received SSH credentials from your hosting provider. ssh username@hostname Replace username with your VPS username and your-vps-ip with the actual IP address or hostname of your VPS.","title":"Step 1: Access Your VPS"},{"location":"blog/posts/56zvvow4p/#step-2-upload-script-files","text":"Transfer the necessary script files to your VPS. You can use secure file transfer methods like SCP or SFTP to upload files from your local machine to the VPS.","title":"Step 2: Upload Script Files"},{"location":"blog/posts/56zvvow4p/#step-3-install-python-dependencies","text":"Install the required Python dependencies on your VPS. Use the package manager appropriate for your Linux distribution. For example, on Ubuntu, you can use apt-get : sudo apt-get update sudo apt-get install python3-pip pip3 install -r requirements.txt Replace requirements.txt with the actual filename containing the dependencies.","title":"Step 3: Install Python Dependencies"},{"location":"blog/posts/56zvvow4p/#step-4-configure-credentials","text":"Set up any necessary credentials for the script. This may include configuring API keys, OAuth tokens, or other authentication details required for your specific use case.","title":"Step 4: Configure Credentials"},{"location":"blog/posts/56zvvow4p/#google-sheets-api","text":"Go to the Google Cloud Console. Create a new project if you don't have one. In the project dashboard, navigate to \"APIs & Services\" > \"Credentials.\" Click on \"Create credentials\" and choose \"OAuth client ID.\" Configure the OAuth consent screen with the necessary details. Select \"Desktop App\" as the application type. Create the OAuth client ID. Download the JSON credentials file (usually named credentials.json).","title":"Google Sheets API"},{"location":"blog/posts/56zvvow4p/#telegram-bot-api-chat-id","text":"Message the parser bot on Telegram. Navigate to the following URL in your browser: https://api.telegram.org/bot<yourtoken>/getUpdates Replace <yourtoken> with your bot's token. Look for the \"chat\" object in the response. The \"id\" value is your chat ID.","title":"Telegram Bot API (Chat ID)"},{"location":"blog/posts/56zvvow4p/#step-5-execute-the-script","text":"Run the Python script on your VPS. Navigate to the directory where you uploaded the script files and execute it. python3 fb_parser.py Replace fb_parser.py with the actual filename of the script. Monitor the script's output for any messages or errors. Depending on your VPS setup, you may choose to run the script in the background using tools like nohup or within a screen session for detached operation.","title":"Step 5: Execute the Script"},{"location":"blog/posts/56zvvow4p/#step-6-ongoing-management","text":"Consider setting up automated scheduling, if required, to run the scraper at specific intervals. You can use tools like cron for scheduling periodic tasks on your VPS.","title":"Step 6: Ongoing Management"},{"location":"blog/posts/56zvvow4p/#conclusion","text":"Transferring script files to your local system or VPS to set up a Facebook Marketplace Scraper is a straightforward process. By following the steps outlined in this document, you can quickly get started with the scraper and begin collecting data from online listings.","title":"Conclusion"},{"location":"blog/posts/56zvvow4p/#references","text":"Facebook Marketplace Scraper Google Sheets API SQLite Telegram Bot API Python Smartproxy","title":"References"},{"location":"blog/posts/5790aml7p/","text":"OpenAI's Developer Conference: A New Era of AI Innovation GPT-4 Turbo with 128K context: Breaking Boundaries in Language Modeling OpenAI announced GPT4-Turbo at its November Developer Conference, a new language model that builds on the success of GPT-3. This model is designed to break boundaries in language modeling, offering increased context length, more control, better knowledge, new modalities, customization, and higher rate limits. As shown, GPT-4 Turbo offers a significant increase in the number of tokens it can handle in its context length, going from 8,000 tokens to 128,000 tokens. This represents a substantial enhancement in the model's ability to maintain context over longer conversations or documents. Compared to the standard GPT-4, this is a huge leap forward in terms of the amount of information that can be processed by the model. The new model also offers more control, specifically in terms of model inputs and outputs, and better knowledge, which includes updating the cut-off date for knowledge about the world to April 2023 and providing the ability for developers to easily add their own knowledge base. New modalities, such as DALL-E 3, Vision, and TTS (text-to-speech) will all be included in the API, with a new version of Whisper speech recognition coming. Customization, including fine-tuning and custom models (which, Altman warned, won\u2019t be cheap), and higher rate limits are also included in the new model, making it a comprehensive upgrade over its predecessors. Multimodal Capabilities: Expanding AI's Horizon GPT-4 Turbo with vision GPT-4 now integrates vision, allowing it to understand and analyze images, enhancing its capabilities beyond text. Developers can utilize this feature through the gpt-4-vision-preview model. It supports a range of applications, including caption generation and detailed image analysis, beneficial for services like BeMyEyes, which aids visually impaired individuals. The vision feature will soon be included in GPT-4's stable release. Costs vary by image size; for example, a 1080\u00d71080 image analysis costs approximately $0.00765. For more details, OpenAI provides a comprehensive vision guide and DALL\u00b7E 3 remains the tool for image generation. GPT-4 Turbo with vision analyzing Old School RuneScape through the RuneLite interface import base64 import logging import os import time from PIL import ImageGrab , Image import pyautogui as gui import pygetwindow as gw import requests # Set up logging to capture events when script runs and any possible errors. log_filename = 'rune_capture.log' # Replace with your desired log file name logging . basicConfig ( filename = log_filename , filemode = 'a' , level = logging . INFO , format = ' %(asctime)s - %(name)s - [ %(levelname)s ] [ %(pathname)s : %(lineno)d ] - %(message)s - [ %(process)d : %(thread)d ]' ) logger = logging . getLogger ( ** name ** ) # Set the client window title. client_window_title = \"RuneLite\" def capture_screenshot (): try : # Get the title of the client window. win = gw . getWindowsWithTitle ( client_window_title )[ 0 ] win . activate () time . sleep ( 1 ) # Get the client window's position. clientWindow = gw . getWindowsWithTitle ( client_window_title )[ 0 ] x1 , y1 = clientWindow . topleft x2 , y2 = clientWindow . bottomright # Define the screenshot path and crop area. path = \"gameWindow.png\" gui . screenshot ( path ) img = Image . open ( path ) img = img . crop (( x1 + 1 , y1 + 40 , x2 - 250 , y2 - 165 )) img . save ( path ) return path except Exception as e : logger . error ( f \"An error occurred while capturing screenshot: { e } \" ) raise def encode_image ( image_path ): try : with open ( image_path , \"rb\" ) as image_file : return base64 . b64encode ( image_file . read ()) . decode ( \"utf-8\" ) except Exception as e : logger . error ( f \"An error occurred while encoding image: { e } \" ) raise def send_image_to_api ( base64_image ): api_key = os . getenv ( \"OPENAI_API_KEY\" ) headers = { \"Content-Type\" : \"application/json\" , \"Authorization\" : f \"Bearer { api_key } \" } payload = { \"model\" : \"gpt-4-vision-preview\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [{ \"type\" : \"text\" , \"text\" : \"What\u2019s in this image?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : f \"data:image/jpeg;base64, { base64_image } \" }}]}, ], \"max_tokens\" : 300 , } try : response = requests . post ( \"https://api.openai.com/v1/chat/completions\" , headers = headers , json = payload ) response . raise_for_status () # Will raise an exception for HTTP errors. return response . json () except Exception as e : logger . error ( f \"An error occurred while sending image to API: { e } \" ) raise if ** name ** == \"**main**\" : try : # Perform the main operations. screenshot_path = capture_screenshot () base64_image = encode_image ( screenshot_path ) api_response = send_image_to_api ( base64_image ) print ( api_response ) except Exception as e : logger . error ( f \"An error occurred in the main function: { e } \" ) DALL\u00b7E 3 Developers can now access DALL\u00b7E 3, a multimodal model that generates images from text directly through the API by specifying dall-e-3 as the model. TTS (Text-to-Speech) OpenAI's newest model is available to generate human-quality speech from text via their text-to-speech API. Revenue-Sharing GPT Store: Empowering Creators The DevDay also cast a spotlight on the newly announced revenue-sharing GPT Store. This platform represents a strategic move towards a more inclusive creator economy within AI, offering compensation to creators of AI applications based on user engagement and usage. This initiative is a nod to the growing importance of content creators in the AI ecosystem and reflects a broader trend of recognizing and rewarding the contributions of individual developers and innovators. Microsoft Partnership and Azure's Role The ongoing collaboration with Microsoft was highlighted, with a focus on how Azure's infrastructure is being optimized to support OpenAI's sophisticated AI models. This partnership is a testament to the shared goal of accelerating AI innovation and enhancing integration across various services and platforms as well as Microsoft's heavy investment in AI. Safe and Gradual AI Integration OpenAI emphasized a strategic approach to AI integration, advocating for a balance between innovation and safety. The organization invites developers to engage with the new tools thoughtfully, ensuring a responsible progression of AI within different sectors. This measured approach is a reflection of OpenAI's commitment to the safe and ethical development of AI technologies. Conclusion The Developer Conference marked a notable milestone for OpenAI and the broader AI community. The launch of GPT4-Turbo and the introduction of new multimodal capabilities, combined with the support of Microsoft's Azure and the innovative revenue-sharing model of the GPT Store, heralds a new phase of growth and experimentation in AI applications.","title":"OpenAI's Developer Conference"},{"location":"blog/posts/5790aml7p/#openais-developer-conference-a-new-era-of-ai-innovation","text":"","title":"OpenAI's Developer Conference: A New Era of AI Innovation"},{"location":"blog/posts/5790aml7p/#gpt-4-turbo-with-128k-context-breaking-boundaries-in-language-modeling","text":"OpenAI announced GPT4-Turbo at its November Developer Conference, a new language model that builds on the success of GPT-3. This model is designed to break boundaries in language modeling, offering increased context length, more control, better knowledge, new modalities, customization, and higher rate limits. As shown, GPT-4 Turbo offers a significant increase in the number of tokens it can handle in its context length, going from 8,000 tokens to 128,000 tokens. This represents a substantial enhancement in the model's ability to maintain context over longer conversations or documents. Compared to the standard GPT-4, this is a huge leap forward in terms of the amount of information that can be processed by the model. The new model also offers more control, specifically in terms of model inputs and outputs, and better knowledge, which includes updating the cut-off date for knowledge about the world to April 2023 and providing the ability for developers to easily add their own knowledge base. New modalities, such as DALL-E 3, Vision, and TTS (text-to-speech) will all be included in the API, with a new version of Whisper speech recognition coming. Customization, including fine-tuning and custom models (which, Altman warned, won\u2019t be cheap), and higher rate limits are also included in the new model, making it a comprehensive upgrade over its predecessors.","title":"GPT-4 Turbo with 128K context: Breaking Boundaries in Language Modeling"},{"location":"blog/posts/5790aml7p/#multimodal-capabilities-expanding-ais-horizon","text":"","title":"Multimodal Capabilities: Expanding AI's Horizon"},{"location":"blog/posts/5790aml7p/#gpt-4-turbo-with-vision","text":"GPT-4 now integrates vision, allowing it to understand and analyze images, enhancing its capabilities beyond text. Developers can utilize this feature through the gpt-4-vision-preview model. It supports a range of applications, including caption generation and detailed image analysis, beneficial for services like BeMyEyes, which aids visually impaired individuals. The vision feature will soon be included in GPT-4's stable release. Costs vary by image size; for example, a 1080\u00d71080 image analysis costs approximately $0.00765. For more details, OpenAI provides a comprehensive vision guide and DALL\u00b7E 3 remains the tool for image generation. GPT-4 Turbo with vision analyzing Old School RuneScape through the RuneLite interface import base64 import logging import os import time from PIL import ImageGrab , Image import pyautogui as gui import pygetwindow as gw import requests # Set up logging to capture events when script runs and any possible errors. log_filename = 'rune_capture.log' # Replace with your desired log file name logging . basicConfig ( filename = log_filename , filemode = 'a' , level = logging . INFO , format = ' %(asctime)s - %(name)s - [ %(levelname)s ] [ %(pathname)s : %(lineno)d ] - %(message)s - [ %(process)d : %(thread)d ]' ) logger = logging . getLogger ( ** name ** ) # Set the client window title. client_window_title = \"RuneLite\" def capture_screenshot (): try : # Get the title of the client window. win = gw . getWindowsWithTitle ( client_window_title )[ 0 ] win . activate () time . sleep ( 1 ) # Get the client window's position. clientWindow = gw . getWindowsWithTitle ( client_window_title )[ 0 ] x1 , y1 = clientWindow . topleft x2 , y2 = clientWindow . bottomright # Define the screenshot path and crop area. path = \"gameWindow.png\" gui . screenshot ( path ) img = Image . open ( path ) img = img . crop (( x1 + 1 , y1 + 40 , x2 - 250 , y2 - 165 )) img . save ( path ) return path except Exception as e : logger . error ( f \"An error occurred while capturing screenshot: { e } \" ) raise def encode_image ( image_path ): try : with open ( image_path , \"rb\" ) as image_file : return base64 . b64encode ( image_file . read ()) . decode ( \"utf-8\" ) except Exception as e : logger . error ( f \"An error occurred while encoding image: { e } \" ) raise def send_image_to_api ( base64_image ): api_key = os . getenv ( \"OPENAI_API_KEY\" ) headers = { \"Content-Type\" : \"application/json\" , \"Authorization\" : f \"Bearer { api_key } \" } payload = { \"model\" : \"gpt-4-vision-preview\" , \"messages\" : [ { \"role\" : \"user\" , \"content\" : [{ \"type\" : \"text\" , \"text\" : \"What\u2019s in this image?\" }, { \"type\" : \"image_url\" , \"image_url\" : { \"url\" : f \"data:image/jpeg;base64, { base64_image } \" }}]}, ], \"max_tokens\" : 300 , } try : response = requests . post ( \"https://api.openai.com/v1/chat/completions\" , headers = headers , json = payload ) response . raise_for_status () # Will raise an exception for HTTP errors. return response . json () except Exception as e : logger . error ( f \"An error occurred while sending image to API: { e } \" ) raise if ** name ** == \"**main**\" : try : # Perform the main operations. screenshot_path = capture_screenshot () base64_image = encode_image ( screenshot_path ) api_response = send_image_to_api ( base64_image ) print ( api_response ) except Exception as e : logger . error ( f \"An error occurred in the main function: { e } \" )","title":"GPT-4 Turbo with vision"},{"location":"blog/posts/5790aml7p/#dalle-3","text":"Developers can now access DALL\u00b7E 3, a multimodal model that generates images from text directly through the API by specifying dall-e-3 as the model.","title":"DALL\u00b7E 3"},{"location":"blog/posts/5790aml7p/#tts-text-to-speech","text":"OpenAI's newest model is available to generate human-quality speech from text via their text-to-speech API.","title":"TTS (Text-to-Speech)"},{"location":"blog/posts/5790aml7p/#revenue-sharing-gpt-store-empowering-creators","text":"The DevDay also cast a spotlight on the newly announced revenue-sharing GPT Store. This platform represents a strategic move towards a more inclusive creator economy within AI, offering compensation to creators of AI applications based on user engagement and usage. This initiative is a nod to the growing importance of content creators in the AI ecosystem and reflects a broader trend of recognizing and rewarding the contributions of individual developers and innovators.","title":"Revenue-Sharing GPT Store: Empowering Creators"},{"location":"blog/posts/5790aml7p/#microsoft-partnership-and-azures-role","text":"The ongoing collaboration with Microsoft was highlighted, with a focus on how Azure's infrastructure is being optimized to support OpenAI's sophisticated AI models. This partnership is a testament to the shared goal of accelerating AI innovation and enhancing integration across various services and platforms as well as Microsoft's heavy investment in AI.","title":"Microsoft Partnership and Azure's Role"},{"location":"blog/posts/5790aml7p/#safe-and-gradual-ai-integration","text":"OpenAI emphasized a strategic approach to AI integration, advocating for a balance between innovation and safety. The organization invites developers to engage with the new tools thoughtfully, ensuring a responsible progression of AI within different sectors. This measured approach is a reflection of OpenAI's commitment to the safe and ethical development of AI technologies.","title":"Safe and Gradual AI Integration"},{"location":"blog/posts/5790aml7p/#conclusion","text":"The Developer Conference marked a notable milestone for OpenAI and the broader AI community. The launch of GPT4-Turbo and the introduction of new multimodal capabilities, combined with the support of Microsoft's Azure and the innovative revenue-sharing model of the GPT Store, heralds a new phase of growth and experimentation in AI applications.","title":"Conclusion"},{"location":"blog/posts/60yzqb6le/","text":"Progress Update: RunescapeGPT - A Runescape ChatGPT Bot Introduction RunescapeGPT is a project I started in order to create an AI-powered color bot for Runescape with enhanced capabilities. I have been working on this project for a few days now, and I am excited to share my progress with you all. In this post, I will be discussing what I have done so far and what I plan to do next. What I've Done So Far 2021-11-16 I have created a GUI for the bot using Qt Creator. It is a simple GUI that is inspired by Sammich's AHK bot. It has all the buttons provided by Sammich's bot. Here is a screenshot of Sammich's GUI: And here is the current state of RunescapeGPT's GUI: Although the GUI is not fully functional yet, it lays a solid foundation. The next steps in development include adding actionable functionality to the buttons. Initially, we'll start with a single script that has a hotkey to send a screenshot to the AI model. This will be a key feature for monitoring the bot's activity and ensuring its smooth operation. The script will capture the current state of the game, including what the bot is doing at any given time, and send this information along with a screenshot to the AI model. This multimodal approach will allow the AI to analyze both the textual data and the visual context of the game, enabling it to make informed decisions about the bot's next actions. Upcoming Features and Enhancements Real-time Monitoring : Integrate a system to always have a variable that reflects the bot's current action. Activity Log and Reporting : Keep a detailed log of the bot's last movement, including timestamps and the duration between actions, to identify and understand if something unusual occurs. AI-Powered Decision Making : In the event of anomalies or breaks, the information, including the screenshot, will be sent to an AI model equipped with multimodal capabilities. This model will analyze the situation and guide the bot accordingly. By implementing these features, RunescapeGPT will become more than just a bot; it will be a sophisticated AI companion that navigates the game's challenges with unprecedented efficiency. Stay tuned for more updates as the project evolves!","title":"Progress Log - RunescapeGPT"},{"location":"blog/posts/60yzqb6le/#progress-update-runescapegpt-a-runescape-chatgpt-bot","text":"","title":"Progress Update: RunescapeGPT - A Runescape ChatGPT Bot"},{"location":"blog/posts/60yzqb6le/#introduction","text":"RunescapeGPT is a project I started in order to create an AI-powered color bot for Runescape with enhanced capabilities. I have been working on this project for a few days now, and I am excited to share my progress with you all. In this post, I will be discussing what I have done so far and what I plan to do next.","title":"Introduction"},{"location":"blog/posts/60yzqb6le/#what-ive-done-so-far","text":"","title":"What I've Done So Far"},{"location":"blog/posts/60yzqb6le/#2021-11-16","text":"I have created a GUI for the bot using Qt Creator. It is a simple GUI that is inspired by Sammich's AHK bot. It has all the buttons provided by Sammich's bot. Here is a screenshot of Sammich's GUI: And here is the current state of RunescapeGPT's GUI: Although the GUI is not fully functional yet, it lays a solid foundation. The next steps in development include adding actionable functionality to the buttons. Initially, we'll start with a single script that has a hotkey to send a screenshot to the AI model. This will be a key feature for monitoring the bot's activity and ensuring its smooth operation. The script will capture the current state of the game, including what the bot is doing at any given time, and send this information along with a screenshot to the AI model. This multimodal approach will allow the AI to analyze both the textual data and the visual context of the game, enabling it to make informed decisions about the bot's next actions.","title":"2021-11-16"},{"location":"blog/posts/60yzqb6le/#upcoming-features-and-enhancements","text":"Real-time Monitoring : Integrate a system to always have a variable that reflects the bot's current action. Activity Log and Reporting : Keep a detailed log of the bot's last movement, including timestamps and the duration between actions, to identify and understand if something unusual occurs. AI-Powered Decision Making : In the event of anomalies or breaks, the information, including the screenshot, will be sent to an AI model equipped with multimodal capabilities. This model will analyze the situation and guide the bot accordingly. By implementing these features, RunescapeGPT will become more than just a bot; it will be a sophisticated AI companion that navigates the game's challenges with unprecedented efficiency. Stay tuned for more updates as the project evolves!","title":"Upcoming Features and Enhancements"},{"location":"blog/posts/ak49vuqhv/","text":"OSRS Money Making Guide 2024 - How to Earn a Free RuneScape Bond Help the OSRS community and earn a bond! One Small Wiki Favour is a project to improve the Old School RuneScape Wiki, where wiki users can contribute to a number of ongoing wiki tasks and qualify for receiving in-game rewards, including bonds . Anyone is welcome to participate, whether you're completely new to the wiki or you've been here for a while. If you have not edited before, then this is a great time to get involved! To participate in this project, join the #oswf-osrs channel in OSRS Wiki's Discord to coordinate with other editors, discuss the project, and claim your rewards. Rules You must have a wiki account and cannot be blocked. Anonymous edits will not count. You can sign up for a wiki account at Special:CreateAccount . The reward for completion of a task will be one bond or equivalent coins . If multiple editors contribute to a goal then the prize may be split between them. Partial rewards may be awarded for completing part of a task, with a minimum award at 25% of a bond. A task must be completed in its entirety before any bonds or partial bonds are awarded for that task. If you are awarded a bond or a share of a bond, you can get in contact with one of the project runners on Discord , and claim it in-game. You can claim a maximum of two bonds over each two week period that tasks are active. Automatic notifications To get automatic notifications of new tasks, you can join the Disord and subscribe to the #oswf-osrs channel. Alternatively, you can build an IFTTT applet using the RSS feed of the recent changes to the wiki. You can then automatically generate a Notion page with the new tasks and have them sent to you via text message or email.","title":"OSRS Money Making Guide 2024 - How to Earn a Free RuneScape Bond"},{"location":"blog/posts/ak49vuqhv/#osrs-money-making-guide-2024-how-to-earn-a-free-runescape-bond","text":"Help the OSRS community and earn a bond! One Small Wiki Favour is a project to improve the Old School RuneScape Wiki, where wiki users can contribute to a number of ongoing wiki tasks and qualify for receiving in-game rewards, including bonds . Anyone is welcome to participate, whether you're completely new to the wiki or you've been here for a while. If you have not edited before, then this is a great time to get involved! To participate in this project, join the #oswf-osrs channel in OSRS Wiki's Discord to coordinate with other editors, discuss the project, and claim your rewards.","title":"OSRS Money Making Guide 2024 - How to Earn a Free RuneScape Bond"},{"location":"blog/posts/ak49vuqhv/#rules","text":"You must have a wiki account and cannot be blocked. Anonymous edits will not count. You can sign up for a wiki account at Special:CreateAccount . The reward for completion of a task will be one bond or equivalent coins . If multiple editors contribute to a goal then the prize may be split between them. Partial rewards may be awarded for completing part of a task, with a minimum award at 25% of a bond. A task must be completed in its entirety before any bonds or partial bonds are awarded for that task. If you are awarded a bond or a share of a bond, you can get in contact with one of the project runners on Discord , and claim it in-game. You can claim a maximum of two bonds over each two week period that tasks are active.","title":"Rules"},{"location":"blog/posts/ak49vuqhv/#automatic-notifications","text":"To get automatic notifications of new tasks, you can join the Disord and subscribe to the #oswf-osrs channel. Alternatively, you can build an IFTTT applet using the RSS feed of the recent changes to the wiki. You can then automatically generate a Notion page with the new tasks and have them sent to you via text message or email.","title":"Automatic notifications"},{"location":"blog/posts/axurqomli/","text":"Automating DVR Surveillance Feed Analysis Using Selenium and Python Introduction In an era where security and monitoring are paramount, leveraging technology to enhance surveillance systems is crucial. Our mission is to automate the process of capturing surveillance feeds from a DVR system for analysis using advanced computer vision techniques. This task addresses the challenge of accessing live video feeds from DVRs that do not readily provide direct stream URLs, such as RTSP, which are essential for real-time video analysis. The Challenge Many DVR (Digital Video Recorder) systems, especially older models or those using proprietary software, do not offer an easy way to access their video feeds for external processing. They often stream video through embedded ActiveX controls in web interfaces, which pose a significant barrier to automation due to their closed nature and security restrictions. Our Approach To overcome these challenges, we propose a method that automates a web browser to periodically capture screenshots of the DVR's camera screens. These screenshots can then be analyzed using a computer vision model to transcribe or interpret the activities captured by the cameras. Our tools of choice are Selenium, a powerful tool for automating web browsers, and Python, a versatile programming language with extensive support for image processing and machine learning. Step-by-Step Guide Setting Up the Environment Selenium WebDriver: Install Selenium WebDriver compatible with your intended browser. Python Environment: Set up a Python environment with the necessary libraries (selenium, datetime, etc.). Browser Automation Navigate to DVR Interface: Use Selenium to open the browser and navigate to the DVR's web interface. Handle Authentication: Automate the login process to access the camera feeds. Capturing Screenshots Regular Intervals: Implement a loop in Python to capture and save screenshots of the camera feed every five seconds. Timestamped Filenames: Save the screenshots with timestamps to ensure uniqueness and facilitate chronological analysis. Analyzing the Captured Screenshots Vision Model Selection: Choose a suitable computer vision model for analyzing the screenshots based on the required analysis (e.g., object detection, and movement tracking). Processing Screenshots: Feed the screenshots to the vision model either in real-time or in batches for analysis. Continuous Monitoring Long-term Operation: Ensure the script can run continuously to monitor the surveillance feed over extended periods. Error Handling: Implement robust error handling to manage browser timeouts, disconnections, or other potential issues. Purpose and Benefits This automated approach is designed to enhance surveillance systems where direct access to video streams is not available. By analyzing the DVR feeds, it can be used for various applications such as: Security Monitoring: Detect unauthorized activities or security breaches. Data Analysis: Gather data over time for pattern recognition or anomaly detection. Event Documentation: Keep a record of events with timestamps for future reference. Conclusion While this approach offers a workaround to the limitations of certain DVR systems, it highlights the potential of integrating modern technology with existing surveillance infrastructure. The combination of Selenium's web automation capabilities and Python's powerful data processing and machine learning libraries opens up new avenues for enhancing security and surveillance systems. Important Note This method, while innovative, is a workaround and has limitations compared to direct video stream access. It is suited for scenarios where no other direct methods are available and real-time processing is not a critical requirement.","title":"Automating DVR Surveillance Feed Analysis Using Selenium and Python"},{"location":"blog/posts/axurqomli/#automating-dvr-surveillance-feed-analysis-using-selenium-and-python","text":"","title":"Automating DVR Surveillance Feed Analysis Using Selenium and Python"},{"location":"blog/posts/axurqomli/#introduction","text":"In an era where security and monitoring are paramount, leveraging technology to enhance surveillance systems is crucial. Our mission is to automate the process of capturing surveillance feeds from a DVR system for analysis using advanced computer vision techniques. This task addresses the challenge of accessing live video feeds from DVRs that do not readily provide direct stream URLs, such as RTSP, which are essential for real-time video analysis.","title":"Introduction"},{"location":"blog/posts/axurqomli/#the-challenge","text":"Many DVR (Digital Video Recorder) systems, especially older models or those using proprietary software, do not offer an easy way to access their video feeds for external processing. They often stream video through embedded ActiveX controls in web interfaces, which pose a significant barrier to automation due to their closed nature and security restrictions.","title":"The Challenge"},{"location":"blog/posts/axurqomli/#our-approach","text":"To overcome these challenges, we propose a method that automates a web browser to periodically capture screenshots of the DVR's camera screens. These screenshots can then be analyzed using a computer vision model to transcribe or interpret the activities captured by the cameras. Our tools of choice are Selenium, a powerful tool for automating web browsers, and Python, a versatile programming language with extensive support for image processing and machine learning.","title":"Our Approach"},{"location":"blog/posts/axurqomli/#step-by-step-guide","text":"Setting Up the Environment Selenium WebDriver: Install Selenium WebDriver compatible with your intended browser. Python Environment: Set up a Python environment with the necessary libraries (selenium, datetime, etc.). Browser Automation Navigate to DVR Interface: Use Selenium to open the browser and navigate to the DVR's web interface. Handle Authentication: Automate the login process to access the camera feeds. Capturing Screenshots Regular Intervals: Implement a loop in Python to capture and save screenshots of the camera feed every five seconds. Timestamped Filenames: Save the screenshots with timestamps to ensure uniqueness and facilitate chronological analysis. Analyzing the Captured Screenshots Vision Model Selection: Choose a suitable computer vision model for analyzing the screenshots based on the required analysis (e.g., object detection, and movement tracking). Processing Screenshots: Feed the screenshots to the vision model either in real-time or in batches for analysis. Continuous Monitoring Long-term Operation: Ensure the script can run continuously to monitor the surveillance feed over extended periods. Error Handling: Implement robust error handling to manage browser timeouts, disconnections, or other potential issues.","title":"Step-by-Step Guide"},{"location":"blog/posts/axurqomli/#purpose-and-benefits","text":"This automated approach is designed to enhance surveillance systems where direct access to video streams is not available. By analyzing the DVR feeds, it can be used for various applications such as: Security Monitoring: Detect unauthorized activities or security breaches. Data Analysis: Gather data over time for pattern recognition or anomaly detection. Event Documentation: Keep a record of events with timestamps for future reference.","title":"Purpose and Benefits"},{"location":"blog/posts/axurqomli/#conclusion","text":"While this approach offers a workaround to the limitations of certain DVR systems, it highlights the potential of integrating modern technology with existing surveillance infrastructure. The combination of Selenium's web automation capabilities and Python's powerful data processing and machine learning libraries opens up new avenues for enhancing security and surveillance systems.","title":"Conclusion"},{"location":"blog/posts/axurqomli/#important-note","text":"This method, while innovative, is a workaround and has limitations compared to direct video stream access. It is suited for scenarios where no other direct methods are available and real-time processing is not a critical requirement.","title":"Important Note"},{"location":"blog/posts/d8sb7a2bc/","text":"Hosting MkDocs Documentation on GitHub Pages This guide will walk you through the process of hosting your MkDocs documentation on GitHub Pages. By following these steps, you can make your documentation accessible online and easily share it with others. Prerequisites Before you begin, make sure you have the following prerequisites in place: A MkDocs project set up on your local machine. A GitHub account where you can create a new repository. Steps 1. Create a GitHub Repository Go to your GitHub account and log in. Click on the \"New\" button to create a new repository. Enter a name for your repository, choose whether it should be public or private, and configure other repository settings as needed. Then, click \"Create repository.\" 2. Push Your MkDocs Project to GitHub To host your MkDocs documentation on GitHub, you need to push your local project to your GitHub repository. Follow these steps: # Initialize a Git repository in your MkDocs project folder (if not already initialized) cd /path/to/your/mkdocs/project git init # Add all the files to the Git repository and commit them git add . git commit -m \"Initial commit\" # Link your local Git repository to your GitHub repository (replace placeholders) git remote add origin https://github.com/your-username/your-repo.git # Push your local repository to GitHub git push -u origin master Replace your-username with your GitHub username and your-repo with the name of your GitHub repository. 3. Enable GitHub Pages GitHub Pages allows you to host static websites directly from your repository. To enable GitHub Pages for your MkDocs documentation, follow these steps: Go to your GitHub repository and click on the \"Settings\" tab. Scroll down to the \"GitHub Pages\" section and click on the \"Source\" dropdown menu. Select \"master branch\" as the source and click \"Save.\" 4. Access Your Documentation Online Once you have enabled GitHub Pages, your MkDocs documentation will be accessible online. To access it, go to the following URL: https://your-username.github.io/your-repo/ Replace your-username with your GitHub username and your-repo with the name of your GitHub repository. Conclusion Hosting your documentation on GitHub Pages can have certain advantages in terms of accessibility and collaboration, but whether it's \"safer\" than keeping everything on your local device depends on your specific needs and security considerations. Here are some points to consider: Advantages of Hosting on GitHub Pages: Accessibility: When you host your documentation on GitHub Pages, it becomes accessible online, allowing a wider audience to access it without requiring access to your local device. Version Control: GitHub provides robust version control capabilities. You can track changes, collaborate with others, and easily revert to previous versions if needed. Backup: Your documentation is stored on GitHub's servers, providing a level of backup. Even if your local device experiences issues, your documentation remains safe on GitHub. Collaboration: Hosting on GitHub allows for collaborative editing and contributions from team members or the open-source community. Availability: GitHub Pages offers high availability and uptime, ensuring your documentation is accessible to users around the world. Security Considerations: Privacy: Make sure you understand the privacy settings of your GitHub repository. If your documentation contains sensitive information, you should keep it private and limit access. Authentication: Implement strong authentication methods for your GitHub account to prevent unauthorized access. Data Ownership: While GitHub is a reputable platform, consider that your data is hosted on third-party servers. Ensure you retain ownership of your documentation content. Backup Strategy: While GitHub provides backup, it's still a good practice to maintain your own backup of critical documentation on your local device or another secure location. Compliance: If you're subject to specific compliance regulations or security requirements, consult with your organization's IT/security team to ensure compliance when hosting documentation on third-party platforms. In summary, hosting your documentation on GitHub Pages can enhance accessibility, collaboration, and version control. It can be a safer option for sharing and collaborating on non-sensitive documentation. However, security and privacy considerations should be evaluated, and you should ensure that your data remains secure and compliant with any applicable regulations.","title":"Hosting MkDocs Documentation on GitHub Pages"},{"location":"blog/posts/d8sb7a2bc/#hosting-mkdocs-documentation-on-github-pages","text":"This guide will walk you through the process of hosting your MkDocs documentation on GitHub Pages. By following these steps, you can make your documentation accessible online and easily share it with others.","title":"Hosting MkDocs Documentation on GitHub Pages"},{"location":"blog/posts/d8sb7a2bc/#prerequisites","text":"Before you begin, make sure you have the following prerequisites in place: A MkDocs project set up on your local machine. A GitHub account where you can create a new repository.","title":"Prerequisites"},{"location":"blog/posts/d8sb7a2bc/#steps","text":"","title":"Steps"},{"location":"blog/posts/d8sb7a2bc/#1-create-a-github-repository","text":"Go to your GitHub account and log in. Click on the \"New\" button to create a new repository. Enter a name for your repository, choose whether it should be public or private, and configure other repository settings as needed. Then, click \"Create repository.\"","title":"1. Create a GitHub Repository"},{"location":"blog/posts/d8sb7a2bc/#2-push-your-mkdocs-project-to-github","text":"To host your MkDocs documentation on GitHub, you need to push your local project to your GitHub repository. Follow these steps: # Initialize a Git repository in your MkDocs project folder (if not already initialized) cd /path/to/your/mkdocs/project git init # Add all the files to the Git repository and commit them git add . git commit -m \"Initial commit\" # Link your local Git repository to your GitHub repository (replace placeholders) git remote add origin https://github.com/your-username/your-repo.git # Push your local repository to GitHub git push -u origin master Replace your-username with your GitHub username and your-repo with the name of your GitHub repository.","title":"2. Push Your MkDocs Project to GitHub"},{"location":"blog/posts/d8sb7a2bc/#3-enable-github-pages","text":"GitHub Pages allows you to host static websites directly from your repository. To enable GitHub Pages for your MkDocs documentation, follow these steps: Go to your GitHub repository and click on the \"Settings\" tab. Scroll down to the \"GitHub Pages\" section and click on the \"Source\" dropdown menu. Select \"master branch\" as the source and click \"Save.\"","title":"3. Enable GitHub Pages"},{"location":"blog/posts/d8sb7a2bc/#4-access-your-documentation-online","text":"Once you have enabled GitHub Pages, your MkDocs documentation will be accessible online. To access it, go to the following URL: https://your-username.github.io/your-repo/ Replace your-username with your GitHub username and your-repo with the name of your GitHub repository.","title":"4. Access Your Documentation Online"},{"location":"blog/posts/d8sb7a2bc/#conclusion","text":"Hosting your documentation on GitHub Pages can have certain advantages in terms of accessibility and collaboration, but whether it's \"safer\" than keeping everything on your local device depends on your specific needs and security considerations. Here are some points to consider: Advantages of Hosting on GitHub Pages: Accessibility: When you host your documentation on GitHub Pages, it becomes accessible online, allowing a wider audience to access it without requiring access to your local device. Version Control: GitHub provides robust version control capabilities. You can track changes, collaborate with others, and easily revert to previous versions if needed. Backup: Your documentation is stored on GitHub's servers, providing a level of backup. Even if your local device experiences issues, your documentation remains safe on GitHub. Collaboration: Hosting on GitHub allows for collaborative editing and contributions from team members or the open-source community. Availability: GitHub Pages offers high availability and uptime, ensuring your documentation is accessible to users around the world. Security Considerations: Privacy: Make sure you understand the privacy settings of your GitHub repository. If your documentation contains sensitive information, you should keep it private and limit access. Authentication: Implement strong authentication methods for your GitHub account to prevent unauthorized access. Data Ownership: While GitHub is a reputable platform, consider that your data is hosted on third-party servers. Ensure you retain ownership of your documentation content. Backup Strategy: While GitHub provides backup, it's still a good practice to maintain your own backup of critical documentation on your local device or another secure location. Compliance: If you're subject to specific compliance regulations or security requirements, consult with your organization's IT/security team to ensure compliance when hosting documentation on third-party platforms. In summary, hosting your documentation on GitHub Pages can enhance accessibility, collaboration, and version control. It can be a safer option for sharing and collaborating on non-sensitive documentation. However, security and privacy considerations should be evaluated, and you should ensure that your data remains secure and compliant with any applicable regulations.","title":"Conclusion"},{"location":"blog/posts/gpcaqmwby/","text":"Voice Profiles ElevenLabs API offers a diverse range of voices, perfect for various use cases like narration and video game character voices. The following voice profiles are available as of November 2023: Rachel: The Articulate Narrator Rachel's voice carries a balance of clarity and tranquility, ideal for narration and audiobook projects. Voice ID: 21m00Tcm4TlvDq8ikWAM Accent: American Description: Calm Age: Young Gender: Female Use Case: Narration Preview Voice Clyde: The Veteran Storyteller Clyde offers a voice rich with experience, perfect for gritty narratives or character roles that demand a seasoned tone. Voice ID: 2EiwWnXFnvU5JabPnv8n Accent: American Description: War veteran Age: Middle-aged Gender: Male Use Case: Video games Preview Voice Domi: The Confident Influencer Domi's commanding voice is filled with confidence, suited for strong narrative leads or powerful corporate presentations. Voice ID: AZnzlk1XvdvUeBnXmlld Accent: American Description: Strong Age: Young Gender: Female Use Case: Narration Preview Voice Dave: The Engaging Entertainer Dave's British-Essex accent adds a unique and engaging flavor, ideal for interactive content and characters with a touch of humor. Voice ID: CYw3kZ02Hs0563khs1Fj Accent: British-Essex Description: Conversational Age: Young Gender: Male Use Case: Video games Preview Voice Fin: The Rugged Sea Captain Fin's voice, with its Irish accent and seasoned timbre, is perfectly suited for characters with depth and a storied past. Voice ID: D38z5RcWu1voky8WS1ja Accent: Irish Description: Sailor Age: Old Gender: Male Use Case: Video games Preview Voice","title":"Available Voices from ElevenLabs API in November 2023"},{"location":"blog/posts/gpcaqmwby/#voice-profiles","text":"ElevenLabs API offers a diverse range of voices, perfect for various use cases like narration and video game character voices. The following voice profiles are available as of November 2023:","title":"Voice Profiles"},{"location":"blog/posts/gpcaqmwby/#rachel-the-articulate-narrator","text":"Rachel's voice carries a balance of clarity and tranquility, ideal for narration and audiobook projects. Voice ID: 21m00Tcm4TlvDq8ikWAM Accent: American Description: Calm Age: Young Gender: Female Use Case: Narration Preview Voice","title":"Rachel: The Articulate Narrator"},{"location":"blog/posts/gpcaqmwby/#clyde-the-veteran-storyteller","text":"Clyde offers a voice rich with experience, perfect for gritty narratives or character roles that demand a seasoned tone. Voice ID: 2EiwWnXFnvU5JabPnv8n Accent: American Description: War veteran Age: Middle-aged Gender: Male Use Case: Video games Preview Voice","title":"Clyde: The Veteran Storyteller"},{"location":"blog/posts/gpcaqmwby/#domi-the-confident-influencer","text":"Domi's commanding voice is filled with confidence, suited for strong narrative leads or powerful corporate presentations. Voice ID: AZnzlk1XvdvUeBnXmlld Accent: American Description: Strong Age: Young Gender: Female Use Case: Narration Preview Voice","title":"Domi: The Confident Influencer"},{"location":"blog/posts/gpcaqmwby/#dave-the-engaging-entertainer","text":"Dave's British-Essex accent adds a unique and engaging flavor, ideal for interactive content and characters with a touch of humor. Voice ID: CYw3kZ02Hs0563khs1Fj Accent: British-Essex Description: Conversational Age: Young Gender: Male Use Case: Video games Preview Voice","title":"Dave: The Engaging Entertainer"},{"location":"blog/posts/gpcaqmwby/#fin-the-rugged-sea-captain","text":"Fin's voice, with its Irish accent and seasoned timbre, is perfectly suited for characters with depth and a storied past. Voice ID: D38z5RcWu1voky8WS1ja Accent: Irish Description: Sailor Age: Old Gender: Male Use Case: Video games Preview Voice","title":"Fin: The Rugged Sea Captain"},{"location":"blog/posts/mvzxnglbu/","text":"Productivity Tools in 2024 Notetaking and Task Management In my attempt to cut down on subscriptions in 2024, I'll be switching to Microsoft Visual Studio Code with GitHub Copilot as my go-to AI assistant in helping me churn out more content for my blog and YouTube channel. I'll be switching to a productivity toolset consisting of Evernote with Kanbanote, Anki, Raindrop.io, and Google Calendar. I want to be more note-focused than ever with data-hungry Large Language Models (LLMs) becoming more of a norm. I've gone through my personal Apple subscriptions and canceled all of them, these are separate from my shared family subscriptions such as Chaupal, a Punjabi, Bhojpuri, and Haryanvi video streaming service. I've also canceled my MidJourney and ChatGPT subscriptions. I intend on using fewer applications so I can utilize the most of what I have and if I do start using a new subscription service I'll be sure to buy residential Turkish proxies to get the best price whilst keeping my running total of subscriptions to a minimum. Accordingly, some other subscription services I need to check Turkish pricing for are: ElevanLabs Grammarly Dropbox To sum up my 2024 productivity stack: Microsoft Visual Studio Code GitHub Copilot Evernote Kanbanote Raindrop.io Google Calendar Useful links: IP Burger for Turkish residential proxies Prepaid Credit Card for Turkish subscriptions Microsoft Visual Studio Code Microsoft Visual Studio Code is a free source-code editor made by Microsoft for Windows, Linux, and macOS. Password Manager RoboForm RoboForm is a password manager and form filler tool that automates password entering and form filling, developed by Siber Systems, Inc. It is available for many web browsers, as a downloadable application, and as a mobile application. RoboForm stores web passwords on its servers, and offers to synchronize passwords between multiple computers and mobile devices. RoboForm offers a Family Plan for up to 5 users which I share with my family. Theme Dracula Theme is a dark theme for programs Alacritty, Alfred, Atom, BetterDiscord, Emacs, Firefox, Gnome Terminal, Google Chrome, Hyper, Insomnia, iTerm, JetBrains IDEs, Notepad++, Slack, Sublime Text, Terminal.app, Vim, Visual Studio, Visual Studio Code, Windows Terminal, and Xcode. With it's easy-on-the-eyes color scheme, Dracula Theme is on my list of must-have themes for any application I use.","title":"Productivity Tools in 2024"},{"location":"blog/posts/mvzxnglbu/#productivity-tools-in-2024","text":"","title":"Productivity Tools in 2024"},{"location":"blog/posts/mvzxnglbu/#notetaking-and-task-management","text":"In my attempt to cut down on subscriptions in 2024, I'll be switching to Microsoft Visual Studio Code with GitHub Copilot as my go-to AI assistant in helping me churn out more content for my blog and YouTube channel. I'll be switching to a productivity toolset consisting of Evernote with Kanbanote, Anki, Raindrop.io, and Google Calendar. I want to be more note-focused than ever with data-hungry Large Language Models (LLMs) becoming more of a norm. I've gone through my personal Apple subscriptions and canceled all of them, these are separate from my shared family subscriptions such as Chaupal, a Punjabi, Bhojpuri, and Haryanvi video streaming service. I've also canceled my MidJourney and ChatGPT subscriptions. I intend on using fewer applications so I can utilize the most of what I have and if I do start using a new subscription service I'll be sure to buy residential Turkish proxies to get the best price whilst keeping my running total of subscriptions to a minimum. Accordingly, some other subscription services I need to check Turkish pricing for are: ElevanLabs Grammarly Dropbox To sum up my 2024 productivity stack: Microsoft Visual Studio Code GitHub Copilot Evernote Kanbanote Raindrop.io Google Calendar Useful links: IP Burger for Turkish residential proxies Prepaid Credit Card for Turkish subscriptions","title":"Notetaking and Task Management"},{"location":"blog/posts/mvzxnglbu/#microsoft-visual-studio-code","text":"Microsoft Visual Studio Code is a free source-code editor made by Microsoft for Windows, Linux, and macOS.","title":"Microsoft Visual Studio Code"},{"location":"blog/posts/mvzxnglbu/#password-manager","text":"","title":"Password Manager"},{"location":"blog/posts/mvzxnglbu/#roboform","text":"RoboForm is a password manager and form filler tool that automates password entering and form filling, developed by Siber Systems, Inc. It is available for many web browsers, as a downloadable application, and as a mobile application. RoboForm stores web passwords on its servers, and offers to synchronize passwords between multiple computers and mobile devices. RoboForm offers a Family Plan for up to 5 users which I share with my family.","title":"RoboForm"},{"location":"blog/posts/mvzxnglbu/#theme","text":"Dracula Theme is a dark theme for programs Alacritty, Alfred, Atom, BetterDiscord, Emacs, Firefox, Gnome Terminal, Google Chrome, Hyper, Insomnia, iTerm, JetBrains IDEs, Notepad++, Slack, Sublime Text, Terminal.app, Vim, Visual Studio, Visual Studio Code, Windows Terminal, and Xcode. With it's easy-on-the-eyes color scheme, Dracula Theme is on my list of must-have themes for any application I use.","title":"Theme"},{"location":"blog/posts/qbli3jaie/","text":"NVIDIA's Nemotron-3-8B-Chat-SteerLM: Empowering Conversational AI with Stateful Text Generation Introduction In the world of AI, language models have taken center stage for their ability to generate human-like text responses to a wide range of queries. NVIDIA's Nemotron-3-8B-Chat-SteerLM is one such model, offering a powerful tool for generative AI creators working on conversational AI models. Let's dive into the details of this model and understand how it works, its intended use, potential risks, and its unique feature of remembering previous answers. Model Overview Nemotron-3-8B-Chat-SteerLM is an 8 billion-parameter generative language model based on the Nemotron-3-8B base model. It boasts customizability through the SteerLM method, allowing users to control model outputs dynamically during inference. This model is designed to generate text responses and code, making it a versatile choice for a range of applications. Intended Application & Domain This model is tailored for text-to-text generation, where it takes text input and generates text output. Its primary purpose is to assist generative AI creators in the development of conversational AI models. Whether it's chatbots, virtual assistants, or customer support systems, this model excels in generating text-based responses to user queries. Model Type Nemotron-3-8B-Chat-SteerLM belongs to the Transformer architecture family, renowned for its effectiveness in natural language processing tasks. Its architecture enables it to understand and generate human-like text. Intended User Developers and data scientists are the primary users of this model. They can leverage it to create conversational AI models that generate coherent and contextually relevant text responses in a conversational context. Stateful Text Generation One of the standout features of this model is its statefulness. It has the ability to remember previous answers in a conversation. This capability allows it to maintain context and generate responses that are not just coherent but also contextually relevant. For example, in a multi-turn conversation, it can refer back to previous responses to ensure continuity and relevancy. How the Model Works Nemotron-3-8B-Chat-SteerLM is a large language model that operates by generating text and code in response to prompts. Users input a text prompt, and the model utilizes its pre-trained knowledge to craft a text-based response. The stateful nature of the model means that it can remember and consider the conversation history, enabling it to generate contextually appropriate responses. This feature enhances the conversational quality of the AI, making interactions feel more natural and meaningful. Performance Metrics The model's performance is evaluated based on two critical metrics: Throughput: This metric measures how many requests the model can handle within a given time frame. It is essential for assessing the model's efficiency in real-world production environments. Latency: Latency gauges the time taken by the model to respond to a single request. Lower latency is desirable, indicating quicker responses and smoother user experiences. Potential Known Risks It's crucial to be aware of potential risks when using Nemotron-3-8B-Chat-SteerLM: Bias and Toxicity: The model was trained on data from the internet, which may contain toxic language and societal biases. Consequently, it may generate responses that amplify these biases and return toxic or offensive content, especially when prompted with toxic inputs. Accuracy and Relevance: The model may generate answers that are inaccurate, omit key information, or include irrelevant or redundant text. This can lead to socially unacceptable or undesirable text, even if the input prompt itself is not offensive. Licensing The use of this model is governed by the \"NVIDIA AI Foundation Models Community License Agreement.\" Users must adhere to the terms and conditions outlined in the agreement when utilizing the model. Conclusion NVIDIA's Nemotron-3-8B-Chat-SteerLM represents a significant advancement in generative AI for conversational applications. With its stateful text generation capability and Transformer architecture, it offers a versatile solution for developers and data scientists working in this domain. However, it's important to be mindful of potential biases and accuracy issues, as well as adhere to the licensing terms when utilizing this powerful AI tool.","title":"NVIDIA's Nemotron-3-8B-Chat-SteerLM - Empowering Conversational AI with Stateful Text Generation"},{"location":"blog/posts/qbli3jaie/#nvidias-nemotron-3-8b-chat-steerlm-empowering-conversational-ai-with-stateful-text-generation","text":"","title":"NVIDIA's Nemotron-3-8B-Chat-SteerLM: Empowering Conversational AI with Stateful Text Generation"},{"location":"blog/posts/qbli3jaie/#introduction","text":"In the world of AI, language models have taken center stage for their ability to generate human-like text responses to a wide range of queries. NVIDIA's Nemotron-3-8B-Chat-SteerLM is one such model, offering a powerful tool for generative AI creators working on conversational AI models. Let's dive into the details of this model and understand how it works, its intended use, potential risks, and its unique feature of remembering previous answers.","title":"Introduction"},{"location":"blog/posts/qbli3jaie/#model-overview","text":"Nemotron-3-8B-Chat-SteerLM is an 8 billion-parameter generative language model based on the Nemotron-3-8B base model. It boasts customizability through the SteerLM method, allowing users to control model outputs dynamically during inference. This model is designed to generate text responses and code, making it a versatile choice for a range of applications.","title":"Model Overview"},{"location":"blog/posts/qbli3jaie/#intended-application-domain","text":"This model is tailored for text-to-text generation, where it takes text input and generates text output. Its primary purpose is to assist generative AI creators in the development of conversational AI models. Whether it's chatbots, virtual assistants, or customer support systems, this model excels in generating text-based responses to user queries.","title":"Intended Application &amp; Domain"},{"location":"blog/posts/qbli3jaie/#model-type","text":"Nemotron-3-8B-Chat-SteerLM belongs to the Transformer architecture family, renowned for its effectiveness in natural language processing tasks. Its architecture enables it to understand and generate human-like text.","title":"Model Type"},{"location":"blog/posts/qbli3jaie/#intended-user","text":"Developers and data scientists are the primary users of this model. They can leverage it to create conversational AI models that generate coherent and contextually relevant text responses in a conversational context.","title":"Intended User"},{"location":"blog/posts/qbli3jaie/#stateful-text-generation","text":"One of the standout features of this model is its statefulness. It has the ability to remember previous answers in a conversation. This capability allows it to maintain context and generate responses that are not just coherent but also contextually relevant. For example, in a multi-turn conversation, it can refer back to previous responses to ensure continuity and relevancy.","title":"Stateful Text Generation"},{"location":"blog/posts/qbli3jaie/#how-the-model-works","text":"Nemotron-3-8B-Chat-SteerLM is a large language model that operates by generating text and code in response to prompts. Users input a text prompt, and the model utilizes its pre-trained knowledge to craft a text-based response. The stateful nature of the model means that it can remember and consider the conversation history, enabling it to generate contextually appropriate responses. This feature enhances the conversational quality of the AI, making interactions feel more natural and meaningful.","title":"How the Model Works"},{"location":"blog/posts/qbli3jaie/#performance-metrics","text":"The model's performance is evaluated based on two critical metrics: Throughput: This metric measures how many requests the model can handle within a given time frame. It is essential for assessing the model's efficiency in real-world production environments. Latency: Latency gauges the time taken by the model to respond to a single request. Lower latency is desirable, indicating quicker responses and smoother user experiences.","title":"Performance Metrics"},{"location":"blog/posts/qbli3jaie/#potential-known-risks","text":"It's crucial to be aware of potential risks when using Nemotron-3-8B-Chat-SteerLM: Bias and Toxicity: The model was trained on data from the internet, which may contain toxic language and societal biases. Consequently, it may generate responses that amplify these biases and return toxic or offensive content, especially when prompted with toxic inputs. Accuracy and Relevance: The model may generate answers that are inaccurate, omit key information, or include irrelevant or redundant text. This can lead to socially unacceptable or undesirable text, even if the input prompt itself is not offensive.","title":"Potential Known Risks"},{"location":"blog/posts/qbli3jaie/#licensing","text":"The use of this model is governed by the \"NVIDIA AI Foundation Models Community License Agreement.\" Users must adhere to the terms and conditions outlined in the agreement when utilizing the model.","title":"Licensing"},{"location":"blog/posts/qbli3jaie/#conclusion","text":"NVIDIA's Nemotron-3-8B-Chat-SteerLM represents a significant advancement in generative AI for conversational applications. With its stateful text generation capability and Transformer architecture, it offers a versatile solution for developers and data scientists working in this domain. However, it's important to be mindful of potential biases and accuracy issues, as well as adhere to the licensing terms when utilizing this powerful AI tool.","title":"Conclusion"},{"location":"blog/posts/qss6cnny5/","text":"Transferring Files Between WSL and Windows Logging into Zomro VPS using WSL in Ubuntu CLI To log into your Zomro VPS using WSL (Windows Subsystem for Linux) in the Ubuntu CLI, you can use the ssh command. Here are the steps to do it: Open your Ubuntu terminal in WSL. You can do this by searching for \"Ubuntu\" in the Windows Start menu and launching it. In the Ubuntu terminal, use the ssh command to connect to your Zomro VPS. Replace your_username with your actual username and your_server_ip with the IP address of your Zomro VPS: ssh your_username @ your_server_ip For example, if your username is \"root\" and your server's IP address is \"123.456.789.0,\" the command would be: ssh root @ 123.456.789.0 Press Enter after entering the command. You will be prompted to enter your password for the VPS. After entering the correct password, you should be logged into your Zomro VPS via SSH. You will see a command prompt for your VPS, and you can start running commands on the remote server. That's it! You have successfully logged into your Zomro VPS using WSL's Ubuntu CLI. You can now manage your server and perform various tasks as needed. Locating File Paths in Ubuntu CLI In Ubuntu CLI (Command Line Interface), it's essential to know how to find the file paths of directories and files. This knowledge allows you to navigate your file system effectively and reference files for various tasks. Here are some useful commands and techniques for locating file paths: Present Working Directory (pwd) The pwd command stands for \"Present Working Directory\" and displays the absolute path of your current location within the file system. Simply enter the following command: pwd The terminal will respond with the absolute path to your current directory, helping you understand where you are in the file system. Listing Directory Contents (ls) The ls command is used to list the contents of a directory. When executed without any arguments, it displays the files and subdirectories in your current directory. For example: ls This command will list the files and directories in your current location. Finding a File (find) If you need to locate a specific file within your file system, you can use the find command. Specify the starting directory and the filename you're looking for. For example, to find a file named \"example.txt\" starting from the root directory, use: find / - name example . txt This command will search the entire file system for \"example.txt\" and display its path if found. Navigating Directories (cd) The cd command allows you to change directories and move through the file system. You can use it to navigate to specific locations. For instance, to move to a directory named \"documents,\" use: cd documents You can also use relative paths, such as cd .. to go up one level or cd /path/to/directory to specify an absolute path. File Explorer Integration In many cases, you can easily locate file paths by using a graphical file explorer like Windows File Explorer. WSL allows you to access your Windows files and directories under the /mnt directory. For example, your Windows C: drive is typically accessible at /mnt/c/ . Understanding how to locate file paths in Ubuntu CLI is crucial for efficient file management and navigation. These commands and techniques will empower you to work effectively with your files and directories. Transferring Files from WSL to Windows Transferring files from your WSL (Windows Subsystem for Linux) environment to your Windows system is a common task and can be done using several methods. I\u2019ll be discussing the Secure Copy method in this tutorial. Using SCP (Secure Copy) You can transfer files from WSL using the scp (Secure Copy) command. Here's the syntax: scp username @ WindowsIP : /path/to/source/file /path/to/destination/in/WSL/ username : Your Windows username. WindowsIP : The IP address or hostname of your Windows system. /path/to/source/file : The path to the file in your Windows file system that you want to copy. /path/to/destination/in/WSL/ : The destination path in your WSL environment. For example, to copy all files located in /root/zomro-selenium-base/screenshots/* to your Windows Desktop, you can use: scp root @ 45.88.107.136 : /root/zomro-selenium-base/screenshots/* \"/mnt/c/Users/Harminder Nijjar/Desktop/\" This command will copy all files in /root/zomro-selenium-base/screenshots/ to your Windows Desktop. Make sure to adjust the source and destination paths as needed for your specific use case. Conclusion Transferring files between WSL and Windows is a common operation and can be accomplished using the Secure Copy (SCP) command. Whether you need to copy files from WSL to Windows or from Windows to WSL, SCP provides a secure and efficient","title":"Transferring Files Between WSL and Windows"},{"location":"blog/posts/qss6cnny5/#transferring-files-between-wsl-and-windows","text":"","title":"Transferring Files Between WSL and Windows"},{"location":"blog/posts/qss6cnny5/#logging-into-zomro-vps-using-wsl-in-ubuntu-cli","text":"To log into your Zomro VPS using WSL (Windows Subsystem for Linux) in the Ubuntu CLI, you can use the ssh command. Here are the steps to do it: Open your Ubuntu terminal in WSL. You can do this by searching for \"Ubuntu\" in the Windows Start menu and launching it. In the Ubuntu terminal, use the ssh command to connect to your Zomro VPS. Replace your_username with your actual username and your_server_ip with the IP address of your Zomro VPS: ssh your_username @ your_server_ip For example, if your username is \"root\" and your server's IP address is \"123.456.789.0,\" the command would be: ssh root @ 123.456.789.0 Press Enter after entering the command. You will be prompted to enter your password for the VPS. After entering the correct password, you should be logged into your Zomro VPS via SSH. You will see a command prompt for your VPS, and you can start running commands on the remote server. That's it! You have successfully logged into your Zomro VPS using WSL's Ubuntu CLI. You can now manage your server and perform various tasks as needed.","title":"Logging into Zomro VPS using WSL in Ubuntu CLI"},{"location":"blog/posts/qss6cnny5/#locating-file-paths-in-ubuntu-cli","text":"In Ubuntu CLI (Command Line Interface), it's essential to know how to find the file paths of directories and files. This knowledge allows you to navigate your file system effectively and reference files for various tasks. Here are some useful commands and techniques for locating file paths:","title":"Locating File Paths in Ubuntu CLI"},{"location":"blog/posts/qss6cnny5/#present-working-directory-pwd","text":"The pwd command stands for \"Present Working Directory\" and displays the absolute path of your current location within the file system. Simply enter the following command: pwd The terminal will respond with the absolute path to your current directory, helping you understand where you are in the file system.","title":"Present Working Directory (pwd)"},{"location":"blog/posts/qss6cnny5/#listing-directory-contents-ls","text":"The ls command is used to list the contents of a directory. When executed without any arguments, it displays the files and subdirectories in your current directory. For example: ls This command will list the files and directories in your current location.","title":"Listing Directory Contents (ls)"},{"location":"blog/posts/qss6cnny5/#finding-a-file-find","text":"If you need to locate a specific file within your file system, you can use the find command. Specify the starting directory and the filename you're looking for. For example, to find a file named \"example.txt\" starting from the root directory, use: find / - name example . txt This command will search the entire file system for \"example.txt\" and display its path if found.","title":"Finding a File (find)"},{"location":"blog/posts/qss6cnny5/#navigating-directories-cd","text":"The cd command allows you to change directories and move through the file system. You can use it to navigate to specific locations. For instance, to move to a directory named \"documents,\" use: cd documents You can also use relative paths, such as cd .. to go up one level or cd /path/to/directory to specify an absolute path.","title":"Navigating Directories (cd)"},{"location":"blog/posts/qss6cnny5/#file-explorer-integration","text":"In many cases, you can easily locate file paths by using a graphical file explorer like Windows File Explorer. WSL allows you to access your Windows files and directories under the /mnt directory. For example, your Windows C: drive is typically accessible at /mnt/c/ . Understanding how to locate file paths in Ubuntu CLI is crucial for efficient file management and navigation. These commands and techniques will empower you to work effectively with your files and directories.","title":"File Explorer Integration"},{"location":"blog/posts/qss6cnny5/#transferring-files-from-wsl-to-windows","text":"Transferring files from your WSL (Windows Subsystem for Linux) environment to your Windows system is a common task and can be done using several methods. I\u2019ll be discussing the Secure Copy method in this tutorial.","title":"Transferring Files from WSL to Windows"},{"location":"blog/posts/qss6cnny5/#using-scp-secure-copy","text":"You can transfer files from WSL using the scp (Secure Copy) command. Here's the syntax: scp username @ WindowsIP : /path/to/source/file /path/to/destination/in/WSL/ username : Your Windows username. WindowsIP : The IP address or hostname of your Windows system. /path/to/source/file : The path to the file in your Windows file system that you want to copy. /path/to/destination/in/WSL/ : The destination path in your WSL environment. For example, to copy all files located in /root/zomro-selenium-base/screenshots/* to your Windows Desktop, you can use: scp root @ 45.88.107.136 : /root/zomro-selenium-base/screenshots/* \"/mnt/c/Users/Harminder Nijjar/Desktop/\" This command will copy all files in /root/zomro-selenium-base/screenshots/ to your Windows Desktop. Make sure to adjust the source and destination paths as needed for your specific use case.","title":"Using SCP (Secure Copy)"},{"location":"blog/posts/qss6cnny5/#conclusion","text":"Transferring files between WSL and Windows is a common operation and can be accomplished using the Secure Copy (SCP) command. Whether you need to copy files from WSL to Windows or from Windows to WSL, SCP provides a secure and efficient","title":"Conclusion"},{"location":"css/css/","text":"css","title":"css"},{"location":"css/css/#css","text":"","title":"css"},{"location":"projects/","text":"AI-Powered Property Surveillance Enhancing property security with AI-powered surveillance systems. Dubbing Pipeline Optimizing voiceover workflows for content creators and viewers. Midjourney Automation Bot Automating Midjourney image generation and social media posting. RunescapeGPT A Runescape ChatGPT Bot. Social Media Automation Automating social media posting and engagement. Web scraping Automating web scraping for data collection. Websites Building and maintaining websites with modern technologies.","title":"Current projects"},{"location":"projects/ai_powered_property_surveillance/training/","text":"Training YOLOv8 on Labeled Data from Label Studio YOLOv8 Training Data for Varied Conditions Images Collect a diverse dataset comprising several thousand annotated images for each of the DVR camera views. Include images that represent different times of day, seasons, weather conditions, and lighting scenarios. Ensure a range of backgrounds and object poses to enhance model generalization. Labels Annotate each image with bounding boxes to accurately mark objects for detection. Create labels for each object, considering variations in appearance under different conditions. Note: Adequate representation of diverse conditions is crucial for robust object detection. Refer to YOLOv8 documentation for specific recommendations and guidelines. 1. Prepare Your Dataset Export Your Dataset Export your dataset from Label Studio in the YOLO format. This will generate a .zip file containing the images and annotations in the required format. Organize Your Dataset Ensure your dataset is properly organized. Each image in the images folder should have a corresponding annotation file in the labels folder. Dataset Structure project-1-at-2023-12-10-11-21-cc95541c images : Contains all the .bmp image files. labels : Contains corresponding .txt files for each image in YOLO format. Prepare Label Files Each label file should have the following format: One line per object. Each line: <class_index> <x_center> <y_center> <width> <height> , normalized to [0,1]. Create a train, val, and test split Create a train, val, and test split of your dataset. The train set should contain 70% of the images, the val set should contain 15% of the images, and the test set should contain 15% of the images. This is the recommended split, but you can adjust it according to your needs. Before running a Python script to split your dataset, ensure you have the necessary directory structure: project-1-at-2023-12-10-11-21-cc95541c \u251c\u2500 images \u2514\u2500 labels import os import shutil import random # Set the seed for reproducibility random . seed ( 42 ) # Paths base_path = 'project-1-at-2023-12-10-11-21-cc95541c' images_path = os . path . join ( base_path , 'images' ) labels_path = os . path . join ( base_path , 'labels' ) # Split Ratios train_ratio = 0.70 val_ratio = 0.15 test_ratio = 0.15 # Create directories for train, val, and test sets for set_type in [ 'train' , 'val' , 'test' ]: for content_type in [ 'images' , 'labels' ]: os . makedirs ( os . path . join ( base_path , set_type , content_type ), exist_ok = True ) # Get all image filenames all_files = [ f for f in os . listdir ( images_path ) if os . path . isfile ( os . path . join ( images_path , f ))] random . shuffle ( all_files ) # Calculate split indices total_files = len ( all_files ) train_end = int ( train_ratio _ total_files ) val_end = train_end + int ( val_ratio _ total_files ) # Split files train_files = all_files [: train_end ] val_files = all_files [ train_end : val_end ] test_files = all_files [ val_end :] # Function to copy files def copy_files ( files , set_type ): for file in files : # Copy image shutil . copy ( os . path . join ( images_path , file ), os . path . join ( base_path , set_type , 'images' )) # Copy corresponding label label_file = file . rsplit ( '.' , 1 )[ 0 ] + '.txt' shutil . copy ( os . path . join ( labels_path , label_file ), os . path . join ( base_path , set_type , 'labels' )) # Copy files to respective directories copy_files ( train_files , 'train' ) copy_files ( val_files , 'val' ) copy_files ( test_files , 'test' ) print ( \"Dataset successfully split into train, val, and test sets.\" ) Make sure to adjust the base_path to the correct path where your project-1-at-2023-12-10-11-21-cc95541c folder is located. How to Run the Script Save the script as a .py file, e.g., split_dataset.py . Run the script using Python from your command line or terminal. The script will create train, val, and test folders with images and labels subdirectories, and distribute your dataset accordingly. 2. Training YOLOv8 on Custom Dataset Create a Data Configuration File Create a YAML file (e.g., data.yaml ) with the following content: train : ./project-1-at-2023-12-10-11-21-cc95541c/images # Training images path val : ./path-to-validation-set # Validation images path nc : <number_of_classes> # Number of classes names : [ 'class1' , 'class2' , ... ] # Class names Replace with the actual number of classes and ['class1', 'class2', ...] with your class names. Example: train : ./project-1-at-2023-12-10-11-21-cc95541c/train/images # Path to training images val : ./project-1-at-2023-12-10-11-21-cc95541c/val/images # Path to validation images test : ./project-1-at-2023-12-10-11-21-cc95541c/test/images # Path to test images (optional) nc : 7 # Number of classes names : - Car - Closed Glass Door - Dog - Metal Roofing - Open Glass Door - Person - Semi Truck Make sure to adjust the paths to your train and val sets accordingly. Configure Training Parameters Create a YAML file (e.g., train.yaml ) with the following content: batch : 16 # Batch size epochs : 100 # Number of epochs weights : yolov5s.pt # Pretrained weights img-size : 640 # Image size Train the Model Run the following script to train the model using Python: from ultralytics import YOLO # Load a new or pre-trained model model = YOLO ( 'path/to/model.yaml' ) # For a new model # model = YOLO('path/to/pretrained_model.pt') # For a pre-trained model # Train the model model . train ( data = 'path/to/dataset.yaml' , epochs = 100 , imgsz = 640 , device = 'cuda' , # or 'cpu', or a list of devices like [0, 1] batch_size = 16 ) Make sure to adjust the paths to your data.yaml file and the number of epochs accordingly.","title":"Training YOLOV8 on DVR footage"},{"location":"projects/ai_powered_property_surveillance/training/#training-yolov8-on-labeled-data-from-label-studio","text":"","title":"Training YOLOv8 on Labeled Data from Label Studio"},{"location":"projects/ai_powered_property_surveillance/training/#yolov8-training-data-for-varied-conditions","text":"","title":"YOLOv8 Training Data for Varied Conditions"},{"location":"projects/ai_powered_property_surveillance/training/#images","text":"Collect a diverse dataset comprising several thousand annotated images for each of the DVR camera views. Include images that represent different times of day, seasons, weather conditions, and lighting scenarios. Ensure a range of backgrounds and object poses to enhance model generalization.","title":"Images"},{"location":"projects/ai_powered_property_surveillance/training/#labels","text":"Annotate each image with bounding boxes to accurately mark objects for detection. Create labels for each object, considering variations in appearance under different conditions. Note: Adequate representation of diverse conditions is crucial for robust object detection. Refer to YOLOv8 documentation for specific recommendations and guidelines.","title":"Labels"},{"location":"projects/ai_powered_property_surveillance/training/#1-prepare-your-dataset","text":"","title":"1. Prepare Your Dataset"},{"location":"projects/ai_powered_property_surveillance/training/#export-your-dataset","text":"Export your dataset from Label Studio in the YOLO format. This will generate a .zip file containing the images and annotations in the required format.","title":"Export Your Dataset"},{"location":"projects/ai_powered_property_surveillance/training/#organize-your-dataset","text":"Ensure your dataset is properly organized. Each image in the images folder should have a corresponding annotation file in the labels folder.","title":"Organize Your Dataset"},{"location":"projects/ai_powered_property_surveillance/training/#dataset-structure","text":"project-1-at-2023-12-10-11-21-cc95541c images : Contains all the .bmp image files. labels : Contains corresponding .txt files for each image in YOLO format.","title":"Dataset Structure"},{"location":"projects/ai_powered_property_surveillance/training/#prepare-label-files","text":"Each label file should have the following format: One line per object. Each line: <class_index> <x_center> <y_center> <width> <height> , normalized to [0,1].","title":"Prepare Label Files"},{"location":"projects/ai_powered_property_surveillance/training/#create-a-train-val-and-test-split","text":"Create a train, val, and test split of your dataset. The train set should contain 70% of the images, the val set should contain 15% of the images, and the test set should contain 15% of the images. This is the recommended split, but you can adjust it according to your needs. Before running a Python script to split your dataset, ensure you have the necessary directory structure: project-1-at-2023-12-10-11-21-cc95541c \u251c\u2500 images \u2514\u2500 labels import os import shutil import random # Set the seed for reproducibility random . seed ( 42 ) # Paths base_path = 'project-1-at-2023-12-10-11-21-cc95541c' images_path = os . path . join ( base_path , 'images' ) labels_path = os . path . join ( base_path , 'labels' ) # Split Ratios train_ratio = 0.70 val_ratio = 0.15 test_ratio = 0.15 # Create directories for train, val, and test sets for set_type in [ 'train' , 'val' , 'test' ]: for content_type in [ 'images' , 'labels' ]: os . makedirs ( os . path . join ( base_path , set_type , content_type ), exist_ok = True ) # Get all image filenames all_files = [ f for f in os . listdir ( images_path ) if os . path . isfile ( os . path . join ( images_path , f ))] random . shuffle ( all_files ) # Calculate split indices total_files = len ( all_files ) train_end = int ( train_ratio _ total_files ) val_end = train_end + int ( val_ratio _ total_files ) # Split files train_files = all_files [: train_end ] val_files = all_files [ train_end : val_end ] test_files = all_files [ val_end :] # Function to copy files def copy_files ( files , set_type ): for file in files : # Copy image shutil . copy ( os . path . join ( images_path , file ), os . path . join ( base_path , set_type , 'images' )) # Copy corresponding label label_file = file . rsplit ( '.' , 1 )[ 0 ] + '.txt' shutil . copy ( os . path . join ( labels_path , label_file ), os . path . join ( base_path , set_type , 'labels' )) # Copy files to respective directories copy_files ( train_files , 'train' ) copy_files ( val_files , 'val' ) copy_files ( test_files , 'test' ) print ( \"Dataset successfully split into train, val, and test sets.\" ) Make sure to adjust the base_path to the correct path where your project-1-at-2023-12-10-11-21-cc95541c folder is located. How to Run the Script Save the script as a .py file, e.g., split_dataset.py . Run the script using Python from your command line or terminal. The script will create train, val, and test folders with images and labels subdirectories, and distribute your dataset accordingly.","title":"Create a train, val, and test split"},{"location":"projects/ai_powered_property_surveillance/training/#2-training-yolov8-on-custom-dataset","text":"","title":"2. Training YOLOv8 on Custom Dataset"},{"location":"projects/ai_powered_property_surveillance/training/#create-a-data-configuration-file","text":"Create a YAML file (e.g., data.yaml ) with the following content: train : ./project-1-at-2023-12-10-11-21-cc95541c/images # Training images path val : ./path-to-validation-set # Validation images path nc : <number_of_classes> # Number of classes names : [ 'class1' , 'class2' , ... ] # Class names Replace with the actual number of classes and ['class1', 'class2', ...] with your class names. Example: train : ./project-1-at-2023-12-10-11-21-cc95541c/train/images # Path to training images val : ./project-1-at-2023-12-10-11-21-cc95541c/val/images # Path to validation images test : ./project-1-at-2023-12-10-11-21-cc95541c/test/images # Path to test images (optional) nc : 7 # Number of classes names : - Car - Closed Glass Door - Dog - Metal Roofing - Open Glass Door - Person - Semi Truck Make sure to adjust the paths to your train and val sets accordingly.","title":"Create a Data Configuration File"},{"location":"projects/ai_powered_property_surveillance/training/#configure-training-parameters","text":"Create a YAML file (e.g., train.yaml ) with the following content: batch : 16 # Batch size epochs : 100 # Number of epochs weights : yolov5s.pt # Pretrained weights img-size : 640 # Image size","title":"Configure Training Parameters"},{"location":"projects/ai_powered_property_surveillance/training/#train-the-model","text":"Run the following script to train the model using Python: from ultralytics import YOLO # Load a new or pre-trained model model = YOLO ( 'path/to/model.yaml' ) # For a new model # model = YOLO('path/to/pretrained_model.pt') # For a pre-trained model # Train the model model . train ( data = 'path/to/dataset.yaml' , epochs = 100 , imgsz = 640 , device = 'cuda' , # or 'cpu', or a list of devices like [0, 1] batch_size = 16 ) Make sure to adjust the paths to your data.yaml file and the number of epochs accordingly.","title":"Train the Model"},{"location":"projects/dubbing_pipeline/dub_with_elevenlabs/","text":"Dub with ElevenLabs Google Chrome Extension At Passivebot , we\u2019re always looking for better and more efficient ways to improve user experience. In line with this goal, we are excited to get working on our latest project: Dub with ElevenLabs Google Chrome Extension. This extension streamlines the experience of viewing dubbed content on YouTube by allowing users to easily switch between dubbed and original audio tracks with a single click. Dubbing Dubbing is a new technology but it is rapidly gaining traction and has already been used to create episodes of popular anime shows like Attack on Titan, My Hero Academia , and Boku no Hero Academia. It is the process of replacing one language with another in a movie or TV series. From Hollywood movies released in multiple languages to Japanese anime dubbed in Spanish, dubbing is now commonplace among content streaming services. Dubbing has now become even more accessible thanks to AI dubbing, a technology developed by ElevenLabs . AI dubbing makes it easier for content creators to offer their work in multiple languages by leveraging the power of artificial intelligence. It ensures that the original speaker's voice characteristics are preserved, which is especially important with dubbed content like film and anime. Dub with ElevenLabs Without an official release of a Dubbing API by ElevenLabs , Passivebot independently developed a Chrome extension named \"Dub with \" ElevenLabs .\" This tool facilitates the viewing of AI-dubbed content on YouTube, targeting content-creators who have not yet adopted AI dubbing technology. It simplifies the process for users to experience dubbed video content. Requirements: Allow users to watch dubbed content on YouTube with a single click The extension will need to simplify the process of having to enter the YouTube link of the video on ElevenLabs to a single click. We intend to do this by running a background script that will automatically detect if the user is on a YouTube video page. If the user is on a YouTube video page, the extension will display a button that will allow the user to automatically input the YouTube link of the video into the ElevenLabs AI dubbing tool, wait for the video to be dubbed, and then switch the video to the dubbed version by changing the video stream to the link of the dubbed video. Design Originally, textension will be designed to be as simple as possible. It will have a single button that will allow the user perform the following actions: Automatically input the YouTube link of the video into the ElevenLabs AI dubbing tool Wait for the video to be dubbed Switch the video to the dubbed version by changing the video stream to the link of the dubbed video Implementation We will be using the following technologies to implement the extension: JavaScript HTML CSS Progress 2023-11-10 12:52 I've created a repository for the project and added the following files: 1. manifest.json : This file contains the metadata for the extension along with what files to load and what permissions to request. 2. background.js : This file contains the background script that will run in the background and detect if the user is on a YouTube video page. 3. content.js : This file contains the content script that will run on the YouTube video page and display the button to the user. I've been able to successfully load the extension in developer mode and have the background script detect if the user is on a YouTube video page. I've also been able to successfully open a new tab with ElevenLabs and input the YouTube link of the video into the AI dubbing tool. I'm currently working on waiting for the video to be dubbed and then switching the video to the dubbed version by changing the video stream to the link of the dubbed video. 2023-11-11 20:29 The project is on pause for my subscription to renew and for new credits to be added to my account. At the current subscription prices, it costs $11 per hour of dubbing using the Creator plan at $22/mo and $8.25 per hour using the Growing Business plan at $330/mo. In the meantime, I'll be look into free alternatives to ElevenLabs .","title":"Dub with ElevenLabs Google Chrome Extension"},{"location":"projects/dubbing_pipeline/dub_with_elevenlabs/#dub-with-elevenlabs-google-chrome-extension","text":"At Passivebot , we\u2019re always looking for better and more efficient ways to improve user experience. In line with this goal, we are excited to get working on our latest project: Dub with ElevenLabs Google Chrome Extension. This extension streamlines the experience of viewing dubbed content on YouTube by allowing users to easily switch between dubbed and original audio tracks with a single click.","title":"Dub with ElevenLabs Google Chrome Extension"},{"location":"projects/dubbing_pipeline/dub_with_elevenlabs/#dubbing","text":"Dubbing is a new technology but it is rapidly gaining traction and has already been used to create episodes of popular anime shows like Attack on Titan, My Hero Academia , and Boku no Hero Academia. It is the process of replacing one language with another in a movie or TV series. From Hollywood movies released in multiple languages to Japanese anime dubbed in Spanish, dubbing is now commonplace among content streaming services. Dubbing has now become even more accessible thanks to AI dubbing, a technology developed by ElevenLabs . AI dubbing makes it easier for content creators to offer their work in multiple languages by leveraging the power of artificial intelligence. It ensures that the original speaker's voice characteristics are preserved, which is especially important with dubbed content like film and anime.","title":"Dubbing"},{"location":"projects/dubbing_pipeline/dub_with_elevenlabs/#dub-with-elevenlabs","text":"Without an official release of a Dubbing API by ElevenLabs , Passivebot independently developed a Chrome extension named \"Dub with \" ElevenLabs .\" This tool facilitates the viewing of AI-dubbed content on YouTube, targeting content-creators who have not yet adopted AI dubbing technology. It simplifies the process for users to experience dubbed video content. Requirements: Allow users to watch dubbed content on YouTube with a single click The extension will need to simplify the process of having to enter the YouTube link of the video on ElevenLabs to a single click. We intend to do this by running a background script that will automatically detect if the user is on a YouTube video page. If the user is on a YouTube video page, the extension will display a button that will allow the user to automatically input the YouTube link of the video into the ElevenLabs AI dubbing tool, wait for the video to be dubbed, and then switch the video to the dubbed version by changing the video stream to the link of the dubbed video.","title":"Dub with ElevenLabs"},{"location":"projects/dubbing_pipeline/dub_with_elevenlabs/#design","text":"Originally, textension will be designed to be as simple as possible. It will have a single button that will allow the user perform the following actions: Automatically input the YouTube link of the video into the ElevenLabs AI dubbing tool Wait for the video to be dubbed Switch the video to the dubbed version by changing the video stream to the link of the dubbed video","title":"Design"},{"location":"projects/dubbing_pipeline/dub_with_elevenlabs/#implementation","text":"We will be using the following technologies to implement the extension: JavaScript HTML CSS","title":"Implementation"},{"location":"projects/dubbing_pipeline/dub_with_elevenlabs/#progress","text":"","title":"Progress"},{"location":"projects/dubbing_pipeline/dub_with_elevenlabs/#2023-11-10-1252","text":"I've created a repository for the project and added the following files: 1. manifest.json : This file contains the metadata for the extension along with what files to load and what permissions to request. 2. background.js : This file contains the background script that will run in the background and detect if the user is on a YouTube video page. 3. content.js : This file contains the content script that will run on the YouTube video page and display the button to the user. I've been able to successfully load the extension in developer mode and have the background script detect if the user is on a YouTube video page. I've also been able to successfully open a new tab with ElevenLabs and input the YouTube link of the video into the AI dubbing tool. I'm currently working on waiting for the video to be dubbed and then switching the video to the dubbed version by changing the video stream to the link of the dubbed video.","title":"2023-11-10 12:52"},{"location":"projects/dubbing_pipeline/dub_with_elevenlabs/#2023-11-11-2029","text":"The project is on pause for my subscription to renew and for new credits to be added to my account. At the current subscription prices, it costs $11 per hour of dubbing using the Creator plan at $22/mo and $8.25 per hour using the Growing Business plan at $330/mo. In the meantime, I'll be look into free alternatives to ElevenLabs .","title":"2023-11-11 20:29"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/","text":"Midjourney Automation Bot Overview The Midjourney Automation Bot is a groundbreaking, open-source project that leverages the capabilities of OpenAI's GPT-3 model for automated image generation. This tool is specifically designed to interact with Discord channels, enabling users to create a wide array of art forms such as illustrations, digital paintings, or sketches through simple text prompts. It stands out for its ease of use, making it a valuable asset for artists, developers, and enthusiasts in the realm of automated image generation. Key Features Automated Discord Interaction: The bot is programmed to autonomously interact with Discord channels, streamlining the process of sending and receiving image generation commands. User-Defined Prompts: Users can input custom prompts to guide the image generation process, offering a high degree of creative control. GPT-3 Integration: Utilizing OpenAI's GPT-3 model, the bot can interpret prompts and generate corresponding images with remarkable accuracy and creativity. Customizable Upscale Options: The bot includes options for upscaling the generated images, allowing users to enhance image quality according to their needs. User-Friendly Web Interface: Equipped with a web interface, the bot offers an intuitive platform for users to interact with and control its functions. Robust Logging: The bot maintains detailed logs of its operations and any encountered errors, ensuring transparency and ease of troubleshooting. Open Source and Customizable: Released under the MIT License, the bot encourages broad usage and modification, catering to a wide range of applications and user modifications. Setup Guide Step 1: Clone the Repository Open your command line interface (CLI). Navigate to the directory where you want to clone the repository. Run the following command: git clone https://github.com/passivebot/midjourney-automation-bot.git This will create a copy of the repository on your local machine. Step 2: Install Dependencies Ensure you have Python and pip installed on your machine. If not, download and install Python from python.org. Pip is included automatically. Navigate to the directory of the cloned repository in your CLI. Use the cd command to change directories. For example: bash cd midjourney-automation-bot Once in the directory, run the following command to install the required dependencies: pip install -r requirements.txt This command will install all the Python packages listed in the requirements.txt file. Step 3: Set the OpenAI API Key in the Environment Variable You need to have an OpenAI API key. If you don't have one, you can obtain it from the OpenAI API portal. Setting the environment variable depends on your operating system. For Windows: Open Command Prompt or PowerShell. Run the following command (replace Your_API_Key with your actual API key): setx OPENAI_API_KEY \"Your_API_Key\" Restart your CLI to apply the changes. For macOS/Linux: Open Terminal. Add the export command to your shell profile file (like .bashrc, .zshrc, etc.). For example, if you're using bash, you can run: echo 'export OPENAI_API_KEY=\"Your_API_Key\"' >> ~/.bashrc Replace Your_API_Key with your actual API key. Apply the changes by running source ~/.bashrc (or the respective file for your shell). Usage Guide Step 1: Open Discord Open your browser and go to Discord. Log in to your Discord account. Navigate to the channel where you've added the Midjourney Discord bot too and copy the URL of the channel. Step 2: Start the Bot Open your CLI and navigate to the directory of the cloned repository. Run the following command: python main.py This will start the bot and open a new browser window with the GUI. Enter the required details including the Discord channel URL, the bot command, the art type, the descriptors, and the topic. Click on the Start Bot button to start the bot. The bot will now start generating ten images in the specified channel, upscale them, and download them to your local machine.","title":"Midjourney Automation Bot"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#midjourney-automation-bot","text":"","title":"Midjourney Automation Bot"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#overview","text":"The Midjourney Automation Bot is a groundbreaking, open-source project that leverages the capabilities of OpenAI's GPT-3 model for automated image generation. This tool is specifically designed to interact with Discord channels, enabling users to create a wide array of art forms such as illustrations, digital paintings, or sketches through simple text prompts. It stands out for its ease of use, making it a valuable asset for artists, developers, and enthusiasts in the realm of automated image generation.","title":"Overview"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#key-features","text":"Automated Discord Interaction: The bot is programmed to autonomously interact with Discord channels, streamlining the process of sending and receiving image generation commands. User-Defined Prompts: Users can input custom prompts to guide the image generation process, offering a high degree of creative control. GPT-3 Integration: Utilizing OpenAI's GPT-3 model, the bot can interpret prompts and generate corresponding images with remarkable accuracy and creativity. Customizable Upscale Options: The bot includes options for upscaling the generated images, allowing users to enhance image quality according to their needs. User-Friendly Web Interface: Equipped with a web interface, the bot offers an intuitive platform for users to interact with and control its functions. Robust Logging: The bot maintains detailed logs of its operations and any encountered errors, ensuring transparency and ease of troubleshooting. Open Source and Customizable: Released under the MIT License, the bot encourages broad usage and modification, catering to a wide range of applications and user modifications.","title":"Key Features"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#setup-guide","text":"","title":"Setup Guide"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#step-1-clone-the-repository","text":"Open your command line interface (CLI). Navigate to the directory where you want to clone the repository. Run the following command: git clone https://github.com/passivebot/midjourney-automation-bot.git This will create a copy of the repository on your local machine.","title":"Step 1: Clone the Repository"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#step-2-install-dependencies","text":"Ensure you have Python and pip installed on your machine. If not, download and install Python from python.org. Pip is included automatically. Navigate to the directory of the cloned repository in your CLI. Use the cd command to change directories. For example: bash cd midjourney-automation-bot Once in the directory, run the following command to install the required dependencies: pip install -r requirements.txt This command will install all the Python packages listed in the requirements.txt file.","title":"Step 2: Install Dependencies"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#step-3-set-the-openai-api-key-in-the-environment-variable","text":"You need to have an OpenAI API key. If you don't have one, you can obtain it from the OpenAI API portal. Setting the environment variable depends on your operating system. For Windows: Open Command Prompt or PowerShell. Run the following command (replace Your_API_Key with your actual API key): setx OPENAI_API_KEY \"Your_API_Key\" Restart your CLI to apply the changes. For macOS/Linux: Open Terminal. Add the export command to your shell profile file (like .bashrc, .zshrc, etc.). For example, if you're using bash, you can run: echo 'export OPENAI_API_KEY=\"Your_API_Key\"' >> ~/.bashrc Replace Your_API_Key with your actual API key. Apply the changes by running source ~/.bashrc (or the respective file for your shell).","title":"Step 3: Set the OpenAI API Key in the Environment Variable"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#usage-guide","text":"","title":"Usage Guide"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#step-1-open-discord","text":"Open your browser and go to Discord. Log in to your Discord account. Navigate to the channel where you've added the Midjourney Discord bot too and copy the URL of the channel.","title":"Step 1: Open Discord"},{"location":"projects/midjourney_automation_bot/wiki/midjourney_automation_bot_overview/#step-2-start-the-bot","text":"Open your CLI and navigate to the directory of the cloned repository. Run the following command: python main.py This will start the bot and open a new browser window with the GUI. Enter the required details including the Discord channel URL, the bot command, the art type, the descriptors, and the topic. Click on the Start Bot button to start the bot. The bot will now start generating ten images in the specified channel, upscale them, and download them to your local machine.","title":"Step 2: Start the Bot"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/","text":"Feature Request: Auto Posting Images on Instagram with Enhanced Customization Description This feature includes dynamically generated captions using GPT-4V and enables the bot to upload random albums or individual images to Instagram. The captions are generated based on specific prompts, and the images are selected randomly from specified directories. Priority High Priority : This feature is essential for automating social media content and enhancing user engagement. It can save time and effort for content creators and marketers. Implementation Plan 1. Caption Generation Objective : Generate professional and formal captions using OpenAI's GPT-3 model. Method : Utilize OpenAI's API to create captions based on specific prompts related to the images. Dependencies : OpenAI API key, Python 3.9.4. 2. Image Selection and Processing Objective : Select random albums or individual images from specified folders. Method : Use Python's os and random libraries to choose random folders or files. Convert PNG images to JPG using the PIL library. Dependencies : Python's os , random , and PIL libraries. 3. Instagram Interaction Objective : Log in to Instagram and upload the selected images with generated captions. Method : Use the instagrapi library to log in, select images, and upload them with captions. Dependencies : instagrapi library, Instagram credentials. 4. Error Handling and Cleanup Objective : Handle exceptions and clean up local files after uploading. Method : Implement try-except blocks to catch errors and remove local files after successful uploads. Dependencies : Python's os library. Technical Considerations Language : Python 3.9.4. Libraries : openai , instagrapi , PIL , os , random . Environment Variables : OpenAI API key, Instagram username, and password. Compatibility : Ensure compatibility with different image formats and Instagram's API changes. Security : Securely handle credentials and API keys. Testing : Implement unit tests to validate each component of the feature. Documentation : Provide detailed documentation for setup, usage, and troubleshooting. Conclusion The \"Auto Posting Images on Instagram with Enhanced Customization\" feature is a high-priority enhancement that can significantly streamline social media management. The implementation plan outlines a systematic approach to developing this feature, considering various technical aspects and dependencies. Proper testing, security measures, and comprehensive documentation will be vital for the successful deployment and user adoption of this feature.","title":"Automated Instagram image uploading with advanced captioning"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#feature-request-auto-posting-images-on-instagram-with-enhanced-customization","text":"","title":"Feature Request: Auto Posting Images on Instagram with Enhanced Customization"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#description","text":"This feature includes dynamically generated captions using GPT-4V and enables the bot to upload random albums or individual images to Instagram. The captions are generated based on specific prompts, and the images are selected randomly from specified directories.","title":"Description"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#priority","text":"High Priority : This feature is essential for automating social media content and enhancing user engagement. It can save time and effort for content creators and marketers.","title":"Priority"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#implementation-plan","text":"","title":"Implementation Plan"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#1-caption-generation","text":"Objective : Generate professional and formal captions using OpenAI's GPT-3 model. Method : Utilize OpenAI's API to create captions based on specific prompts related to the images. Dependencies : OpenAI API key, Python 3.9.4.","title":"1. Caption Generation"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#2-image-selection-and-processing","text":"Objective : Select random albums or individual images from specified folders. Method : Use Python's os and random libraries to choose random folders or files. Convert PNG images to JPG using the PIL library. Dependencies : Python's os , random , and PIL libraries.","title":"2. Image Selection and Processing"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#3-instagram-interaction","text":"Objective : Log in to Instagram and upload the selected images with generated captions. Method : Use the instagrapi library to log in, select images, and upload them with captions. Dependencies : instagrapi library, Instagram credentials.","title":"3. Instagram Interaction"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#4-error-handling-and-cleanup","text":"Objective : Handle exceptions and clean up local files after uploading. Method : Implement try-except blocks to catch errors and remove local files after successful uploads. Dependencies : Python's os library.","title":"4. Error Handling and Cleanup"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#technical-considerations","text":"Language : Python 3.9.4. Libraries : openai , instagrapi , PIL , os , random . Environment Variables : OpenAI API key, Instagram username, and password. Compatibility : Ensure compatibility with different image formats and Instagram's API changes. Security : Securely handle credentials and API keys. Testing : Implement unit tests to validate each component of the feature. Documentation : Provide detailed documentation for setup, usage, and troubleshooting.","title":"Technical Considerations"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/automated_instagram_image_uploading_with_advanced_captioning/#conclusion","text":"The \"Auto Posting Images on Instagram with Enhanced Customization\" feature is a high-priority enhancement that can significantly streamline social media management. The implementation plan outlines a systematic approach to developing this feature, considering various technical aspects and dependencies. Proper testing, security measures, and comprehensive documentation will be vital for the successful deployment and user adoption of this feature.","title":"Conclusion"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/","text":"Feature Request: Improving Documentation Description This feature focuses on enhancing the existing documentation of the project, making it more comprehensive, user-friendly, and accessible. It includes updating the current guides, adding new tutorials, and providing clear examples for better understanding. Priority Medium Priority : Improved documentation is vital for both new users and experienced developers, facilitating easier navigation and understanding of the system. Implementation Plan 1. Assessing Current Documentation Objective : Identify gaps, outdated information, and areas that need clarification in the existing documentation. Method : Review the current documentation, gather feedback from users, and create a list of areas for improvement. 2. Updating Existing Guides Objective : Revise and update existing guides to reflect the latest changes and best practices. Method : Collaborate with developers to ensure that the documentation aligns with the current codebase and functionalities. 3. Creating New Tutorials and Examples Objective : Provide step-by-step tutorials and clear examples to assist users in understanding how to use the system. Method : Work with experienced users and developers to create practical examples and tutorials. 4. Enhancing Accessibility Objective : Make documentation accessible to a broader audience, including non-technical users. Method : Use simple language, add visual aids, and ensure that the documentation is available in different formats. Technical Considerations Language : English, with potential translations into other languages. Tools : Markdown, documentation generators like Sphinx or Jekyll. Compatibility : Ensure that the documentation is viewable on different devices and browsers. Testing : Regularly review and update the documentation to keep it current. Collaboration : Engage with the community for feedback and contributions. Timeline -[ ] Week 1-2 : Assessing current documentation and identifying areas for improvement. - [ ] Week 3-4 : Updating existing guides and aligning them with the latest changes. -[ ] Week 5-6 : Creating new tutorials and examples. -[ ] Week 7-8 : Enhancing accessibility and final review. Conclusion Improved documentation is essential for the growth and sustainability of the project. It not only helps new users to get started quickly but also supports experienced developers in understanding the more complex aspects of the system. A systematic approach, collaboration with the community, and regular updates will ensure that the documentation remains an invaluable resource for all users.","title":"Improved Documentation"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#feature-request-improving-documentation","text":"","title":"Feature Request: Improving Documentation"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#description","text":"This feature focuses on enhancing the existing documentation of the project, making it more comprehensive, user-friendly, and accessible. It includes updating the current guides, adding new tutorials, and providing clear examples for better understanding.","title":"Description"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#priority","text":"Medium Priority : Improved documentation is vital for both new users and experienced developers, facilitating easier navigation and understanding of the system.","title":"Priority"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#implementation-plan","text":"","title":"Implementation Plan"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#1-assessing-current-documentation","text":"Objective : Identify gaps, outdated information, and areas that need clarification in the existing documentation. Method : Review the current documentation, gather feedback from users, and create a list of areas for improvement.","title":"1. Assessing Current Documentation"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#2-updating-existing-guides","text":"Objective : Revise and update existing guides to reflect the latest changes and best practices. Method : Collaborate with developers to ensure that the documentation aligns with the current codebase and functionalities.","title":"2. Updating Existing Guides"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#3-creating-new-tutorials-and-examples","text":"Objective : Provide step-by-step tutorials and clear examples to assist users in understanding how to use the system. Method : Work with experienced users and developers to create practical examples and tutorials.","title":"3. Creating New Tutorials and Examples"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#4-enhancing-accessibility","text":"Objective : Make documentation accessible to a broader audience, including non-technical users. Method : Use simple language, add visual aids, and ensure that the documentation is available in different formats.","title":"4. Enhancing Accessibility"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#technical-considerations","text":"Language : English, with potential translations into other languages. Tools : Markdown, documentation generators like Sphinx or Jekyll. Compatibility : Ensure that the documentation is viewable on different devices and browsers. Testing : Regularly review and update the documentation to keep it current. Collaboration : Engage with the community for feedback and contributions.","title":"Technical Considerations"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#timeline","text":"-[ ] Week 1-2 : Assessing current documentation and identifying areas for improvement. - [ ] Week 3-4 : Updating existing guides and aligning them with the latest changes. -[ ] Week 5-6 : Creating new tutorials and examples. -[ ] Week 7-8 : Enhancing accessibility and final review.","title":"Timeline"},{"location":"projects/midjourney_automation_bot/wiki/features/requested_features/improved_documentation/#conclusion","text":"Improved documentation is essential for the growth and sustainability of the project. It not only helps new users to get started quickly but also supports experienced developers in understanding the more complex aspects of the system. A systematic approach, collaboration with the community, and regular updates will ensure that the documentation remains an invaluable resource for all users.","title":"Conclusion"},{"location":"projects/runescapegpt/wiki/runescapegpt_overview/","text":"An Overview of RunescapeGPT: Enhancing Bot Functionality through Advanced AI Techniques and Community-Driven Data RunescapeGPT emerges as an exemplary innovation in the domain of gaming automation for Old School RuneScape (OSRS), characterized by its integration of Retrieval-Augmented Generation (RAG) and the utilization of datasets derived from the DreamBot community forums. This sophisticated system represents the zenith of collaborative intelligence, channeling the collective expertise of gamers to refine and elevate the operational efficiency of gaming bots. Leveraging Collective Insight The compendium of knowledge contained within the DreamBot forums encapsulates a diverse range of discussions, strategies, and technical expertise from dedicated OSRS enthusiasts. By assimilating this repository of shared knowledge, RunescapeGPT transcends traditional bot capabilities, employing the nuanced wisdom of seasoned players to enhance its algorithmic performance. The Strategic Advantage of Retrieval-Augmented Generation RAG represents a paradigm shift in machine learning, merging the generative prowess of language models with robust information retrieval systems. In the context of RunescapeGPT, this technology strategically extracts pertinent information from the DreamBot forums, which then informs and refines the bot\u2019s code generation processes. Such a methodology ensures that the bot\u2019s actions are not only contextually appropriate but also reflective of the community\u2019s best practices. Capabilities of RunescapeGPT Advanced Adaptive Learning: The bot exhibits an aptitude for learning from both the direct outcomes of its actions and the indirect input of aggregated community data, fostering a progressively intelligent gameplay experience. Refined Dynamic Code Generation: RunescapeGPT\u2019s application of RAG enables it to dynamically generate codes that are attuned to the fluctuating scenarios within the game, yielding a more effective and strategic bot functionality. Community-Informed Development: By harnessing the collective intelligence of the DreamBot forums, RunescapeGPT benefits from a continuous influx of contemporary strategies and solutions, thus embodying a bot that is shaped by the community it serves. Commitment to Ethical Standards and Regulatory Compliance: The development of RunescapeGPT is meticulously aligned with ethical AI deployment practices, ensuring adherence to OSRS's terms of service and community conduct codes. Conclusion The inception of RunescapeGPT marks a significant advancement in gaming automation technology. By leveraging RAG and drawing upon the extensive datasets provided by the DreamBot forums, RunescapeGPT is not merely an automation tool but a learning entity that incrementally improves through collective gaming intelligence. This advancement not only solidifies RunescapeGPT's position as a leading-edge bot but also exemplifies the potential for AI to evolve in concert with player communities, setting a new benchmark for intelligent automation within the gaming milieu.","title":"RunescapeGPT"},{"location":"projects/runescapegpt/wiki/runescapegpt_overview/#an-overview-of-runescapegpt-enhancing-bot-functionality-through-advanced-ai-techniques-and-community-driven-data","text":"RunescapeGPT emerges as an exemplary innovation in the domain of gaming automation for Old School RuneScape (OSRS), characterized by its integration of Retrieval-Augmented Generation (RAG) and the utilization of datasets derived from the DreamBot community forums. This sophisticated system represents the zenith of collaborative intelligence, channeling the collective expertise of gamers to refine and elevate the operational efficiency of gaming bots.","title":"An Overview of RunescapeGPT: Enhancing Bot Functionality through Advanced AI Techniques and Community-Driven Data"},{"location":"projects/runescapegpt/wiki/runescapegpt_overview/#leveraging-collective-insight","text":"The compendium of knowledge contained within the DreamBot forums encapsulates a diverse range of discussions, strategies, and technical expertise from dedicated OSRS enthusiasts. By assimilating this repository of shared knowledge, RunescapeGPT transcends traditional bot capabilities, employing the nuanced wisdom of seasoned players to enhance its algorithmic performance.","title":"Leveraging Collective Insight"},{"location":"projects/runescapegpt/wiki/runescapegpt_overview/#the-strategic-advantage-of-retrieval-augmented-generation","text":"RAG represents a paradigm shift in machine learning, merging the generative prowess of language models with robust information retrieval systems. In the context of RunescapeGPT, this technology strategically extracts pertinent information from the DreamBot forums, which then informs and refines the bot\u2019s code generation processes. Such a methodology ensures that the bot\u2019s actions are not only contextually appropriate but also reflective of the community\u2019s best practices.","title":"The Strategic Advantage of Retrieval-Augmented Generation"},{"location":"projects/runescapegpt/wiki/runescapegpt_overview/#capabilities-of-runescapegpt","text":"Advanced Adaptive Learning: The bot exhibits an aptitude for learning from both the direct outcomes of its actions and the indirect input of aggregated community data, fostering a progressively intelligent gameplay experience. Refined Dynamic Code Generation: RunescapeGPT\u2019s application of RAG enables it to dynamically generate codes that are attuned to the fluctuating scenarios within the game, yielding a more effective and strategic bot functionality. Community-Informed Development: By harnessing the collective intelligence of the DreamBot forums, RunescapeGPT benefits from a continuous influx of contemporary strategies and solutions, thus embodying a bot that is shaped by the community it serves. Commitment to Ethical Standards and Regulatory Compliance: The development of RunescapeGPT is meticulously aligned with ethical AI deployment practices, ensuring adherence to OSRS's terms of service and community conduct codes.","title":"Capabilities of RunescapeGPT"},{"location":"projects/runescapegpt/wiki/runescapegpt_overview/#conclusion","text":"The inception of RunescapeGPT marks a significant advancement in gaming automation technology. By leveraging RAG and drawing upon the extensive datasets provided by the DreamBot forums, RunescapeGPT is not merely an automation tool but a learning entity that incrementally improves through collective gaming intelligence. This advancement not only solidifies RunescapeGPT's position as a leading-edge bot but also exemplifies the potential for AI to evolve in concert with player communities, setting a new benchmark for intelligent automation within the gaming milieu.","title":"Conclusion"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/","text":"Unfollow Non-Followers on Instagram Using Instagrapi Disclaimer : Before proceeding, it's important to note that using unofficial APIs can lead to potential violations of Instagram's terms of service. Such actions may result in restrictions on your account, up to and including a permanent ban. The information provided here is for educational purposes, and I strongly advise against misusing it in any way. Introduction In the world of Instagram, engagement and follower count are often seen as measures of popularity and influence. It's not uncommon for users to seek a balanced follower-to-following ratio. If you're looking to clean up your Instagram following list and unfollow users who do not follow you back, Instagrapi\u2014a powerful, unofficial Instagram API wrapper for Python\u2014can help automate this process. Code Install Instagrapi First and foremost, Instagrapi needs to be installed in your Python environment. It can be installed easily using pip, Python's package installer. Simply run the following command in your terminal or command prompt: pip install instagrapi This command downloads and installs the Instagrapi library, making its functionality available for your Python scripts. Create an Instagrapi Client To interact with Instagram, you must create an instance of the Instagrapi client. This is your gateway to performing API calls: from instagrapi import Client client = Client () This snippet imports the Client class from the Instagrapi package and instantiates it. Login to Instagram Next, you'll need to authenticate with Instagram. Replace \"username\" and \"password\" with your actual Instagram credentials: client . login ( \"username\" , \"password\" ) The login method signs you into Instagram, allowing your script to act on your behalf. Remember to keep your credentials secure and never share them with anyone. Get Your Followers and Following Now, retrieve the list of user IDs for both your followers and the accounts you follow: followers = client . user_followers ( client . user_id ) following = client . user_following ( client . user_id ) These methods return a dictionary of users who are your followers and users you are following, respectively. Identify Users to Unfollow With both lists at your disposal, you can now identify which users you follow that do not follow you back: users_to_unfollow = [ user for user in following if user not in followers ] This line of code uses a list comprehension to create a new list of user IDs that represents people you follow who aren't following you back. Unfollow Users Once you have the list of non-followers, you can loop through it and unfollow each user: for user_to_unfollow in users_to_unfollow : client . user_unfollow ( user_to_unfollow ) This loop calls the user_unfollow method for each user in the users_to_unfollow list, effectively cleaning up your following list. Logout (Optional) After the script has completed its task, you may choose to log out of the client session: client . logout () Logging out is a good practice to end the session, especially when running the script periodically. Conclusion This script serves as a tool for managing your social media presence more effectively especially if you have a large following. It can be run periodically to keep your following list clean and balanced.","title":"Unfollowing non-followers"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#unfollow-non-followers-on-instagram-using-instagrapi","text":"Disclaimer : Before proceeding, it's important to note that using unofficial APIs can lead to potential violations of Instagram's terms of service. Such actions may result in restrictions on your account, up to and including a permanent ban. The information provided here is for educational purposes, and I strongly advise against misusing it in any way.","title":"Unfollow Non-Followers on Instagram Using Instagrapi"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#introduction","text":"In the world of Instagram, engagement and follower count are often seen as measures of popularity and influence. It's not uncommon for users to seek a balanced follower-to-following ratio. If you're looking to clean up your Instagram following list and unfollow users who do not follow you back, Instagrapi\u2014a powerful, unofficial Instagram API wrapper for Python\u2014can help automate this process.","title":"Introduction"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#code","text":"","title":"Code"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#install-instagrapi","text":"First and foremost, Instagrapi needs to be installed in your Python environment. It can be installed easily using pip, Python's package installer. Simply run the following command in your terminal or command prompt: pip install instagrapi This command downloads and installs the Instagrapi library, making its functionality available for your Python scripts.","title":"Install Instagrapi"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#create-an-instagrapi-client","text":"To interact with Instagram, you must create an instance of the Instagrapi client. This is your gateway to performing API calls: from instagrapi import Client client = Client () This snippet imports the Client class from the Instagrapi package and instantiates it.","title":"Create an Instagrapi Client"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#login-to-instagram","text":"Next, you'll need to authenticate with Instagram. Replace \"username\" and \"password\" with your actual Instagram credentials: client . login ( \"username\" , \"password\" ) The login method signs you into Instagram, allowing your script to act on your behalf. Remember to keep your credentials secure and never share them with anyone.","title":"Login to Instagram"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#get-your-followers-and-following","text":"Now, retrieve the list of user IDs for both your followers and the accounts you follow: followers = client . user_followers ( client . user_id ) following = client . user_following ( client . user_id ) These methods return a dictionary of users who are your followers and users you are following, respectively.","title":"Get Your Followers and Following"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#identify-users-to-unfollow","text":"With both lists at your disposal, you can now identify which users you follow that do not follow you back: users_to_unfollow = [ user for user in following if user not in followers ] This line of code uses a list comprehension to create a new list of user IDs that represents people you follow who aren't following you back.","title":"Identify Users to Unfollow"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#unfollow-users","text":"Once you have the list of non-followers, you can loop through it and unfollow each user: for user_to_unfollow in users_to_unfollow : client . user_unfollow ( user_to_unfollow ) This loop calls the user_unfollow method for each user in the users_to_unfollow list, effectively cleaning up your following list.","title":"Unfollow Users"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#logout-optional","text":"After the script has completed its task, you may choose to log out of the client session: client . logout () Logging out is a good practice to end the session, especially when running the script periodically.","title":"Logout (Optional)"},{"location":"projects/social_media_automation/instagram/Unfollowing%20non_followers/unfollowing_non_followers/#conclusion","text":"This script serves as a tool for managing your social media presence more effectively especially if you have a large following. It can be run periodically to keep your following list clean and balanced.","title":"Conclusion"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/","text":"Downloading Media Introduction In this tutorial, we will be covering how to download all media from a specific Instagram profile along with captions and hashtags, then store this data in a SQLite database. This will allow us to store the data we scrape from Instagram to later be uploaded to Truth Social. We will be using the following tools: Instagrapi SQLite Project Description Due to the recent actions of mainstream technology giants against figures like Alex Jones and then-President Donald Trump, alternative digital ecosystems have grown in popularity. Platforms like Truth Social endorse unrestricted speech, attracting users looking for alternatives to the mainstream. By utilizing tools like Instagrapi and SQLite, we can store the data we scrape from Instagram to later be uploaded to Truth Social. Project Objectives Download, at minimum, 100 media from a specific Instagram profile along with captions and hashtags, then store this data in a SQLite database. Deadline Achieve the project goals by 2023-10-13. Choosing a Platform Truth Social trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"Truth Social\",\"geo\":\"US\",\"time\":\"today 12-m\"},{\"keyword\":\"Parler\",\"geo\":\"US\",\"time\":\"today 12-m\"},{\"keyword\":\"Gettr\",\"geo\":\"US\",\"time\":\"today 12-m\"},{\"keyword\":\"Gab\",\"geo\":\"US\",\"time\":\"today 12-m\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"geo=US&q=Truth%20Social,Parler,Gettr,Gab&hl=en&date=today 12-m,today 12-m,today 12-m,today 12-m\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); Based on the Google Trends data, Truth Social currently leads in popularity among alternative platforms. This indicates a potentially large user base, making it the ideal choice for profile replication. Selecting a Profile to Replicate Nicole Wolf Our initial step involves the careful selection of a profile to replicate. In the context of this tutorial, we have chosen to replicate the profile of a well-known Instagram influencer, Nicole Wolf, a professional web developer and dancer, known as @joeel56 . With over 176,000 followers and a public profile, Nicole's online presence is significant. Given that her German background aligns with the demographics of Truth Social, this profile is ideal for replication. Creating a SQLite Database Installation SQLite is a relational database management system that is embedded into the Python programming language. This database management system is well-documented and can be accessed here . We will be using this database management system to store the data we scrape from Instagram. The following sections will outline the steps required to create a SQLite database. Install the \"sqlite3\" library if you haven't already. You can install it using pip: pip install sqlite3 Database Initialization After installing the \"sqlite3\" library, we will need to create a SQLite database. This database will store the data we scrape from Instagram. To create a SQLite database, we will need to import the sqlite3 library. We will then need to initialize a connection to this database. This will create a SQLite database that we can use to store the data we scrape from Instagram. import sqlite3 # Create a connection to the SQLite database conn = sqlite3 . connect ( \"instagram_data.db\" ) Creating a Table for Media Data After creating a SQLite database, we will need to create a table in this database. This table will store the data we scrape from Instagram. To create a table in a SQLite database, we will need to create a cursor object. We will then need to execute a SQL query to create a table in our SQLite database. This will create a table in our SQLite database that we can use to store the data we scrape from Instagram. # Define cursor cursor = conn . cursor () # Establish a table structure cursor . execute ( ''' CREATE TABLE IF NOT EXISTS media_data ( id INTEGER PRIMARY KEY, media BLOB NOT NULL, caption TEXT, hashtags TEXT ) ''' ) # Commit changes and close connection conn . commit () conn . close () Replicating an Instagram Profile with Instagrapi Instagrapi Installation Instagrapi is a Python library that can be used to programmatically interact with Instagram. This library is well-documented and can be accessed here . We will be using this library to replicate our Instagram profile. The following sections will outline the steps required to replicate our Instagram profile. Install the \"instagrapi\" library if you haven't already. You can install it using pip: pip install instagrapi Creating an Instagram Session To download all media from a specific Instagram profile along with captions and hashtags, then store this data in a Notion database, we will first need to create an Instagram session. This session will allow us to programmatically interact with Instagram. To create an Instagram session, we will need to import the Client class from the instagrapi library. We will then need to create an instance of this class and pass in our Instagram username and password. This will create an Instagram session that we can use to programmatically interact with Instagram. from instagrapi import Client # Initialize the client client = Client () Authenticating an Instagram Session After creating an Instagram session, we will need to authenticate this session. This will allow us to programmatically interact with Instagram. To authenticate an Instagram session, we will need to call the login method on our Instagram session and pass in our Instagram username and password. This will authenticate our Instagram session and allow us to programmatically interact with Instagram. # Authenticate the client client . login ( \"username\" , \"password\" ) Downloading Media from an Instagram Profile After downloading all media from a specific Instagram profile, we will need to store this data in a SQLite database. This will allow us to store the data we scrape from Instagram. To store this data in a SQLite database, we will need to create a connection to our SQLite database. We will then need to create a cursor object. We will then need to execute a SQL query to insert this data into our SQLite database. This will store this data in our SQLite database and allow us to store the data we scrape from Instagram. def download_media_from_profile ( username , max_count = 100 ): user_info = cl . user_info_by_username ( username ) user_id = user_info . pk # Get the user's media medias = cl . user_medias ( user_id , amount = max_count ) for media in medias : media_id = media . pk caption = media . caption_text # Get the caption hashtags = [ tag . strip ( \"#\" ) for tag in caption . split () if tag . startswith ( \"#\" )] # Extract hashtags hashtags = \", \" . join ( hashtags ) # Convert hashtags to a string # Initialize media_blob as None to handle unexpected media types media_blob = None # Download the media (photo or video) and convert it to binary if media . media_type == 1 : # Photo path = cl . photo_download ( media_id , folder = f \" { os . getcwd () } \" ) media_blob = convert_to_binary ( path ) elif media . media_type == 2 : # Video path = cl . video_download ( media_id , folder = f \" { os . getcwd () } \" ) media_blob = convert_to_binary ( path ) # Check if the media_blob is already in the database cursor . execute ( \"SELECT id FROM media_data WHERE media = ?\" , ( media_blob ,)) existing_media = cursor . fetchone () # If the media doesn't exist in the database, insert it if not existing_media and media_blob : cursor . execute ( \"INSERT INTO media_data (media, caption, hashtags) VALUES (?, ?, ?)\" , ( media_blob , caption , hashtags )) conn . commit () # Optionally, remove the downloaded file to save space os . remove ( path ) logger . info ( f \"Downloaded media with caption: { caption } \" ) logger . info ( f \"Hashtags: { hashtags } \\n \" ) elif existing_media : logger . info ( f \"Media already exists in the database. Skipping download for media with caption: { caption } \" ) Closing an Instagram Session After downloading all media from a specific Instagram profile, we will need to close this Instagram session. This will allow us to store the data we scrape from Instagram. To close an Instagram session, we will need to call the logout method on our Instagram session. This will close our Instagram session and allow us to store the data we scrape from Instagram. # Close the client client . logout () Conclusion By following the above steps, one can efficiently download all media from a specific Instagram profile along with captions and hashtags, then store this data in a SQLite database. This will allow one to store the data we scrape from Instagram to later be uploaded to Truth Social.","title":"Downloading media"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#downloading-media","text":"","title":"Downloading Media"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#introduction","text":"In this tutorial, we will be covering how to download all media from a specific Instagram profile along with captions and hashtags, then store this data in a SQLite database. This will allow us to store the data we scrape from Instagram to later be uploaded to Truth Social. We will be using the following tools: Instagrapi SQLite","title":"Introduction"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#project-description","text":"Due to the recent actions of mainstream technology giants against figures like Alex Jones and then-President Donald Trump, alternative digital ecosystems have grown in popularity. Platforms like Truth Social endorse unrestricted speech, attracting users looking for alternatives to the mainstream. By utilizing tools like Instagrapi and SQLite, we can store the data we scrape from Instagram to later be uploaded to Truth Social.","title":"Project Description"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#project-objectives","text":"Download, at minimum, 100 media from a specific Instagram profile along with captions and hashtags, then store this data in a SQLite database.","title":"Project Objectives"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#deadline","text":"Achieve the project goals by 2023-10-13.","title":"Deadline"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#choosing-a-platform","text":"","title":"Choosing a Platform"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#truth-social","text":"trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"Truth Social\",\"geo\":\"US\",\"time\":\"today 12-m\"},{\"keyword\":\"Parler\",\"geo\":\"US\",\"time\":\"today 12-m\"},{\"keyword\":\"Gettr\",\"geo\":\"US\",\"time\":\"today 12-m\"},{\"keyword\":\"Gab\",\"geo\":\"US\",\"time\":\"today 12-m\"}],\"category\":0,\"property\":\"\"}, {\"exploreQuery\":\"geo=US&q=Truth%20Social,Parler,Gettr,Gab&hl=en&date=today 12-m,today 12-m,today 12-m,today 12-m\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); Based on the Google Trends data, Truth Social currently leads in popularity among alternative platforms. This indicates a potentially large user base, making it the ideal choice for profile replication.","title":"Truth Social"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#selecting-a-profile-to-replicate","text":"","title":"Selecting a Profile to Replicate"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#nicole-wolf","text":"Our initial step involves the careful selection of a profile to replicate. In the context of this tutorial, we have chosen to replicate the profile of a well-known Instagram influencer, Nicole Wolf, a professional web developer and dancer, known as @joeel56 . With over 176,000 followers and a public profile, Nicole's online presence is significant. Given that her German background aligns with the demographics of Truth Social, this profile is ideal for replication.","title":"Nicole Wolf"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#creating-a-sqlite-database","text":"","title":"Creating a SQLite Database"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#installation","text":"SQLite is a relational database management system that is embedded into the Python programming language. This database management system is well-documented and can be accessed here . We will be using this database management system to store the data we scrape from Instagram. The following sections will outline the steps required to create a SQLite database. Install the \"sqlite3\" library if you haven't already. You can install it using pip: pip install sqlite3","title":"Installation"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#database-initialization","text":"After installing the \"sqlite3\" library, we will need to create a SQLite database. This database will store the data we scrape from Instagram. To create a SQLite database, we will need to import the sqlite3 library. We will then need to initialize a connection to this database. This will create a SQLite database that we can use to store the data we scrape from Instagram. import sqlite3 # Create a connection to the SQLite database conn = sqlite3 . connect ( \"instagram_data.db\" )","title":"Database Initialization"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#creating-a-table-for-media-data","text":"After creating a SQLite database, we will need to create a table in this database. This table will store the data we scrape from Instagram. To create a table in a SQLite database, we will need to create a cursor object. We will then need to execute a SQL query to create a table in our SQLite database. This will create a table in our SQLite database that we can use to store the data we scrape from Instagram. # Define cursor cursor = conn . cursor () # Establish a table structure cursor . execute ( ''' CREATE TABLE IF NOT EXISTS media_data ( id INTEGER PRIMARY KEY, media BLOB NOT NULL, caption TEXT, hashtags TEXT ) ''' ) # Commit changes and close connection conn . commit () conn . close ()","title":"Creating a Table for Media Data"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#replicating-an-instagram-profile-with-instagrapi","text":"","title":"Replicating an Instagram Profile with Instagrapi"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#instagrapi-installation","text":"Instagrapi is a Python library that can be used to programmatically interact with Instagram. This library is well-documented and can be accessed here . We will be using this library to replicate our Instagram profile. The following sections will outline the steps required to replicate our Instagram profile. Install the \"instagrapi\" library if you haven't already. You can install it using pip: pip install instagrapi","title":"Instagrapi Installation"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#creating-an-instagram-session","text":"To download all media from a specific Instagram profile along with captions and hashtags, then store this data in a Notion database, we will first need to create an Instagram session. This session will allow us to programmatically interact with Instagram. To create an Instagram session, we will need to import the Client class from the instagrapi library. We will then need to create an instance of this class and pass in our Instagram username and password. This will create an Instagram session that we can use to programmatically interact with Instagram. from instagrapi import Client # Initialize the client client = Client ()","title":"Creating an Instagram Session"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#authenticating-an-instagram-session","text":"After creating an Instagram session, we will need to authenticate this session. This will allow us to programmatically interact with Instagram. To authenticate an Instagram session, we will need to call the login method on our Instagram session and pass in our Instagram username and password. This will authenticate our Instagram session and allow us to programmatically interact with Instagram. # Authenticate the client client . login ( \"username\" , \"password\" )","title":"Authenticating an Instagram Session"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#downloading-media-from-an-instagram-profile","text":"After downloading all media from a specific Instagram profile, we will need to store this data in a SQLite database. This will allow us to store the data we scrape from Instagram. To store this data in a SQLite database, we will need to create a connection to our SQLite database. We will then need to create a cursor object. We will then need to execute a SQL query to insert this data into our SQLite database. This will store this data in our SQLite database and allow us to store the data we scrape from Instagram. def download_media_from_profile ( username , max_count = 100 ): user_info = cl . user_info_by_username ( username ) user_id = user_info . pk # Get the user's media medias = cl . user_medias ( user_id , amount = max_count ) for media in medias : media_id = media . pk caption = media . caption_text # Get the caption hashtags = [ tag . strip ( \"#\" ) for tag in caption . split () if tag . startswith ( \"#\" )] # Extract hashtags hashtags = \", \" . join ( hashtags ) # Convert hashtags to a string # Initialize media_blob as None to handle unexpected media types media_blob = None # Download the media (photo or video) and convert it to binary if media . media_type == 1 : # Photo path = cl . photo_download ( media_id , folder = f \" { os . getcwd () } \" ) media_blob = convert_to_binary ( path ) elif media . media_type == 2 : # Video path = cl . video_download ( media_id , folder = f \" { os . getcwd () } \" ) media_blob = convert_to_binary ( path ) # Check if the media_blob is already in the database cursor . execute ( \"SELECT id FROM media_data WHERE media = ?\" , ( media_blob ,)) existing_media = cursor . fetchone () # If the media doesn't exist in the database, insert it if not existing_media and media_blob : cursor . execute ( \"INSERT INTO media_data (media, caption, hashtags) VALUES (?, ?, ?)\" , ( media_blob , caption , hashtags )) conn . commit () # Optionally, remove the downloaded file to save space os . remove ( path ) logger . info ( f \"Downloaded media with caption: { caption } \" ) logger . info ( f \"Hashtags: { hashtags } \\n \" ) elif existing_media : logger . info ( f \"Media already exists in the database. Skipping download for media with caption: { caption } \" )","title":"Downloading Media from an Instagram Profile"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#closing-an-instagram-session","text":"After downloading all media from a specific Instagram profile, we will need to close this Instagram session. This will allow us to store the data we scrape from Instagram. To close an Instagram session, we will need to call the logout method on our Instagram session. This will close our Instagram session and allow us to store the data we scrape from Instagram. # Close the client client . logout ()","title":"Closing an Instagram Session"},{"location":"projects/social_media_automation/truth_social/account_cloning/downloading_media/downloading_media/#conclusion","text":"By following the above steps, one can efficiently download all media from a specific Instagram profile along with captions and hashtags, then store this data in a SQLite database. This will allow one to store the data we scrape from Instagram to later be uploaded to Truth Social.","title":"Conclusion"},{"location":"projects/social_media_automation/truth_social/account_cloning/uploading_media/uploading_media/","text":"Uploading Media Introduction I am currently working on this section. Please check back later.","title":"Uploading Media"},{"location":"projects/social_media_automation/truth_social/account_cloning/uploading_media/uploading_media/#uploading-media","text":"","title":"Uploading Media"},{"location":"projects/social_media_automation/truth_social/account_cloning/uploading_media/uploading_media/#introduction","text":"I am currently working on this section. Please check back later.","title":"Introduction"},{"location":"projects/web_scraping/web_scraping_overview/","text":"Harnessing Facebook Marketplace Data with Python Scrapers Overview In the vast and competitive world of e-commerce, Facebook Marketplace stands out as a treasure trove of consumer data. To tap into this resource, I've developed three Python-based web scrapers, each tailored to navigate and extract valuable marketplace insights with precision and efficiency. This overview provides a glimpse into their mechanics and the philosophy behind their creation. Key Features Diverse Data Collection: The scrapers are designed to collect various data points, including product titles, prices, and locations, allowing for a comprehensive market analysis. Adaptability: Each tool has unique features suited for different scraping needs, from simple data extraction to handling dynamic content and large-scale operations. Efficient Automation: The scrapers automate the tedious task of data collection, enabling users to focus on data analysis and application. Ethical Considerations: These tools adhere to best practices in web scraping, ensuring they operate within the legal and ethical boundaries set by the target website. Comparative Analysis Comprehensive Python Scraper The first scraper is a full-fledged Python script that uses libraries like requests and BeautifulSoup to parse HTML content and sqlite3 to manage data storage. It is the most thorough tool among the three, designed for depth and detail in data collection. Proxy Service Scraper The second scraper operates through proxy services, demonstrating a lightweight and efficient approach to bypass common web scraping barriers. It is especially useful for quick data retrieval across different geographic locations Asynchronous Scraper The third scraper leverages asyncio and playwright, showcasing the power of asynchronous programming to perform multiple requests simultaneously. It is adept at dealing with JavaScript-heavy pages and is indicative of the future of web scraping. Setup Guide Prerequisites Python installation Pip for managing Python packages An understanding of command line operations Basic knowledge of HTML and web technologies Installation Clone the relevant repository. Install the required dependencies via pip. Configure necessary environmental variables, such as API keys. Usage Guide Navigate to the desired directory. Run the scraper using the Python command. Monitor the output and logs for data and potential errors. In conclusion, these three scrapers offer a window into the potential of web scraping for e-commerce platforms like Facebook Marketplace. With careful design and ethical usage, they serve as powerful tools for businesses and analysts to gain an edge in the ever-changing market landscape.","title":"Web scraping overview"},{"location":"projects/web_scraping/web_scraping_overview/#harnessing-facebook-marketplace-data-with-python-scrapers","text":"","title":"Harnessing Facebook Marketplace Data with Python Scrapers"},{"location":"projects/web_scraping/web_scraping_overview/#overview","text":"In the vast and competitive world of e-commerce, Facebook Marketplace stands out as a treasure trove of consumer data. To tap into this resource, I've developed three Python-based web scrapers, each tailored to navigate and extract valuable marketplace insights with precision and efficiency. This overview provides a glimpse into their mechanics and the philosophy behind their creation.","title":"Overview"},{"location":"projects/web_scraping/web_scraping_overview/#key-features","text":"Diverse Data Collection: The scrapers are designed to collect various data points, including product titles, prices, and locations, allowing for a comprehensive market analysis. Adaptability: Each tool has unique features suited for different scraping needs, from simple data extraction to handling dynamic content and large-scale operations. Efficient Automation: The scrapers automate the tedious task of data collection, enabling users to focus on data analysis and application. Ethical Considerations: These tools adhere to best practices in web scraping, ensuring they operate within the legal and ethical boundaries set by the target website.","title":"Key Features"},{"location":"projects/web_scraping/web_scraping_overview/#comparative-analysis","text":"","title":"Comparative Analysis"},{"location":"projects/web_scraping/web_scraping_overview/#comprehensive-python-scraper","text":"The first scraper is a full-fledged Python script that uses libraries like requests and BeautifulSoup to parse HTML content and sqlite3 to manage data storage. It is the most thorough tool among the three, designed for depth and detail in data collection.","title":"Comprehensive Python Scraper"},{"location":"projects/web_scraping/web_scraping_overview/#proxy-service-scraper","text":"The second scraper operates through proxy services, demonstrating a lightweight and efficient approach to bypass common web scraping barriers. It is especially useful for quick data retrieval across different geographic locations","title":"Proxy Service Scraper"},{"location":"projects/web_scraping/web_scraping_overview/#asynchronous-scraper","text":"The third scraper leverages asyncio and playwright, showcasing the power of asynchronous programming to perform multiple requests simultaneously. It is adept at dealing with JavaScript-heavy pages and is indicative of the future of web scraping.","title":"Asynchronous Scraper"},{"location":"projects/web_scraping/web_scraping_overview/#setup-guide","text":"","title":"Setup Guide"},{"location":"projects/web_scraping/web_scraping_overview/#prerequisites","text":"Python installation Pip for managing Python packages An understanding of command line operations Basic knowledge of HTML and web technologies","title":"Prerequisites"},{"location":"projects/web_scraping/web_scraping_overview/#installation","text":"Clone the relevant repository. Install the required dependencies via pip. Configure necessary environmental variables, such as API keys.","title":"Installation"},{"location":"projects/web_scraping/web_scraping_overview/#usage-guide","text":"Navigate to the desired directory. Run the scraper using the Python command. Monitor the output and logs for data and potential errors. In conclusion, these three scrapers offer a window into the potential of web scraping for e-commerce platforms like Facebook Marketplace. With careful design and ethical usage, they serve as powerful tools for businesses and analysts to gain an edge in the ever-changing market landscape.","title":"Usage Guide"},{"location":"projects/web_scraping/facebook/facebook_marketplace/bright_data_facebook_marketplace_scraper/","text":"TODO","title":"Bright Data Facebook Marketplace Scraper"},{"location":"projects/web_scraping/facebook/facebook_marketplace/oxylabs_facebook_marketplace_scraper/","text":"TODO","title":"Oxylabs Facebook Marketplace Scraper"},{"location":"projects/web_scraping/facebook/facebook_marketplace/smartproxy_facebook_marketplace_scraper/","text":"TODO","title":"Smartproxy Facebook Marketplace Scraper"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/","text":"Integrating Tailwind CSS with Material for MkDocs Insiders Introduction Combine the power of Tailwind CSS with Material for MkDocs Insiders to create a documentation site that not only has the robust features and beautiful design of Material but also the utility-first flexibility and customization of Tailwind CSS. This guide will walk you through the steps to integrate Tailwind CSS into your MkDocs project, allowing you to tailor the look and feel of your site with ease. Setting Up Material for MkDocs Start by setting up Material for MkDocs as per the official guidelines. Ensure you have a running MkDocs environment before proceeding with the integration of Tailwind CSS. Install Tailwind CSS Within your MkDocs project directory, run the following command to install the necessary packages: npm install tailwindcss postcss autoprefixer These packages include Tailwind CSS for the utility-first CSS framework, PostCSS for processing CSS with JavaScript, and Autoprefixer for handling CSS vendor prefixes. Setting up PostCSS and Tailwind Create a postcss.config.js file in the root of your MkDocs project with the following content: module . exports = { plugins : [ require ( 'tailwindcss' ), require ( 'autoprefixer' )] } Then, initialize your Tailwind configuration file which Tailwind uses to read your customization settings: npx tailwindcss init Create Your Custom CSS File Make a new CSS file in the docs/stylesheets/ directory named tailwind.css . This file will import Tailwind's layers for you to use throughout your project: @ import 'tailwindcss/base' ; @ import 'tailwindcss/components' ; @ import 'tailwindcss/utilities' ; Build Tailwind CSS Compile your CSS with Tailwind's styles by running: npx tailwindcss build docs/stylesheets/tailwind.css -o docs/stylesheets/output.css This command will process your custom CSS file and output a fully compiled CSS file with all of Tailwind's utility classes. Update MkDocs Configuration Include the compiled CSS in your MkDocs configuration by editing mkdocs.yml : extra_css : - stylesheets/output.css Customization You can now use Tailwind CSS classes in your Markdown content or in any HTML templates you're using within your MkDocs site. Build MkDocs To see the changes take effect, build your MkDocs project with: mkdocs build Note: Any changes made to Tailwind or your custom CSS require you to rebuild Tailwind CSS and then rebuild your MkDocs project.","title":"Tailwind CSS"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#integrating-tailwind-css-with-material-for-mkdocs-insiders","text":"","title":"Integrating Tailwind CSS with Material for MkDocs Insiders"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#introduction","text":"Combine the power of Tailwind CSS with Material for MkDocs Insiders to create a documentation site that not only has the robust features and beautiful design of Material but also the utility-first flexibility and customization of Tailwind CSS. This guide will walk you through the steps to integrate Tailwind CSS into your MkDocs project, allowing you to tailor the look and feel of your site with ease.","title":"Introduction"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#setting-up-material-for-mkdocs","text":"Start by setting up Material for MkDocs as per the official guidelines. Ensure you have a running MkDocs environment before proceeding with the integration of Tailwind CSS.","title":"Setting Up Material for MkDocs"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#install-tailwind-css","text":"Within your MkDocs project directory, run the following command to install the necessary packages: npm install tailwindcss postcss autoprefixer These packages include Tailwind CSS for the utility-first CSS framework, PostCSS for processing CSS with JavaScript, and Autoprefixer for handling CSS vendor prefixes.","title":"Install Tailwind CSS"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#setting-up-postcss-and-tailwind","text":"Create a postcss.config.js file in the root of your MkDocs project with the following content: module . exports = { plugins : [ require ( 'tailwindcss' ), require ( 'autoprefixer' )] } Then, initialize your Tailwind configuration file which Tailwind uses to read your customization settings: npx tailwindcss init","title":"Setting up PostCSS and Tailwind"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#create-your-custom-css-file","text":"Make a new CSS file in the docs/stylesheets/ directory named tailwind.css . This file will import Tailwind's layers for you to use throughout your project: @ import 'tailwindcss/base' ; @ import 'tailwindcss/components' ; @ import 'tailwindcss/utilities' ;","title":"Create Your Custom CSS File"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#build-tailwind-css","text":"Compile your CSS with Tailwind's styles by running: npx tailwindcss build docs/stylesheets/tailwind.css -o docs/stylesheets/output.css This command will process your custom CSS file and output a fully compiled CSS file with all of Tailwind's utility classes.","title":"Build Tailwind CSS"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#update-mkdocs-configuration","text":"Include the compiled CSS in your MkDocs configuration by editing mkdocs.yml : extra_css : - stylesheets/output.css","title":"Update MkDocs Configuration"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#customization","text":"You can now use Tailwind CSS classes in your Markdown content or in any HTML templates you're using within your MkDocs site.","title":"Customization"},{"location":"projects/websites/mkdocs/knowledge_base/adding_assets/additional_css/tailwind_css/setup_material_mkdocs_insiders_tailwind/#build-mkdocs","text":"To see the changes take effect, build your MkDocs project with: mkdocs build Note: Any changes made to Tailwind or your custom CSS require you to rebuild Tailwind CSS and then rebuild your MkDocs project.","title":"Build MkDocs"},{"location":"projects/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/","text":"Rendering 3D GitHub Contributions on AWS Introduction In the rapidly evolving landscape of cloud computing and 3D rendering, innovative solutions are constantly being developed to handle complex tasks more efficiently. One such breakthrough is the use of AWS Lambda for rendering 3D scenes, particularly for projects where high scalability and quick turnaround times are crucial. This approach is especially relevant when dealing with a large volume of simpler assets that need to be rendered swiftly, leveraging the power of cloud computing. The concept of running Blender, a popular open-source 3D graphics software, on AWS Lambda, presents a unique blend of flexibility and power. This method is ideal for scenarios where each asset is simple enough to be processed within the constraints of Lambda functions, which, as of my last update, offer up to 6 vCPUs and 10GB of RAM. For more complex rendering tasks, alternatives like EC2 instances or AWS Thinkbox Deadline might be more suitable due to their enhanced computational capabilities. The inspiration for utilizing Blender on AWS Lambda came from discovering that a similar approach was successfully implemented by Theodo in 2021. Technology Stack Frontend Stack The frontend of this project is developed with Reflex, a Python-based framework ideal for crafting interactive web applications. Reflex's key advantage lies in its ability to enable component reuse across both frontend and backend, crucial for a cohesive user experience. It also offers an extensive range of components for creating a responsive and interactive frontend. Styling is achieved through Tailwind CSS, a utility-first framework that facilitates rapid, custom UI development. This choice is beneficial as it simplifies customization without the need for extensive CSS coding. For hosting, the project utilizes Vercel, a cloud platform optimized for static sites and serverless functions. Vercel's strengths include easy deployment and efficient management of serverless functions, making it a fitting choice for the project's frontend needs. This combination of technologies ensures a streamlined, efficient, and aesthetically pleasing user interface. Language Python Framework Reflex Styling Tailwind CSS Hosting Vercel Backend Stack The project's backend is developed using Reflex, a Python-based framework ideal for building interactive web applications. Reflex's ability to facilitate component reuse across both frontend and backend significantly enhances the user experience. Additionally, it provides a variety of responsive and interactive components for frontend development. For hosting, the backend utilizes AWS Lambda, a serverless computing service. This choice is advantageous due to its scalability and cost-effectiveness, as it only charges for the compute time used. The project also integrates Reflex Database, a NoSQL database optimized for Python, enabling straightforward data storage and retrieval without SQL. Furthermore, AWS S3 is employed for cloud storage, offering efficient data handling capabilities in Python without the need for SQL coding. This combination of technologies ensures a robust and efficient backend infrastructure for the project. Language Python Framework Reflex Hosting AWS Lambda Database Reflex Database Other AWS S3 Online IDE The online IDE used for this project was Replit, a cloud-based IDE that allows you to code in Python, JavaScript, HTML, CSS, and more. Replit is a great choice for this project because it allows you to easily collaborate with others and share your code with the world. https://replit.com/@harmindersinghnijjar/Bute-3D-GitHub-Contributions Render Pipeline The depicted render pipeline represents a sophisticated, cloud-based 3D rendering process, leveraging AWS services and Blender's capabilities. This pipeline is designed to efficiently handle 3D rendering requests, process them using cloud resources, and deliver the final product back to the user. Below is a detailed description of each step in this pipeline: User Request Initiation: The process begins with the user initiating a 3D rendering request. This is typically done through an API endpoint, which acts as the interface between the user and the cloud-based rendering system. API Endpoint: Upon receiving a request, the API endpoint acts as a gateway, funneling the user's requirements into the AWS ecosystem. This step is crucial for interpreting the user's request and preparing the system for the subsequent data retrieval process. Data Retrieval: This stage involves fetching the necessary information required for the 3D rendering task. The data retrieval process is a critical component, ensuring that all the required elements, such as textures, models, and other assets, are gathered and ready for processing. Blender on AWS Lambda: Once the data is retrieved, it is sent to Blender, which is hosted on AWS Lambda. AWS Lambda provides a serverless compute service, allowing Blender to run in a highly scalable, event-driven environment. This setup is particularly efficient for handling varying loads and can scale up or down based on the complexity of the rendering tasks. 3D Replication Creation: In this phase, Blender processes the data to create a 3D replication. This step involves the actual rendering process where Blender utilizes its powerful 3D graphics tools to generate the desired 3D graphical files. Uploading to AWS S3 Bucket: After the rendering is complete, the 3D graphical files are uploaded to an AWS S3 bucket. S3 provides secure, scalable object storage, making it an ideal choice for storing large files like 3D renders. The use of S3 ensures that the rendered files are stored safely and are easily accessible for download. User Download: Finally, the link to the rendered 3D graphical files in the S3 bucket is sent back to the user. This completes the full cycle of the rendering process, allowing the user to download the final product directly from the cloud. This full-cycle 3D rendering process on AWS exemplifies a modern, cloud-based approach to 3D graphics production. It highlights the integration of serverless computing, cloud storage, and powerful rendering software to deliver a seamless, scalable, and efficient 3D rendering service.","title":"3D GitHub Contributions"},{"location":"projects/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#rendering-3d-github-contributions-on-aws","text":"","title":"Rendering 3D GitHub Contributions on AWS"},{"location":"projects/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#introduction","text":"In the rapidly evolving landscape of cloud computing and 3D rendering, innovative solutions are constantly being developed to handle complex tasks more efficiently. One such breakthrough is the use of AWS Lambda for rendering 3D scenes, particularly for projects where high scalability and quick turnaround times are crucial. This approach is especially relevant when dealing with a large volume of simpler assets that need to be rendered swiftly, leveraging the power of cloud computing. The concept of running Blender, a popular open-source 3D graphics software, on AWS Lambda, presents a unique blend of flexibility and power. This method is ideal for scenarios where each asset is simple enough to be processed within the constraints of Lambda functions, which, as of my last update, offer up to 6 vCPUs and 10GB of RAM. For more complex rendering tasks, alternatives like EC2 instances or AWS Thinkbox Deadline might be more suitable due to their enhanced computational capabilities. The inspiration for utilizing Blender on AWS Lambda came from discovering that a similar approach was successfully implemented by Theodo in 2021.","title":"Introduction"},{"location":"projects/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#technology-stack","text":"","title":"Technology Stack"},{"location":"projects/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#frontend-stack","text":"The frontend of this project is developed with Reflex, a Python-based framework ideal for crafting interactive web applications. Reflex's key advantage lies in its ability to enable component reuse across both frontend and backend, crucial for a cohesive user experience. It also offers an extensive range of components for creating a responsive and interactive frontend. Styling is achieved through Tailwind CSS, a utility-first framework that facilitates rapid, custom UI development. This choice is beneficial as it simplifies customization without the need for extensive CSS coding. For hosting, the project utilizes Vercel, a cloud platform optimized for static sites and serverless functions. Vercel's strengths include easy deployment and efficient management of serverless functions, making it a fitting choice for the project's frontend needs. This combination of technologies ensures a streamlined, efficient, and aesthetically pleasing user interface. Language Python Framework Reflex Styling Tailwind CSS Hosting Vercel","title":"Frontend Stack"},{"location":"projects/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#backend-stack","text":"The project's backend is developed using Reflex, a Python-based framework ideal for building interactive web applications. Reflex's ability to facilitate component reuse across both frontend and backend significantly enhances the user experience. Additionally, it provides a variety of responsive and interactive components for frontend development. For hosting, the backend utilizes AWS Lambda, a serverless computing service. This choice is advantageous due to its scalability and cost-effectiveness, as it only charges for the compute time used. The project also integrates Reflex Database, a NoSQL database optimized for Python, enabling straightforward data storage and retrieval without SQL. Furthermore, AWS S3 is employed for cloud storage, offering efficient data handling capabilities in Python without the need for SQL coding. This combination of technologies ensures a robust and efficient backend infrastructure for the project. Language Python Framework Reflex Hosting AWS Lambda Database Reflex Database Other AWS S3","title":"Backend Stack"},{"location":"projects/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#online-ide","text":"The online IDE used for this project was Replit, a cloud-based IDE that allows you to code in Python, JavaScript, HTML, CSS, and more. Replit is a great choice for this project because it allows you to easily collaborate with others and share your code with the world. https://replit.com/@harmindersinghnijjar/Bute-3D-GitHub-Contributions","title":"Online IDE"},{"location":"projects/websites/vercel/gitbute/3d_github_contributions/3d_github_contributions/#render-pipeline","text":"The depicted render pipeline represents a sophisticated, cloud-based 3D rendering process, leveraging AWS services and Blender's capabilities. This pipeline is designed to efficiently handle 3D rendering requests, process them using cloud resources, and deliver the final product back to the user. Below is a detailed description of each step in this pipeline: User Request Initiation: The process begins with the user initiating a 3D rendering request. This is typically done through an API endpoint, which acts as the interface between the user and the cloud-based rendering system. API Endpoint: Upon receiving a request, the API endpoint acts as a gateway, funneling the user's requirements into the AWS ecosystem. This step is crucial for interpreting the user's request and preparing the system for the subsequent data retrieval process. Data Retrieval: This stage involves fetching the necessary information required for the 3D rendering task. The data retrieval process is a critical component, ensuring that all the required elements, such as textures, models, and other assets, are gathered and ready for processing. Blender on AWS Lambda: Once the data is retrieved, it is sent to Blender, which is hosted on AWS Lambda. AWS Lambda provides a serverless compute service, allowing Blender to run in a highly scalable, event-driven environment. This setup is particularly efficient for handling varying loads and can scale up or down based on the complexity of the rendering tasks. 3D Replication Creation: In this phase, Blender processes the data to create a 3D replication. This step involves the actual rendering process where Blender utilizes its powerful 3D graphics tools to generate the desired 3D graphical files. Uploading to AWS S3 Bucket: After the rendering is complete, the 3D graphical files are uploaded to an AWS S3 bucket. S3 provides secure, scalable object storage, making it an ideal choice for storing large files like 3D renders. The use of S3 ensures that the rendered files are stored safely and are easily accessible for download. User Download: Finally, the link to the rendered 3D graphical files in the S3 bucket is sent back to the user. This completes the full cycle of the rendering process, allowing the user to download the final product directly from the cloud. This full-cycle 3D rendering process on AWS exemplifies a modern, cloud-based approach to 3D graphics production. It highlights the integration of serverless computing, cloud storage, and powerful rendering software to deliver a seamless, scalable, and efficient 3D rendering service.","title":"Render Pipeline"}]}